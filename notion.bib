@inproceedings{10.1145/3408877.3432527,
author = {Brinkley, Julian and Huff, Earl W. and Boateng, Kwajo},
title = {Tough but Effective: Exploring the Use of Remote Participatory Design in an Inclusive Design Course Through Student Reflections},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432527},
doi = {10.1145/3408877.3432527},
abstract = {Accessibility is a topic that is recognized as increasingly important in both industry and academia. In industry, there is a growing awareness of the importance of designing technologies that provide an optimal user experience regardless of the user's ability or impairment. At the university level, this awareness of the need for accessibility minded future technologists has encouraged the incorporation of accessibility and inclusive design content into a wide range of computer science and related courses. Despite this growing awareness, the actual process of teaching students about the needs and perspectives of persons with disabilities remains challenging. Within this paper we report on our experience teaching a graduate course on inclusive design with a specific focus on our experience incorporating a participatory design (PD) activity involving older adults as the course's concluding design activity. While students responded favorably to much of the course's content, learning was enhanced through the design activity which mirrored a real-world project and included three older adult (80+) co-designers. Despite the benefits of PD, students were at times overwhelmed by the workload and the logistical difficulties of working within a remote participatory design process. We believe that a review of our experience and student reflections may prove beneficial to those similarly teaching inclusive design and accessibility.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {170–176},
numpages = {7},
keywords = {education, participatory design, accessibility, assistive technology},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}
@inproceedings{10.1145/3328778.3366931,
author = {Brinkley, Julian},
title = {Participation at What Cost? Teaching Accessibility Using Participatory Design: An Experience Report},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366931},
doi = {10.1145/3328778.3366931},
abstract = {As institutions respond to market demand in their training of the next generation of technology designers, there is an increasing awareness of the need to add accessibility to computer science and informatics curricula. Advocates have suggested three strategies for including accessibility and discussions of disability in courses: changing a lecture, adding a lecture or adding a new course. In this paper we report on our experiences with the latter; incorporating accessibility within two new graduate and undergraduate inclusive design courses taught concurrently. We found that while the use of participatory design was decidedly effective in supporting student learning and ameliorating ableist attitudes, creating and managing teams comprised of students and visually impaired co-designers proved challenging. Despite these challenges, overall, students demonstrated steady growth in their grasp of inclusive design concepts as they tackled accessibility challenges through a series of mobility-related group projects. Efficiencies were also realized through the concurrent teaching of both courses though the pace of course deliverables proved challenging at times for undergraduates. We argue that a review of our experience may help others interested in teaching accessibility related courses, specifically in course design and execution.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {114–120},
numpages = {7},
keywords = {participatory design, education, assistive technology, accessibility},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}
@inbook{Costanza2020Design,
	author = {Costanza-Chock, Sasha},
	booktitle = {Design {Justice}},
	edition = {1},
	year = {2020},
	month = {feb 27},
	note = {https://designjustice.mitpress.mit.edu/pub/cfohnud7},
	publisher = {},
	title = {Design {Practices}: ``{Nothing} about {Us} without {Us}''},
}
@inproceedings{10.1145/1228175.1228209,
author = {Farrell, Vivienne and Farrell, Graham and Mouzakis, Kon and Pilgrim, Chris and Byrt, Pauline},
title = {PICTIOL: A Case Study in Participatory Design},
year = {2006},
isbn = {1595935452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1228175.1228209},
doi = {10.1145/1228175.1228209},
abstract = {Participatory design is an essential element of the skill set of professional interface developers and therefore is a significant component of HCI courses at universities. The PICTIVE technique is a 'low-fidelity' collaborative design technique that encourages participatory design. Significant challenges arise when attempting to introduce participatory design techniques such as PICTIVE to students who may not be studying on campus.This paper is a case-study in the design, evolution and refinement of an educational software tool designed to provide off-campus students with experience in collaborative user-centred software design.This paper investigates the origins and value of participatory design and its implementation using the PICTIVE technique. The paper describes the process of creating PICTIOL, a web-delivered solution to provide experience in problem-based learning, emulating the PICTIVE technique. Stages in development of the new software are described, including various HCI testing techniques and the iterative design/implementation/feedback loop. The paper concludes with a discussion of the potential of the PICTIOL in education and industry.Whilst the focus of the project was on the development of the PICTIOL tool, the very process of creating PICTIOL is itself an example of collaborative user-centred software design.},
booktitle = {Proceedings of the 18th Australia Conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments},
pages = {191–198},
numpages = {8},
keywords = {participatory design, CSCW, problem-based learning, PICTIVE, collaborative online learning, graphical user interface},
location = {Sydney, Australia},
series = {OZCHI '06}
}
@incollection{LAZAR2017153,
title = {Chapter 7 - Case studies},
editor = {Jonathan Lazar and Jinjuan Heidi Feng and Harry Hochheiser},
booktitle = {Research Methods in Human Computer Interaction (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {153-185},
year = {2017},
isbn = {978-0-12-805390-4},
doi = {https://doi.org/10.1016/B978-0-12-805390-4.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128053904000078},
author = {Jonathan Lazar and Jinjuan Heidi Feng and Harry Hochheiser},
keywords = {Case studies, Qualitative research, Triangulation, Embedded, Holistic},
abstract = {Understanding specific details of how technology is used, or might be used, often requires in-depth examination of activities conducted in context. Case studies use extensive examination of individuals and groups facing specific challenges to understand real and potential impacts of computing technology. Case studies involve in-depth, in-context examination of a small number of cases, using multiple data sources analyzed through qualitative methods to build nuanced descriptions capturing the complexities of the environments in question. Possible goals of case studies include exploration of design opportunities; explanation of activities in context; descriptions of systems, contexts, or processes, and demonstration of the successful use of novel tools. Case studies may be intrinsic—only of interest to a specific situation—or instrumental, hoping to generate insights of more general interest. Challenges in case study research include selection of cases, development of research questions, applying qualitative analysis techniques to relevant observations, and developing clear and sound presentations of case study results.}
}
@inproceedings{gururangan-etal-2022-whose,
    title = "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection",
    author = "Gururangan, Suchin  and
      Card, Dallas  and
      Dreier, Sarah  and
      Gade, Emily  and
      Wang, Leroy  and
      Wang, Zeyu  and
      Zettlemoyer, Luke  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.165",
    pages = "2562--2580",
    abstract = "Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles{---}written by students from across the country{---}we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban zones (ZIP codes) are more likely to be classified as high quality. We also show that this quality measurement is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.",
}
@inproceedings{kim-etal-2019-compound,
    title = "Compound Probabilistic Context-Free Grammars for Grammar Induction",
    author = "Kim, Yoon  and
      Dyer, Chris  and
      Rush, Alexander",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1228",
    doi = "10.18653/v1/P19-1228",
    pages = "2369--2385",
    abstract = "We study a formalization of the grammar induction problem that models sentences as being generated by a compound probabilistic context free grammar. In contrast to traditional formulations which learn a single stochastic grammar, our context-free rule probabilities are modulated by a per-sentence continuous latent variable, which induces marginal dependencies beyond the traditional context-free assumptions. Inference in this context-dependent grammar is performed by collapsed variational inference, in which an amortized variational posterior is placed on the continuous variable, and the latent trees are marginalized with dynamic programming. Experiments on English and Chinese show the effectiveness of our approach compared to recent state-of-the-art methods for grammar induction from words with neural language models.",
}
@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract={We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
}
@misc{hong2022dfx,
      title={DFX: A Low-latency Multi-FPGA Appliance for Accelerating Transformer-based Text Generation}, 
      author={Seongmin Hong and Seungjae Moon and Junsoo Kim and Sungjae Lee and Minsub Kim and Dongsoo Lee and Joo-Young Kim},
      year={2022},
      eprint={2209.10797},
      archivePrefix={arXiv},
      primaryClass={eess.SY},
      abstract={Transformer is a deep learning language model widely used for natural language processing (NLP) services in datacenters. Among transformer models, Generative Pre-trained Transformer (GPT) has achieved remarkable performance in text generation, or natural language generation (NLG), which needs the processing of a large input context in the summarization stage, followed by the generation stage that produces a single word at a time. The conventional platforms such as GPU are specialized for the parallel processing of large inputs in the summarization stage, but their performance significantly degrades in the generation stage due to its sequential characteristic. Therefore, an efficient hardware platform is required to address the high latency caused by the sequential characteristic of text generation. In this paper, we present DFX, a multi-FPGA acceleration appliance that executes GPT-2 model inference end-to-end with low latency and high throughput in both summarization and generation stages. DFX uses model parallelism and optimized dataflow that is model-and-hardware-aware for fast simultaneous workload execution among devices. Its compute cores operate on custom instructions and provide GPT-2 operations end-to-end. We implement the proposed hardware architecture on four Xilinx Alveo U280 FPGAs and utilize all of the channels of the high bandwidth memory (HBM) and the maximum number of compute resources for high hardware efficiency. DFX achieves 5.58x speedup and 3.99x energy efficiency over four NVIDIA V100 GPUs on the modern GPT-2 model. DFX is also 8.21x more cost-effective than the GPU appliance, suggesting that it is a promising solution for text generation workloads in cloud datacenters.}
}
@misc{fan2022adaptable,
      title={Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design}, 
      author={Hongxiang Fan and Thomas Chau and Stylianos I. Venieris and Royson Lee and Alexandros Kouris and Wayne Luk and Nicholas D. Lane and Mohamed S. Abdelfattah},
      year={2022},
      eprint={2209.09570},
      archivePrefix={arXiv},
      primaryClass={cs.AR},
      abstract={Attention-based neural networks have become pervasive in many AI tasks. Despite their excellent algorithmic performance, the use of the attention mechanism and feed-forward network (FFN) demands excessive computational and memory resources, which often compromises their hardware performance. Although various sparse variants have been introduced, most approaches only focus on mitigating the quadratic scaling of attention on the algorithm level, without explicitly considering the efficiency of mapping their methods on real hardware designs. Furthermore, most efforts only focus on either the attention mechanism or the FFNs but without jointly optimizing both parts, causing most of the current designs to lack scalability when dealing with different input lengths. This paper systematically considers the sparsity patterns in different variants from a hardware perspective. On the algorithmic level, we propose FABNet, a hardware-friendly variant that adopts a unified butterfly sparsity pattern to approximate both the attention mechanism and the FFNs. On the hardware level, a novel adaptable butterfly accelerator is proposed that can be configured at runtime via dedicated hardware control to accelerate different butterfly layers using a single unified hardware engine. On the Long-Range-Arena dataset, FABNet achieves the same accuracy as the vanilla Transformer while reducing the amount of computation by 10 to 66 times and the number of parameters 2 to 22 times. By jointly optimizing the algorithm and hardware, our FPGA-based butterfly accelerator achieves 14.2 to 23.2 times speedup over state-of-the-art accelerators normalized to the same computational budget. Compared with optimized CPU and GPU designs on Raspberry Pi 4 and Jetson Nano, our system is up to 273.8 and 15.1 times faster under the same power budget.}
}
@misc{yazdanbakhsh2022sparse,
      title={Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation}, 
      author={Amir Yazdanbakhsh and Ashkan Moradifirouzabadi and Zheng Li and Mingu Kang},
      year={2022},
      eprint={2209.00606},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      abstract={As its core computation, a self-attention mechanism gauges pairwise correlations across the entire input sequence. Despite favorable performance, calculating pairwise correlations is prohibitively costly. While recent work has shown the benefits of runtime pruning of elements with low attention scores, the quadratic complexity of self-attention mechanisms and their on-chip memory capacity demands are overlooked. This work addresses these constraints by architecting an accelerator, called SPRINT, which leverages the inherent parallelism of ReRAM crossbar arrays to compute attention scores in an approximate manner. Our design prunes the low attention scores using a lightweight analog thresholding circuitry within ReRAM, enabling SPRINT to fetch only a small subset of relevant data to on-chip memory. To mitigate potential negative repercussions for model accuracy, SPRINT re-computes the attention scores for the few fetched data in digital. The combined in-memory pruning and on-chip recompute of the relevant attention scores enables SPRINT to transform quadratic complexity to a merely linear one. In addition, we identify and leverage a dynamic spatial locality between the adjacent attention operations even after pruning, which eliminates costly yet redundant data fetches. We evaluate our proposed technique on a wide range of state-of-the-art transformer models. On average, SPRINT yields 7.5x speedup and 19.6x energy reduction when total 16KB on-chip memory is used, while virtually on par with iso-accuracy of the baseline models (on average 0.36% degradation).},
}
@inproceedings{Zadeh_2020,
	doi = {10.1109/micro50266.2020.00071},  
	url = {https://doi.org/10.1109%2Fmicro50266.2020.00071},
	year = 2020,
	month = {oct},
	publisher = {IEEE},
	author = {Ali Hadi Zadeh and Isak Edo and Omar Mohamed Awad and Andreas Moshovos},
	title = {{GOBO}: Quantizing Attention-Based {NLP} Models for Low Latency and Energy Efficient Inference},
  abstract = {Attention-based models have demonstrated remarkable success in various natural language understanding tasks. However, efficient execution remains a challenge for these models which are memory-bound due to their massive number of parameters. We present GOBO, a model quantization technique that compresses the vast majority (typically 99.9%) of the 32-bit floating-point parameters of state-of-the-art BERT models and their variants to 3 bits while maintaining their accuracy. Unlike other quantization methods, GOBO does not require fine-tuning nor retraining to compensate for the quantization error. We present two practical hardware applications of GOBO. In the first GOBO reduces memory storage and traffic and as a result inference latency and energy consumption. This GOBO memory compression mechanism is plug-in compatible with many architectures; we demonstrate it with the TPU, Eyeriss, and an architecture using Tensor Cores-like units. Second, we present a co-designed hardware architecture that also reduces computation. Uniquely, the GOBO architecture maintains most of the weights in 3b even during computation, a property that: (1) makes the processing elements area efficient, allowing us to pack more compute power per unit area, (2) replaces most multiply-accumulations with additions, and (3) reduces the off-chip traffic by amplifying on-chip memory capacity.},
	booktitle = {2020 53rd Annual {IEEE}/{ACM} International Symposium on Microarchitecture ({MICRO})}
}

@inproceedings{10.1109/ISCA45697.2020.00023,
author = {Abts, Dennis and Ross, Jonathan and Sparling, Jonathan and Wong-VanHaren, Mark and Baker, Max and Hawkins, Tom and Bell, Andrew and Thompson, John and Kahsai, Temesghen and Kimmell, Garrin and Hwang, Jennifer and Leslie-Hurd, Rebekah and Bye, Michael and Creswick, E. R. and Boyd, Matthew and Venigalla, Mahitha and Laforge, Evan and Purdy, Jon and Kamath, Purushotham and Maheshwari, Dinesh and Beidler, Michael and Rosseel, Geert and Ahmad, Omar and Gagarin, Gleb and Czekalski, Richard and Rane, Ashay and Parmar, Sahil and Werner, Jeff and Sproch, Jim and Macias, Adrian and Kurtz, Brian},
title = {Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning Workloads},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00023},
doi = {10.1109/ISCA45697.2020.00023},
abstract = {In this paper, we introduce the Tensor Streaming Processor (TSP) architecture, a functionally-sliced microarchitecture with memory units interleaved with vector and matrix deep learning functional units in order to take advantage of dataflow locality of deep learning operations. The TSP is built based on two key observations: (1) machine learning workloads exhibit abundant data parallelism, which can be readily mapped to tensors in hardware, and (2) a simple and deterministic processor with producer-consumer stream programming model enables precise reasoning and control of hardware components, achieving good performance and power efficiency. The TSP is designed to exploit parallelism inherent in machine-learning workloads including instruction-level, memory concurrency, data and model parallelism, while guaranteeing determinism by eliminating all reactive elements in the hardware (e.g. arbiters, and caches). Early ResNet50 image classification results demonstrate 20.4K processed images per second (IPS) with a batch-size of one---a 4x improvement compared to other modern GPUs and accelerators [44]. Our first ASIC implementation of the TSP architecture yields a computational density of more than 1 TeraOp/s per square mm of silicon for its 25x29 mm 14nm chip operating at a nominal clock frequency of 900 MHz. The TSP demonstrates a novel hardware-software approach to achieve fast, yet predictable, performance on machine-learning workloads within a desired power envelope.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {145–158},
numpages = {14},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1109/ISCA45697.2020.00045,
author = {Reddi, Vijay Janapa and Cheng, Christine and Kanter, David and Mattson, Peter and Schmuelling, Guenther and Wu, Carole-Jean and Anderson, Brian and Breughe, Maximilien and Charlebois, Mark and Chou, William and Chukka, Ramesh and Coleman, Cody and Davis, Sam and Deng, Pan and Diamos, Greg and Duke, Jared and Fick, Dave and Gardner, J. Scott and Hubara, Itay and Idgunji, Sachin and Jablin, Thomas B. and Jiao, Jeff and John, Tom St. and Kanwar, Pankaj and Lee, David and Liao, Jeffery and Lokhmotov, Anton and Massa, Francisco and Meng, Peng and Micikevicius, Paulius and Osborne, Colin and Pekhimenko, Gennady and Rajan, Arun Tejusve Raghunath and Sequeira, Dilip and Sirasao, Ashish and Sun, Fei and Tang, Hanlin and Thomson, Michael and Wei, Frank and Wu, Ephrem and Xu, Lingjie and Yamada, Koichi and Yu, Bing and Yuan, George and Zhong, Aaron and Zhang, Peizhao and Zhou, Yuchen},
title = {MLPerf Inference Benchmark},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00045},
doi = {10.1109/ISCA45697.2020.00045},
abstract = {Machine-learning (ML) hardware and software system demand is burgeoning. Driven by ML applications, the number of different ML inference systems has exploded. Over 100 organizations are building ML inference chips, and the systems that incorporate existing models span at least three orders of magnitude in power consumption and five orders of magnitude in performance; they range from embedded devices to data-center solutions. Fueling the hardware are a dozen or more software frameworks and libraries. The myriad combinations of ML hardware and ML software make assessing ML-system performance in an architecture-neutral, representative, and reproducible manner challenging. There is a clear need for industry-wide standard ML benchmarking and evaluation criteria. MLPerf Inference answers that call. In this paper, we present our benchmarking method for evaluating ML inference systems. Driven by more than 30 organizations as well as more than 200 ML engineers and practitioners, MLPerf prescribes a set of rules and best practices to ensure comparability across systems with wildly differing architectures. The first call for submissions garnered more than 600 reproducible inference-performance measurements from 14 organizations, representing over 30 systems that showcase a wide range of capabilities. The submissions attest to the benchmark's flexibility and adaptability.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {446–459},
numpages = {14},
keywords = {machine learning, inference, benchmarking},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1109/ISCA45697.2020.00080,
author = {Choukse, Esha and Sullivan, Michael B. and O'Connor, Mike and Erez, Mattan and Pool, Jeff and Nellans, David and Keckler, Stephen W.},
title = {Buddy Compression: Enabling Larger Memory for Deep Learning and HPC Workloads on GPUs},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00080},
doi = {10.1109/ISCA45697.2020.00080},
abstract = {GPUs accelerate high-throughput applications, which require orders-of-magnitude higher memory bandwidth than traditional CPU-only systems. However, the capacity of such high-bandwidth memory tends to be relatively small. Buddy Compression is an architecture that makes novel use of compression to utilize a larger buddy-memory from the host or disaggregated memory, effectively increasing the memory capacity of the GPU. Buddy Compression splits each compressed 128B memory-entry between the high-bandwidth GPU memory and a slower-but-larger buddy memory such that compressible memory-entries are accessed completely from GPU memory, while incompressible entries source some of their data from off-GPU memory. With Buddy Compression, compressibility changes never result in expensive page movement or re-allocation. Buddy Compression achieves on average 1.9x effective GPU memory expansion for representative HPC applications and 1.5x for deep learning training, performing within 2% of an unrealistic system with no memory limit. This makes Buddy Compression attractive for performance-conscious developers that require additional GPU memory capacity.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {926–939},
numpages = {14},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1109/ISCA45697.2020.00085,
author = {Klenk, Benjamin and Jiang, Nan and Thorson, Greg and Dennison, Larry},
title = {An In-Network Architecture for Accelerating Shared-Memory Multiprocessor Collectives},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00085},
doi = {10.1109/ISCA45697.2020.00085},
abstract = {The slowdown of single-chip performance scaling combined with the growing demands of computing ever larger problems efficiently has led to a renewed interest in distributed architectures and specialized hardware. Dedicated accelerators for common or critical operations are becoming cost-effective additions to processors, peripherals, and networks. In this paper we focus on one such operation, the All-Reduce, which is both a common and critical feature of neural network training. All-Reduce is impossible to fully parallelize and difficult to amortize, so it benefits greatly from hardware acceleration.We are proposing an accelerator-centric, shared-memory network that improves All-Reduce performance through in-network reductions, as well as accelerating other collectives like Multicast. We propose switch designs to support in-network computation, including two reduction methods that offer trade-offs in implementation complexity and performance. Additionally, we propose network endpoint modifications to further improve collectives.We present simulation results for a 16 GPU system showing that our collective acceleration design improves the All-Reduce operation by up to 2x for large messages and up to 18x for small messages when compared with a state-of-the-art software algorithm, leading up to 1.4x faster DL training times for networks like Transformer. We demonstrate that this design is scalable to large systems and present results for up to 128 GPUs.},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {996–1009},
numpages = {14},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1109/ISCA45697.2020.00092,
author = {Zheng, Bojian and Vijaykumar, Nandita and Pekhimenko, Gennady},
title = {Echo: Compiler-Based GPU Memory Footprint Reduction for LSTM RNN Training},
year = {2020},
isbn = {9781728146614},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA45697.2020.00092},
doi = {10.1109/ISCA45697.2020.00092},
abstract = {The Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNNs) are a popular class of machine learning models for analyzing sequential data. Their training on modern GPUs, however, is limited by the GPU memory capacity. Our profiling results of the LSTM RNN-based Neural Machine Translation (NMT) model reveal that feature maps of the attention and RNN layers form the memory bottleneck, and runtime is unevenly distributed across different layers when training on GPUs. Based on these two observations, we propose to recompute the feature maps of the attention and RNN layers rather than stashing them persistently in the GPU memory.While the idea of feature map recomputation has been considered before, existing solutions fail to deliver satisfactory footprint reduction, as they do not address two key challenges. For each feature map recomputation to be efficient, its effect on (1) the total memory footprint, and (2) the total execution time has to be carefully estimated. To this end, we propose Echo, a new compiler-based optimization scheme that addresses the first challenge with a practical mechanism that estimates the memory benefits of recomputation over the entire computation graph, and the second challenge by non-conservatively estimating the recomputation runtime overhead leveraging layer specifics. Echo reduces the GPU memory footprint automatically and transparently without any changes required to the training source code, and is effective for models beyond LSTM RNNs.We evaluate Echo on numerous state-of-the-art machine learning workloads, including NMT, DeepSpeech2, Transformer, and ResNet, on real systems with modern GPUs and observe footprint reduction ratios of 1.89x on average and 3.13x maximum. Such reduction can be converted into faster training with a larger batch size, savings in GPU energy consumption (e.g., training with one GPU as fast as with four), and/or an increase in the maximum number of layers under the same GPU memory budget. Echo is open-sourced as a part of the MXNet 2.0 framework.1},
booktitle = {Proceedings of the ACM/IEEE 47th Annual International Symposium on Computer Architecture},
pages = {1089–1102},
numpages = {14},
keywords = {DNN training, LSTM RNN, GPU memory footprint reduction},
location = {Virtual Event},
series = {ISCA '20}
}

@inproceedings{10.1109/ISCA52012.2021.00010,
author = {Jouppi, Norman P. and Yoon, Doe Hyun and Ashcraft, Matthew and Gottscho, Mark and Jablin, Thomas B. and Kurian, George and Laudon, James and Li, Sheng and Ma, Peter and Ma, Xiaoyu and Norrie, Thomas and Patil, Nishant and Prasad, Sushma and Young, Cliff and Zhou, Zongwei and Patterson, David},
title = {Ten Lessons from Three Generations Shaped Google's TPUv4i},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00010},
doi = {10.1109/ISCA52012.2021.00010},
abstract = {Google deployed several TPU generations since 2015, teaching us lessons that changed our views: semi-conductor technology advances unequally; compiler compatibility trumps binary compatibility, especially for VLIW domain-specific architectures (DSA); target total cost of ownership vs initial cost; support multi-tenancy; deep neural networks (DNN) grow 1.5X annually; DNN advances evolve workloads; some inference tasks require floating point; inference DSAs need air-cooling; apps limit latency, not batch size; and backwards ML compatibility helps deploy DNNs quickly. These lessons molded TPUv4i, an inference DSA deployed since 2020.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {1–14},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00011,
author = {Jang, Jun-Woo and Lee, Sehwan and Kim, Dongyoung and Park, Hyunsun and Ardestani, Ali Shafiee and Choi, Yeongjae and Kim, Channoh and Kim, Yoojin and Yu, Hyeongseok and Abdel-Aziz, Hamzah and Park, Jun-Seok and Lee, Heonsoo and Lee, Dongwoo and Kim, Myeong Woo and Jung, Hanwoong and Nam, Heewoo and Lim, Dongguen and Lee, Seungwon and Song, Joon-Ho and Kwon, Suknam and Hassoun, Joseph and Lim, SukHwan and Choi, Changkyu},
title = {Sparsity-Aware and Re-Configurable NPU Architecture for Samsung Flagship Mobile SoC},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00011},
doi = {10.1109/ISCA52012.2021.00011},
abstract = {Of late, deep neural networks have become ubiquitous in mobile applications. As mobile devices generally require immediate response while maintaining user privacy, the demand for on-device machine learning technology is on the increase. Nevertheless, mobile devices suffer from restricted hardware resources, whereas deep neural networks involve considerable computation and communication. Therefore, the implementation of a neural-network specialized hardware accelerator, generally called neural processing unit (NPU), has started to gain attention for the mobile application processor (AP). However, NPUs for commercial mobile AP face two challenges that are difficult to realize simultaneously: execution of a wide range of applications and efficient performance.In this paper, we propose a flexible but efficient NPU architecture for a Samsung flagship mobile system-on-chip (SoC). To implement an efficient NPU, we design an energy-efficient inner-product engine that utilizes the input feature map sparsity. We propose a re-configurable MAC array to enhance the flexibility of the proposed NPU, dynamic internal memory port assignment to maximize on-chip memory bandwidth utilization, and efficient architecture to support mixed-precision arithmetic. We implement the proposed NPU using the Samsung 5nm library. Our silicon measurement experiments demonstrate that the proposed NPU achieves 290.7 FPS and 13.6 TOPS/W, when executing an 8-bit quantized Inception-v3 model [1] with a single NPU core. In addition, we analyze the proposed zero-skipping architecture in detail. Finally, we present the findings and lessons learned when implementing the commercial mobile NPU and interesting avenues for future work.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {15–28},
numpages = {14},
keywords = {sparsity, neural network, re-configurable, mixed-precision, accelerator, neural processing unit},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00012,
author = {Thompto, Brian W. and Nguyen, Dung Q. and Moreira, Jos\'{e} E. and Bertran, Ramon and Jacobson, Hans and Eickemeyer, Richard J. and Rao, Rahul M. and Goulet, Michael and Byers, Marcy and Gonzalez, Christopher J. and Swaminathan, Karthik and Dhanwada, Nagu R. and M\"{u}ller, Silvia M. and Wagner, Andreas and Sadasivam, Satish Kumar and Montoye, Robert K. and Starke, William J. and Zoellin, Christian G. and Floyd, Michael S. and Stuecheli, Jeffrey and Chandramoorthy, Nandhini and Wellman, John-David and Buyuktosunoglu, Alper and Pflanz, Matthias and Sinharoy, Balaram and Bose, Pradip},
title = {Energy Efficiency Boost in the AI-Infused POWER10 Processor},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00012},
doi = {10.1109/ISCA52012.2021.00012},
abstract = {We present the novel micro-architectural features, supported by an innovative and novel pre-silicon methodology in the design of POWER10. The resulting projected energy efficiency boost over POWER9 is 2.6x at core level (for SPECint) and up to 3x at socket level. In addition, a new feature supporting inline AI acceleration was added to the POWER ISA and incorporated into the POWER10 processor core design. The resulting boost in SIMD/AI socket performance is projected to be up to 10x for FP32 and 21x for INT8 models of ResNet-50 and BERT-Large. In this paper, we describe the novel methodology deployed and used not only to obtain these efficiency boosts for traditional workloads, but also to infuse AI/ML/HPC capability directly into the POWER10 core.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {29–42},
numpages = {14},
keywords = {AI acceleration, microprocessor design methodology, POWER10, pre-silicon modeling, energy efficiency},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00014,
author = {Naffziger, Samuel and Beck, Noah and Burd, Thomas and Lepak, Kevin and Loh, Gabriel H. and Subramony, Mahesh and White, Sean},
title = {Pioneering Chiplet Technology and Design for the AMD EPYC™ and Ryzen™ Processor Families},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00014},
doi = {10.1109/ISCA52012.2021.00014},
abstract = {For decades, Moore's Law has delivered the ability to integrate an exponentially increasing number of devices in the same silicon area at a roughly constant cost. This has enabled tremendous levels of integration, where the capabilities of computer systems that previously occupied entire rooms can now fit on a single integrated circuit.In recent times, the steady drum beat of Moore's Law has started to slow down. Whereas device density historically doubled every 18--24 months, the rate of recent silicon process advancements has declined. While improvements in device scaling continue, albeit at a reduced pace, the industry is simultaneously observing increases in manufacturing costs.In response, the industry is now seeing a trend toward reversing direction on the traditional march toward more integration. Instead, multiple industry and academic groups are advocating that systems on chips (SoCs) be "disintegrated" into multiple smaller "chiplets." This paper details the technology challenges that motivated AMD to use chiplets, the technical solutions we developed for our products, and how we expanded the use of chiplets from individual processors to multiple product families.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {57–70},
numpages = {14},
keywords = {moore's law, processors, industry, modular, chiplets},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00021,
author = {Venkataramani, Swagath and Srinivasan, Vijayalakshmi and Wang, Wei and Sen, Sanchari and Zhang, Jintao and Agrawal, Ankur and Kar, Monodeep and Jain, Shubham and Mannari, Alberto and Tran, Hoang and Li, Yulong and Ogawa, Eri and Ishizaki, Kazuaki and Inoue, Hiroshi and Schaal, Marcel and Serrano, Mauricio and Choi, Jungwook and Sun, Xiao and Wang, Naigang and Chen, Chia-Yu and Allain, Allison and Bonano, James and Cao, Nianzheng and Casatuta, Robert and Cohen, Matthew and Fleischer, Bruce and Guillorn, Michael and Haynie, Howard and Jung, Jinwook and Kang, Mingu and Kim, Kyu-hyoun and Koswatta, Siyu and Lee, Saekyu and Lutz, Martin and Mueller, Silvia and Oh, Jinwook and Ranjan, Ashish and Ren, Zhibin and Rider, Scot and Schelm, Kerstin and Scheuermann, Michael and Silberman, Joel and Yang, Jie and Zalani, Vidhi and Zhang, Xin and Zhou, Ching and Ziegler, Matt and Shah, Vinay and Ohara, Moriyoshi and Lu, Pong-Fei and Curran, Brian and Shukla, Sunil and Chang, Leland and Gopalakrishnan, Kailash},
title = {RaPiD: AI Accelerator for Ultra-Low Precision Training and Inference},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00021},
doi = {10.1109/ISCA52012.2021.00021},
abstract = {The growing prevalence and computational demands of Artificial Intelligence (AI) workloads has led to widespread use of hardware accelerators in their execution. Scaling the performance of AI accelerators across generations is pivotal to their success in commercial deployments. The intrinsic error-resilient nature of AI workloads present a unique opportunity for performance/energy improvement through precision scaling. Motivated by the recent algorithmic advances in precision scaling for inference and training, we designed RAPID1, a 4-core AI accelerator chip supporting a spectrum of precisions, namely, 16 and 8-bit floating-point and 4 and 2-bit fixed-point. The 36mm2 RAPID chip fabricated in 7nm EUV technology delivers a peak 3.5 TFLOPS/W in HFP8 mode and 16.5 TOPS/W in INT4 mode at nominal voltage. Using a performance model calibrated to within 1% of the measurement results, we evaluated DNN inference using 4-bit fixed-point representation for a 4-core 1 RAPID chip system and DNN training using 8-bit floating point representation for a 768 TFLOPs AI system comprising 4 32-core RAPID chips. Our results show INT4 inference for batch size of 1 achieves 3 - 13.5 (average 7) TOPS/W and FP8 training for a mini-batch of 512 achieves a sustained 102 - 588 (average 203) TFLOPS across a wide range of applications.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {153–166},
numpages = {14},
keywords = {deep neural networks, reduced precision, hardware acceleration},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00022,
author = {Nori, Anant V. and Bera, Rahul and Balachandran, Shankar and Rakshit, Joydeep and Omer, Om J. and Abuhatzera, Avishaii and Kuttanna, Belliappa and Subramoney, Sreenivas},
title = {REDUCT: Keep It Close, Keep It Cool! Efficient Scaling of DNN Inference on Multi-Core CPUs with near-Cache Compute},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00022},
doi = {10.1109/ISCA52012.2021.00022},
abstract = {Deep Neural Networks (DNN) are used in a variety of applications and services. With the evolving nature of DNNs, the race to build optimal hardware (both in datacenter and edge) continues. General purpose multi-core CPUs offer unique attractive advantages for DNN inference at both datacenter [60] and edge [71]. Most of the CPU pipeline design complexity is targeted towards optimizing general-purpose single thread performance, and is overkill for relatively simpler, but still hugely important, data parallel DNN inference workloads. Addressing this disparity efficiently can enable both raw performance scaling and overall performance/Watt improvements for multi-core CPU DNN inference.We present REDUCT, where we build innovative solutions that bypass traditional CPU resources which impact DNN inference power and limit its performance. Fundamentally, REDUCT's "Keep it close" policy enables consecutive pieces of work to be executed close to each other. REDUCT enables instruction delivery/decode close to execution and instruction execution close to data. Simple ISA extensions encode the fixed-iteration count loop-y workload behavior enabling an effective bypass of many power-hungry front-end stages of the wide Out-of-Order (OoO) CPU pipeline. Per core performance scales efficiently by distributing lightweight tensor compute near all caches in a multi-level cache hierarchy. This maximizes the cumulative utilization of the existing architectural bandwidth resources in the system and minimizes movement of data.Across a number of DNN models, REDUCT achieves a 2.3X increase in convolution performance/Watt with a 2X to 3.94X scaling in raw performance. Similarly, REDUCT achieves a 1.8X increase in inner-product performance/Watt with 2.8X scaling in performance. REDUCT performance/power scaling is achieved with no increase to cache capacity or bandwidth and a mere 2.63% increase in area. Crucially, REDUCT operates entirely within the CPU programming and memory model, simplifying software development, while achieving performance similar to or better than state-of-the-art Domain Specific Accelerators (DSA) for DNN inference, providing fresh design choices in the AI era.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {167–180},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00023,
author = {Huang, Jiayi and Majumder, Pritam and Kim, Sungkeun and Muzahid, Abdullah and Yum, Ki Hwan and Kim, Eun Jung},
title = {Communication Algorithm-Architecture Co-Design for Distributed Deep Learning},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00023},
doi = {10.1109/ISCA52012.2021.00023},
abstract = {Large-scale distributed deep learning training has enabled developments of more complex deep neural network models to learn from larger datasets for sophisticated tasks. In particular, distributed stochastic gradient descent intensively invokes all-reduce operations for gradient update, which dominates communication time during iterative training epochs. In this work, we identify the inefficiency in widely used all-reduce algorithms, and the opportunity of algorithm-architecture co-design. We propose MULTITREE all-reduce algorithm with topology and resource utilization awareness for efficient and scalable all-reduce operations, which is applicable to different interconnect topologies. Moreover, we co-design the network interface to schedule and coordinate the all-reduce messages for contention-free communications, working in synergy with the algorithm. The flow control is also simplified to exploit the bulk data transfer of big gradient exchange. We evaluate the co-design using different all-reduce data sizes for synthetic study, demonstrating its effectiveness on various interconnection network topologies, in addition to state-of-the-art deep neural networks for real workload experiments. The results show that MULTITREE achieves 2.3X and 1.56X communication speedup, as well as up to 81% and 30% training time reduction compared to ring all-reduce and state-of-the-art approaches, respectively.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {181–194},
numpages = {14},
keywords = {distributed deep learning, data-parallel training, algorithm-architecture co-design, all-reduce, interconnection network},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00051,
author = {Zhang, Xingyao and Xia, Haojun and Zhuang, Donglin and Sun, Hao and Fu, Xin and Taylor, Michael B. and Song, Shuaiwen Leon},
title = {η-LSTM: Co-Designing Highly-Efficient Large LSTM Training via Exploiting Memory-Saving and Architectural Design Opportunities},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00051},
doi = {10.1109/ISCA52012.2021.00051},
abstract = {Recently, the recurrent neural network, or its most popular type---the Long Short Term Memory (LSTM) network---has achieved great success in a broad spectrum of real-world application domains, such as autonomous driving, natural language processing, sentiment analysis, and epidemiology. Due to the complex features of the real-world tasks, current LSTM models become increasingly bigger and more complicated for enhancing the learning ability and prediction accuracy. However, through our in-depth characterization on the state-of-the-art general-purpose deep-learning accelerators, we observe that the LSTM training execution grows inefficient in terms of storage, performance, and energy consumption, under an increasing model size. With further algorithmic and architectural analysis, we identify the root cause for large LSTM training inefficiency: massive intermediate variables. To enable a highly-efficient LSTM training solution for the ever-growing model size, we exploit some unique memory-saving and performance improvement opportunities from the LSTM training procedure, and leverage them to propose the first cross-stack training solution, η-LSTM, for large LSTM models. η-LSTM comprises both software-level and hardware-level innovations that effectively lower the memory footprint upper-bound and excessive data movements during large LSTM training, while also drastically improving training performance and energy efficiency. Experimental results on six real-world large LSTM training benchmarks demonstrate that η-LSTM reduces the required memory footprint by an average of 57.5% (up to 75.8%) and brings down the data movements for weight matrices, activation data, and intermediate variables by 40.9%, 32.9%, and 80.0%, respectively. Furthermore, it outperforms the state-of-the-art GPU implementation for LSTM training by an average of 3.99\texttimes{} (up to 5.73\texttimes{}) on performance and 2.75\texttimes{} (up to 4.25x ) on energy. We hope this work can shed some light on how to design high logic utilization for future NPUs.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {567–580},
numpages = {14},
keywords = {machine learning, recurrent neural network, accelerator, neural nets},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00060,
author = {Ham, Tae Jun and Lee, Yejin and Seo, Seong Hoon and Kim, Soosung and Choi, Hyunji and Jung, Sung Jun and Lee, Jae W.},
title = {ELSA: Hardware-Software Co-Design for Efficient, Lightweight Self-Attention Mechanism in Neural Networks},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00060},
doi = {10.1109/ISCA52012.2021.00060},
abstract = {The self-attention mechanism is rapidly emerging as one of the most important key primitives in neural networks (NNs) for its ability to identify the relations within input entities. The self-attention-oriented NN models such as Google Transformer and its variants have established the state-of-the-art on a very wide range of natural language processing tasks, and many other self-attention-oriented models are achieving competitive results in computer vision and recommender systems as well. Unfortunately, despite its great benefits, the self-attention mechanism is an expensive operation whose cost increases quadratically with the number of input entities that it processes, and thus accounts for a significant portion of the inference runtime. Thus, this paper presents ELSA (Efficient, Lightweight Self-Attention), a hardware-software co-designed solution to substantially reduce the runtime as well as energy spent on the self-attention mechanism. Specifically, based on the intuition that not all relations are equal, we devise a novel approximation scheme that significantly reduces the amount of computation by efficiently filtering out relations that are unlikely to affect the final output. With the specialized hardware for this approximate self-attention mechanism, ELSA achieves a geomean speedup of 58.1X as well as over three orders of magnitude improvements in energy efficiency compared to GPU on self-attention computation in modern NN models while maintaining less than 1% loss in the accuracy metric.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {692–705},
numpages = {14},
keywords = {hardware accelerator, attention, neural network},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00061,
author = {Zhao, Yongwei and Liu, Chang and Du, Zidong and Guo, Qi and Hu, Xing and Zhuang, Yimin and Zhang, Zhenxing and Song, Xinkai and Li, Wei and Zhang, Xishan and Li, Ling and Xu, Zhiwei and Chen, Tianshi},
title = {Cambricon-Q: A Hybrid Architecture for Efficient Training},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00061},
doi = {10.1109/ISCA52012.2021.00061},
abstract = {Deep neural network (DNN) training is notoriously time-consuming, and quantization is promising to improve the training efficiency with reduced bandwidth/storage requirements and computation costs. However, state-of-the-art quantized algorithms with negligible training accuracy loss, which require on-the-fly statistic-based quantization over a great amount of data (e.g., neurons and weights) and high-precision weight update, cannot be effectively deployed on existing DNN accelerators. To address this problem, we propose the first customized architecture for efficient quantized training with negligible accuracy loss, which is named as Cambricon-Q. Cambricon-Q features a hybrid architecture consisting of an ASIC acceleration core and a near-data-processing (NDP) engine. The acceleration core mainly targets at improving the efficiency of statistic-based quantization with specialized computing units for both statistical analysis (e.g., determining maximum) and data reformating, while the NDP engine avoids transferring the high-precision weights from the off-chip memory to the acceleration core. Experimental results show that on the evaluated benchmarks, Cambricon-Q improves the energy efficiency of DNN training by 6.41X and 1.62X, performance by 4.20X and 1.70X compared to GPU and TPU, respectively, with only ⩽ 0.4% accuracy degradation compared with full precision training.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {706–719},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00062,
author = {Lu, Liqiang and Guan, Naiqing and Wang, Yuyue and Jia, Liancheng and Luo, Zizhang and Yin, Jieming and Cong, Jason and Liang, Yun},
title = {TENET: A Framework for Modeling Tensor Dataflow Based on Relation-Centric Notation},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00062},
doi = {10.1109/ISCA52012.2021.00062},
abstract = {Accelerating tensor applications on spatial architectures provides high performance and energy-efficiency, but requires accurate performance models for evaluating various dataflow alternatives. Such modeling relies on the notation of tensor dataflow and the formulation of performance metrics. Recent proposed compute-centric and data-centric notations describe the dataflow using imperative directives. However, these two notations are less expressive and thus lead to limited optimization opportunities and inaccurate performance models.In this paper, we propose a framework TENET that models hardware dataflow of tensor applications. We start by introducing a relation-centric notation, which formally describes the hardware dataflow for tensor computation. The relation-centric notation specifies the hardware dataflow, PE interconnection, and data assignment in a uniform manner using relations. The relation-centric notation is more expressive than the compute-centric and data-centric notations by using more sophisticated affine transformations. Another advantage of relation-centric notation is that it inherently supports accurate metrics estimation, including data reuse, bandwidth, latency, and energy. TENET computes each performance metric by counting the relations using integer set structures and operators. Overall, TENET achieves 37.4% and 51.4% latency reduction for CONV and GEMM kernels compared with the state-of-the-art data-centric notation by identifying more sophisticated hardware dataflows.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {720–733},
numpages = {14},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1109/ISCA52012.2021.00088,
author = {Wang, Yang and Zhang, Chen and Xie, Zhiqiang and Guo, Cong and Liu, Yunxin and Leng, Jingwen},
title = {Dual-Side Sparse Tensor Core},
year = {2021},
isbn = {9781450390866},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ISCA52012.2021.00088},
doi = {10.1109/ISCA52012.2021.00088},
abstract = {Leveraging sparsity in deep neural network (DNN) models is promising for accelerating model inference. Yet existing GPUs can only leverage the sparsity from weights but not activations, which are dynamic, unpredictable, and hence challenging to exploit. In this work, we propose a novel architecture to efficiently harness the dual-side sparsity (i.e., weight and activation sparsity). We take a systematic approach to understand the (dis)advantages of previous sparsity-related architectures and propose a novel, unexplored paradigm that combines outer-product computation primitive and bitmap-based encoding format. We demonstrate the feasibility of our design with minimal changes to the existing production-scale inner-product-based Tensor Core. We propose a set of novel ISA extensions and co-design the matrix-matrix multiplication and convolution algorithms, which are the two dominant computation patterns in today's DNN models, to exploit our new dual-side sparse Tensor Core. Our evaluation shows that our design can fully unleash the dualside DNN sparsity and improve the performance by up to one order of magnitude with small hardware overhead.},
booktitle = {Proceedings of the 48th Annual International Symposium on Computer Architecture},
pages = {1083–1095},
numpages = {13},
keywords = {graphics processing units, general sparse matrix-matrix multiplication, convolution, pruning, neural networks},
location = {Virtual Event, Spain},
series = {ISCA '21}
}

@inproceedings{10.1145/3352460.3358319,
author = {Shi, Zhan and Huang, Xiangru and Jain, Akanksha and Lin, Calvin},
title = {Applying Deep Learning to the Cache Replacement Problem},
year = {2019},
isbn = {9781450369381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3352460.3358319},
doi = {10.1145/3352460.3358319},
abstract = {Despite its success in many areas, deep learning is a poor fit for use in hardware predictors because these models are impractically large and slow, but this paper shows how we can use deep learning to help design a new cache replacement policy. We first show that for cache replacement, a powerful LSTM learning model can in an offline setting provide better accuracy than current hardware predictors. We then perform analysis to interpret this LSTM model, deriving a key insight that allows us to design a simple online model that matches the offline model's accuracy with orders of magnitude lower cost.The result is the Glider cache replacement policy, which we evaluate on a set of 33 memory-intensive programs from the SPEC 2006, SPEC 2017, and GAP (graph-processing) benchmark suites. In a single-core setting, Glider outperforms top finishers from the 2nd Cache Replacement Championship, reducing the miss rate over LRU by 8.9%, compared to reductions of 7.1% for Hawkeye, 6.5% for MPPPB, and 7.5% for SHiP++. On a four-core system, Glider improves IPC over LRU by 14.7%, compared with improvements of 13.6% (Hawkeye), 13.2% (MPPPB), and 11.4% (SHiP++).},
booktitle = {Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {413–425},
numpages = {13},
keywords = {deep learning, caches, cache replacement},
location = {Columbus, OH, USA},
series = {MICRO '52}
}

@inproceedings{10.1145/3373376.3378494,
author = {Hyun, Bongjoon and Kwon, Youngeun and Choi, Yujeong and Kim, John and Rhu, Minsoo},
title = {NeuMMU: Architectural Support for Efficient Address Translations in Neural Processing Units},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378494},
doi = {10.1145/3373376.3378494},
abstract = {To satisfy the compute and memory demands of deep neural networks (DNNs), neural processing units (NPUs) are widely being utilized for accelerating DNNs. Similar to how GPUs have evolved from a slave device into a mainstream processor architecture, it is likely that NPUs will become first-class citizens in this fast-evolving heterogeneous architecture space. This paper makes a case for enabling address translation in NPUs to decouple the virtual and physical memory address space. Through a careful data-driven application characterization study, we root-cause several limitations of prior GPU-centric address translation schemes and propose a memory management unit (MMU) that is tailored for NPUs. Compared to an oracular MMU design point, our proposal incurs only an average 0.06% performance overhead.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1109–1124},
numpages = {16},
keywords = {npu, neural processing unit, mmu, neural network, machine learning, address translation},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3373376.3378499,
author = {Luo, Qinyi and He, Jiaao and Zhuo, Youwei and Qian, Xuehai},
title = {Prague: High-Performance Heterogeneity-Aware Asynchronous Decentralized Training},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378499},
doi = {10.1145/3373376.3378499},
abstract = {Distributed deep learning training usually adopts All-Reduce as the synchronization mechanism for data parallel algorithms due to its high performance in homogeneous environment. However, its performance is bounded by the slowest worker among all workers. For this reason, it is significantly slower in heterogeneous settings. AD-PSGD, a newly proposed synchronization method which provides numerically fast convergence and heterogeneity tolerance, suffers from deadlock issues and high synchronization overhead. Is it possible to get the best of both worlds --- designing a distributed training method that has both high performance like All-Reduce in homogeneous environment and good heterogeneity tolerance like AD-PSGD?In this paper, we propose Prague, a high-performance heterogeneity-aware asynchronous decentralized training approach. We achieve the above goal with intensive synchronization optimization by exploring the interplay between algorithm and system implementation, or statistical and hardware efficiency. To reduce synchronization cost, we propose a novel communication primitive, Partial All-Reduce, that enables fast synchronization among a group of workers. To reduce serialization cost, we propose static group scheduling in homogeneous environment and simple techniques, i.e., Group Buffer and Group Division, to largely eliminate conflicts with slightly reduced randomness. Our experiments show that in homogeneous environment, Prague is 1.2x faster than the state-of-the-art implementation of All-Reduce, 5.3x faster than Parameter Server and 3.7x faster than AD-PSGD. In a heterogeneous setting, Prague tolerates slowdowns well and achieves 4.4x speedup over All-Reduce.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {401–416},
numpages = {16},
keywords = {heterogeneity, machine learning, deep learning, decentralized training},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3373376.3378505,
author = {Peng, Xuan and Shi, Xuanhua and Dai, Hulin and Jin, Hai and Ma, Weiliang and Xiong, Qian and Yang, Fan and Qian, Xuehai},
title = {Capuchin: Tensor-Based GPU Memory Management for Deep Learning},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378505},
doi = {10.1145/3373376.3378505},
abstract = {In recent years, deep learning has gained unprecedented success in various domains, the key of the success is the larger and deeper deep neural networks (DNNs) that achieved very high accuracy. On the other side, since GPU global memory is a scarce resource, large models also pose a significant challenge due to memory requirement in the training process. This restriction limits the DNN architecture exploration flexibility.In this paper, we propose Capuchin, a tensor-based GPU memory management module that reduces the memory footprint via tensor eviction/prefetching and recomputation. The key feature of Capuchin is that it makes memory management decisions based on dynamic tensor access pattern tracked at runtime. This design is motivated by the observation that the access pattern to tensors is regular during training iterations. Based on the identified patterns, one can exploit the total memory optimization space and offer the fine-grain and flexible control of when and how to perform memory optimization techniques.We deploy Capuchin in a widely-used deep learning framework, Tensorflow, and show that Capuchin can reduce the memory footprint by up to 85% among 6 state-of-the-art DNNs compared to the original Tensorflow. Especially, for the NLP task BERT, the maximum batch size that Capuchin can outperforms Tensorflow and gradient-checkpointing by 7x and 2.1x, respectively. We also show that Capuchin outperforms vDNN and gradient-checkpointing by up to 286% and 55% under the same memory oversubscription.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {891–905},
numpages = {15},
keywords = {deep learning training, tensor access, gpu memory management},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3373376.3378522,
author = {Mireshghallah, Fatemehsadat and Taram, Mohammadkazem and Ramrakhyani, Prakash and Jalali, Ali and Tullsen, Dean and Esmaeilzadeh, Hadi},
title = {Shredder: Learning Noise Distributions to Protect Inference Privacy},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378522},
doi = {10.1145/3373376.3378522},
abstract = {A wide variety of deep neural applications increasingly rely on the cloud to perform their compute-heavy inference. This common practice requires sending private and privileged data over the network to remote servers, exposing it to the service provider and potentially compromising its privacy. Even if the provider is trusted, the data can still be vulnerable over communication channels or via side-channel attacks in the cloud. To that end, this paper aims to reduce the information content of the communicated data with as little as possible compromise on the inference accuracy by making the sent data noisy. An undisciplined addition of noise can significantly reduce the accuracy of inference, rendering the service unusable. To address this challenge, this paper devises Shredder, an end-to-end framework, that, without altering the topology or the weights of a pre-trained network, learns additive noise distributions that significantly reduce the information content of communicated data while maintaining the inference accuracy. The key idea is finding the additive noise distributions by casting it as a disjoint offline learning process with a loss function that strikes a balance between accuracy and information degradation. The loss function also exposes a knob for a disciplined and controlled asymmetric trade-off between privacy and accuracy. While keeping the DNN intact, Shredder divides inference between the cloud and the edge device, striking a balance between computation and communication. In the separate phase of inference, the edge device takes samples from the Laplace distributions that were collected during the proposed offline learning phase and populates a noise tensor with these sampled elements. Then, the edge device merely adds this populated noise tensor to the intermediate results to be sent to the cloud. As such, Shredder enables accurate inference on noisy intermediate data without the need to update the model or the cloud, or any training process during inference. We also formally show that Shredder maximizes privacy with minimal impact on DNN accuracy while the tradeoff between privacy and accuracy is controlled through a mathematical knob. Experimentation with six real-world DNNs from text processing and image classification shows that Shredder reduces the mutual information between the input and the communicated data to the cloud by 74.70% compared to the original execution while only sacrificing 1.58% loss in accuracy. On average, Shredder also offers a speedup of 1.79x over Wi-Fi and 2.17x over LTE compared to cloud-only execution when using an off-the-shelf mobile GPU (Tegra X2) on the edge.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {3–18},
numpages = {16},
keywords = {cloud computing, neural networks, deep learning, noise, privacy, edge computing, inference},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3466752.3480052,
author = {Tao, Yaoyu and Zhang, Zhengya},
title = {HiMA: A Fast and Scalable History-Based Memory Access Engine for Differentiable Neural Computer},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480052},
doi = {10.1145/3466752.3480052},
abstract = {Memory-augmented neural networks (MANNs) provide better inference performance in many tasks with the help of an external memory. The recently developed differentiable neural computer (DNC) is a MANN that has been shown to outperform in representing complicated data structures and learning long-term dependencies. DNC’s higher performance is derived from new history-based attention mechanisms in addition to the previously used content-based attention mechanisms. History-based mechanisms require a variety of new compute primitives and state memories, which are not supported by existing neural network (NN) or MANN accelerators. We present HiMA, a tiled, history-based memory access engine with distributed memories in tiles. HiMA incorporates a multi-mode network-on-chip (NoC) to reduce the communication latency and improve scalability. An optimal submatrix-wise memory partition strategy is applied to reduce the amount of NoC traffic; and a two-stage usage sort method leverages distributed tiles to improve computation speed. To make HiMA fundamentally scalable, we create a distributed version of DNC called DNC-D to allow almost all memory operations to be applied to local memories with trainable weighted summation to produce the global memory output. Two approximation techniques, usage skimming and softmax approximation, are proposed to further enhance hardware efficiency. HiMA prototypes are created in RTL and synthesized in a 40nm technology. By simulations, HiMA running DNC and DNC-D demonstrates 6.47 \texttimes{} and 39.1 \texttimes{} higher speed, 22.8 \texttimes{} and 164.3 \texttimes{} better area efficiency, and 6.1 \texttimes{} and 61.2 \texttimes{} better energy efficiency over the state-of-the-art MANN accelerator. Compared to an Nvidia 3080Ti GPU, HiMA demonstrates speedup by up to 437 \texttimes{} and 2,646 \texttimes{} when running DNC and DNC-D, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {845–856},
numpages = {12},
keywords = {memory-augmented neural networks, memory access engine, differentiable neural computer},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480057,
author = {Drumond, Mario and Coulon, Louis and Pourhabibi, Arash and Y\"{u}z\"{u}g\"{u}ler, Ahmet Caner and Falsafi, Babak and Jaggi, Martin},
title = {Equinox: Training (for Free) on a Custom Inference Accelerator},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480057},
doi = {10.1145/3466752.3480057},
abstract = {DNN inference accelerators executing online services exhibit low average loads because of service demand variability, leading to poor resource utilization. Unfortunately, reclaiming idle inference cycles is difficult as other workloads can not execute on a custom accelerator. With recent proposals for the use of fixed-point arithmetic in training, there are opportunities for training services to piggyback on inference accelerators. We make the observation that a key challenge in doing so is maintaining service-level latency constraints for inference. We show that relaxing latency constraints in an inference accelerator with ALU arrays that are batching-optimized achieves near-optimal throughput for a given area and power envelope while maintaining inference services’ tail latency goals. We present Equinox, a custom inference accelerator designed to piggyback training. Equinox employs a uniform arithmetic encoding to accommodate inference and training and a priority hardware scheduler with adaptive batching that interleaves training during idle inference cycles. For a 500μs inference service time constraint, Equinox achieves 6.67 \texttimes{} higher throughput than a latency-optimal inference accelerator. Despite not being optimized for training services, Equinox achieves up to 78% of the throughput of a dedicated training accelerator that saturates the available compute resources and DRAM bandwidth. Finally, Equinox’s controller logic incurs less than 1% power and area overhead, while the uniform encoding (to enable training) incurs 13% power and 4% area overhead compared to a fixed-point inference accelerator.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {421–433},
numpages = {13},
keywords = {DNN inference, DNN accelerators, systolic arrays},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480090,
author = {Liu, Liu and Lin, Jilan and Qu, Zheng and Ding, Yufei and Xie, Yuan},
title = {ENMC: Extreme Near-Memory Classification via Approximate Screening},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480090},
doi = {10.1145/3466752.3480090},
abstract = {Extreme classification (XC) is the essential component of large-scale Deep Learning Systems for a wide range of application domains, including image recognition, language modeling, and recommendation. As classification categories keep scaling in real-world applications, the classifier’s parameters could reach several thousands of Gigabytes, way exceed the on-chip memory capacity. With the advent of near-memory processing (NMP) architectures, offloading the XC component onto NMP units could alleviate the memory-intensive problem. However, naive NMP design with limited area and power budget cannot afford the computational complexity of full classification. To tackle the problem, we first propose a novel screening method to reduce the computation and memory consumption by efficiently approximating the classification output and identifying a small portion of key candidates that require accurate results. Then, we design a new extreme-classification-tailored NMP architecture, namely ENMC, to support both screening and candidates-only classification. Overall, our approximate screening method achieves 7.3 \texttimes{} speedup over the CPU baseline, and ENMC further improves the performance by 7.4 \texttimes{} and demonstrates 2.7 \texttimes{} speedup compared with the state-of-the-art NMP baseline.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {1309–1322},
numpages = {14},
keywords = {Near-memory processing, Extreme classification},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480095,
author = {Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M. and Brooks, David and Wei, Gu-Yeon},
title = {EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480095},
doi = {10.1145/3466752.3480095},
abstract = {Transformer-based language models such as BERT provide significant accuracy improvement to a multitude of natural language processing (NLP) tasks. However, their hefty computational and memory demands make them challenging to deploy to resource-constrained edge platforms with strict latency requirements. We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware energy optimizations for multi-task NLP. EdgeBERT employs entropy-based early exit predication in order to perform dynamic voltage-frequency scaling (DVFS), at a sentence granularity, for minimal energy consumption while adhering to a prescribed target latency. Computation and memory footprint overheads are further alleviated by employing a calibrated combination of adaptive attention span, selective network pruning, and floating-point quantization. Furthermore, in order to maximize the synergistic benefits of these algorithms in always-on and intermediate edge computing settings, we specialize a 12nm scalable hardware accelerator system, integrating a fast-switching low-dropout voltage regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as, high-density embedded non-volatile memories (eNVMs) wherein the sparse floating-point bit encodings of the shared multi-task parameters are carefully stored. Altogether, latency-aware multi-task NLP inference acceleration on the EdgeBERT hardware system generates up to 7 \texttimes{}, 2.5 \texttimes{}, and 53 \texttimes{} lower energy compared to the conventional inference without early stopping, the latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson Tegra X2 mobile GPU, respectively.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {830–844},
numpages = {15},
keywords = {software and hardware co-design, embedded non-volatile memories, latency-aware, natural language processing},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480100,
author = {Avalos Baddouh, Cesar and Khairy, Mahmoud and Green, Roland N. and Payer, Mathias and Rogers, Timothy G.},
title = {Principal Kernel Analysis: A Tractable Methodology to Simulate Scaled GPU Workloads},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480100},
doi = {10.1145/3466752.3480100},
abstract = {Simulating all threads in a scaled GPU workload results in prohibitive simulation cost. Cycle-level simulation is orders of magnitude slower than native silicon, the only solution is to reduce the amount of work simulated while accurately representing the program. Existing solutions to simulate GPU programs either scale the input size, simulate the first several billion instructions, or simulate a portion of both the GPU and the workload. These solutions lack validation against scaled systems, produce unrealistic contention conditions and frequently miss critical code sections. Existing CPU sampling mechanisms, like SimPoint, reduce per-thread workload, and are ill-suited to GPU programs where reducing the number of threads is critical. Sampling solutions on GPUs space lack silicon validation, require per-workload parameter tuning, and do not scale. A tractable solution, validated on contemporary scaled workloads, is needed to provide credible simulation results. By studying scaled workloads with centuries-long simulation times, we uncover practical and algorithmic limitations of existing solutions and propose Principal Kernel Analysis: a hierarchical program sampling methodology that concisely represents GPU programs by selecting representative kernel portions using a scalable profiling methodology, tractable clustering algorithm and detection of intra-kernel IPC stability. We validate Principal Kernel Analysis across 147 workloads and three GPU generations using the Accel-Sim simulator, demonstrating a better performance/error tradeoff than prior work and that century-long MLPerf simulations are reduced to hours with an average cycle error of 27% versus silicon.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {724–737},
numpages = {14},
keywords = {GPU, Workload sampling, Simulation methodology},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480106,
author = {Awad, Omar Mohamed and Mahmoud, Mostafa and Edo, Isak and Zadeh, Ali Hadi and Bannon, Ciaran and Jayarajan, Anand and Pekhimenko, Gennady and Moshovos, Andreas},
title = {FPRaker: A Processing Element For Accelerating Neural Network Training},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480106},
doi = {10.1145/3466752.3480106},
abstract = {We present FPRaker, a processing element for composing training accelerators. FPRaker processes several floating-point multiply-accumulation operations concurrently and accumulates their result into a higher precision accumulator. FPRaker boosts performance and energy efficiency during training by taking advantage of the values that naturally appear during training. It processes the significand of the operands of each multiply-accumulate as a series of signed powers of two. The conversion to this form is done on-the-fly. This exposes ineffectual work that can be skipped: values when encoded have few terms and some of them can be discarded as they would fall outside the range of the accumulator given the limited precision of floating-point. FPRaker also takes advantage of spatial correlation in values across channels and uses delta-encoding off-chip to reduce memory footprint and bandwidth. We demonstrate that FPRaker can be used to compose an accelerator for training and that it can improve performance and energy efficiency compared to using optimized bit-parallel floating-point units under iso-compute area constraints. We also demonstrate that FPRaker delivers additional benefits when training incorporates pruning and quantization. Finally, we show that FPRaker naturally amplifies performance with training methods that use a different precision per layer.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {857–869},
numpages = {13},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480123,
author = {Lu, Hang and Chang, Liang and Li, Chenglong and Zhu, Zixuan and Lu, Shengjian and Liu, Yanhuan and Zhang, Mingzhe},
title = {Distilling Bit-Level Sparsity Parallelism for General Purpose Deep Learning Acceleration},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480123},
doi = {10.1145/3466752.3480123},
abstract = {Along with the rapid evolution of deep neural networks, the ever-increasing complexity imposes formidable computation intensity to the hardware accelerator. In this paper, we propose a novel computing philosophy called “bit interleaving” and the associate accelerator design called “Bitlet” to maximally exploit the bit-level sparsity. Apart from existing bit-serial/parallel accelerators, Bitlet leverages the abundant “sparsity parallelism” in the parameters to enforce the inference acceleration. Bitlet is versatile by supporting diverse precisions on a single platform, including floating-point 32 and fixed-point from 1b to 24b. The versatility enables Bitlet feasible for both efficient inference and training. Empirical studies on 12 domain-specific deep learning applications highlight the following results: (1) up to 81 \texttimes{} /21 \texttimes{} energy efficiency improvement for training/inference over recent high performance GPUs; (2) up to 15 \texttimes{} /8 \texttimes{} higher speedup/efficiency over state-of-the-art fixed-point accelerators; (3) 1.5mm2 area and scalable power consumption from 570mW (float32) to 432mW (16b) and 365mW (8b) @28nm TSMC; (4) highly configurable justified by ablation and sensitivity studies.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {963–976},
numpages = {14},
keywords = {neural network, bit-level sparsity, accelerator},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3466752.3480125,
author = {Lu, Liqiang and Jin, Yicheng and Bi, Hangrui and Luo, Zizhang and Li, Peng and Wang, Tao and Liang, Yun},
title = {Sanger: A Co-Design Framework for Enabling Sparse Attention Using Reconfigurable Architecture},
year = {2021},
isbn = {9781450385572},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3466752.3480125},
doi = {10.1145/3466752.3480125},
abstract = {In recent years, attention-based models have achieved impressive performance in natural language processing and computer vision applications by effectively capturing contextual knowledge from the entire sequence. However, the attention mechanism inherently contains a large number of redundant connections, imposing a heavy computational burden on model deployment. To this end, sparse attention has emerged as an attractive approach to reduce the computation and memory footprint, which involves the sampled dense-dense matrix multiplication (SDDMM) and sparse-dense matrix multiplication (SpMM) at the same time, thus requiring the hardware to eliminate zero-valued operations effectively. Existing techniques based on irregular sparse patterns or regular but coarse-grained patterns lead to low hardware efficiency or less computation saving. This paper proposes Sanger, a framework that harvests sparsity in the attention mechanism through synergistic hardware and software co-design. The software part prunes the attention matrix into a dynamic structured pattern, and the hardware part features a reconfigurable architecture that exploits such patterns. Specifically, we dynamically sparsify vanilla attention based on a quantized prediction of the attention matrix. Then, the sparse mask is re-arranged into structured blocks that are more amenable to hardware implementation. The hardware design of Sanger features a score-stationary dataflow that keeps sparse scores stationary in the PE to avoid decoding overhead. Using this dataflow and a reconfigurable systolic array design, we can unify the computation of SDDMM and SpMM operations. Typically, the PEs can be configured during runtime to support different data access and partial sum accumulation schemes. Experiments on BERT show that Sanger can prune the model to 0.08 - 0.27 sparsity without accuracy loss, achieving 4.64X, 22.7X, 2.39X, and 1.47X speedup compared to V100 GPU, AMD Ryzen Threadripper 3970X CPU, as well as the state-of-the-art attention accelerators A3 and SpAtten.},
booktitle = {MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {977–991},
numpages = {15},
keywords = {reconfigurable architecture, hardware-software co-design, systolic array, attention, sparse, Transformer},
location = {Virtual Event, Greece},
series = {MICRO '21}
}

@inproceedings{10.1145/3470496.3527378,
author = {Umar, Muhammad and Hua, Weizhe and Zhang, Zhiru and Suh, G. Edward},
title = {SoftVN: Efficient Memory Protection via Software-Provided Version Numbers},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527378},
doi = {10.1145/3470496.3527378},
abstract = {Trusted execution environments (TEEs) in processors protect off-chip memory (DRAM), and ensure its confidentiality and integrity using memory encryption and integrity verification. However, such memory protection can incur significant performance overhead as it requires additional memory accesses for protection metadata such as version numbers (VNs) and MACs. This paper proposes SoftVN, an extension to the current memory protection schemes, which significantly reduces the overhead of today's state-of-the-art by allowing software to provide VNs for memory accesses. For memory-intensive applications with simple memory access patterns for large data structures, the VNs only need to be maintained for data structures instead of individual cache blocks and can be tracked in software with low efforts. Off-chip VN accesses for memory reads can be removed if they are tracked and provided by software. We evaluate SoftVN by simulating a diverse set of memory-intensive applications, including deep learning, graph processing, and bioinformatics algorithms. The experimental results show that SoftVN reduces the memory protection overhead by 82% compared to the baseline similar to Intel SGX, and improves the performance by 33% on average. The maximum performance improvement can be as high as 65%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {160–172},
numpages = {13},
keywords = {memory protection, trusted execution environment (TEE)},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527382,
author = {Rashidi, Saeed and Won, William and Srinivasan, Sudarshan and Sridharan, Srinivas and Krishna, Tushar},
title = {Themis: A Network Bandwidth-Aware Collective Scheduling Policy for Distributed Training of DL Models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527382},
doi = {10.1145/3470496.3527382},
abstract = {Distributed training is a solution to reduce DNN training time by splitting the task across multiple NPUs (e.g., GPU/TPU). However, distributed training adds communication overhead between the NPUs in order to synchronize the gradients and/or activation, depending on the parallelization strategy. In next-generation platforms for training at scale, NPUs will be connected through multidimensional networks with diverse, heterogeneous bandwidths. This work identifies a looming challenge of keeping all network dimensions busy and maximizing the network BW within the hybrid environment if we leverage scheduling techniques for collective communication on systems today. We propose Themis, a novel collective scheduling scheme that dynamically schedules collectives (divided into chunks) to balance the communication loads across all dimensions, further improving the network BW utilization. Our results show that on average, Themis can improve the network BW utilization of the single All-Reduce by 1.72\texttimes{} (2.70\texttimes{} max), and improve the end-to-end training iteration performance of real workloads such as ResNet-152, GNMT, DLRM, and Transformer-1T by 1.49\texttimes{} (2.25\texttimes{} max), 1.30\texttimes{} (1.78\texttimes{} max), 1.30\texttimes{} (1.77\texttimes{} max), and 1.25\texttimes{} (1.53\texttimes{} max), respectively.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {581–596},
numpages = {16},
keywords = {bandwidth-aware communication scheduling, distributed training, collective communication},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527384,
author = {Lee, Jounghoo and Ha, Yeonan and Lee, Suhyun and Woo, Jinyoung and Lee, Jinho and Jang, Hanhwi and Kim, Youngsok},
title = {GCoM: A Detailed GPU Core Model for Accurate Analytical Modeling of Modern GPUs},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527384},
doi = {10.1145/3470496.3527384},
abstract = {Analytical models can greatly help computer architects perform orders of magnitude faster early-stage design space exploration than using cycle-level simulators. To facilitate rapid design space exploration for graphics processing units (GPUs), prior studies have proposed GPU analytical models which capture first-order stall events causing performance degradation; however, the existing analytical models cannot accurately model modern GPUs due to their outdated and highly abstract GPU core microarchitecture assumptions. Therefore, to accurately evaluate the performance of modern GPUs, we need a new GPU analytical model which accurately captures the stall events incurred by the significant changes in the core microarchitectures of modern GPUs.We propose GCoM, an accurate GPU analytical model which faithfully captures the key core-side stall events of modern GPUs. Through detailed microarchitecture-driven GPU core modeling, GCoM accurately models modern GPUs by revealing the following key core-side stalls overlooked by the existing GPU analytical models. First, GCoM identifies the compute structural stall events caused by the limited per-sub-core functional units. Second, GCoM exposes the memory structural stalls due to the limited banks and shared nature of per-core L1 data caches. Third, GCoM correctly predicts the memory data stalls induced by the sectored L1 data caches which split a cache line into a set of sectors sharing the same tag. Fourth, GCoM captures the idle stalls incurred by the inter- and intra-core load imbalances. Our experiments using an NVIDIA RTX 2060 configuration show that GCoM greatly improves the modeling accuracy by achieving a mean absolute error of 10.0% against Accel-Sim cycle-level simulator, whereas the state-of-the-art GPU analytical model achieves a mean absolute error of 44.9%.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {424–436},
numpages = {13},
keywords = {interval analysis, performance modeling, graphics processing units},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527396,
author = {Bleier, Nathaniel and Mubarik, Muhammad Husnain and Chakraborty, Srijan and Kishore, Shreyas and Kumar, Rakesh},
title = {Rethinking Programmable Earable Processors},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527396},
doi = {10.1145/3470496.3527396},
abstract = {Earables such as earphones [15, 16, 73], hearing aids [28], and smart glasses [2, 14] are poised to be a prominent programmable computing platform in the future. In this paper, we ask the question: what kind of programmable hardware would be needed to support earable computing in future? To understand hardware requirements, we propose EarBench, a suite of representative emerging earable applications with diverse sensor-based inputs and computation requirements. Our analysis of EarBench applications shows that, on average, there is a 13.54\texttimes{}-3.97\texttimes{} performance gap between the computational needs of EarBench applications and the performance of the microprocessors that several of today's programmable earable SoCs are based on; more complex microprocessors have unacceptable energy efficiency for Earable applications. Our analysis also shows that EarBench applications are dominated by a small number of digital signal processing (DSP) and machine learning (ML)-based kernels that have significant computational similarity. We propose SpEaC --- a coarse-grained reconfigurable spatial architecture - as an energy-efficient programmable processor for earable applications. SpEaC targets earable applications efficiently using a) a reconfigurable fixed-point multiply-and-add augmented reduction tree-based substrate with support for vectorized complex operations that is optimized for the earable ML and DSP kernel code and b) a tightly coupled control core for executing other code (including non-matrix computation, or non-multiply or add operations in the earable DSP kernel code). Unlike other CGRAs that typically target general-purpose computations, SpEaC substrate is optimized for energy-efficient execution of the earable kernels at the expense of generality. Across all our kernels, SpEaC outperforms programmable cores modeled after M4, M7, A53, and HiFi4 DSP by 99.3\texttimes{}, 32.5\texttimes{}, 14.8\texttimes{}, and 9.8\texttimes{} respectively. At 63 mW in 28 nm, the energy efficiency benefits are 1.55 \texttimes{}, 9.04\texttimes{}, 68.3 \texttimes{}, and 32.7 \texttimes{} respectively; energy efficiency benefits are 15.7 \texttimes{} -- 1087 \texttimes{} over a low power Mali T628 MP6 GPU.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {454–467},
numpages = {14},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527404,
author = {Lew, Jonathan S. and Liu, Yunpeng and Gong, Wenyi and Goli, Negar and Evans, R. David and Aamodt, Tor M.},
title = {Anticipating and Eliminating Redundant Computations in Accelerated Sparse Training},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527404},
doi = {10.1145/3470496.3527404},
abstract = {Deep Neural Networks (DNNs) are the state of art in image, speech, and text processing. To address long training times and high energy consumption, custom accelerators can exploit sparsity, that is zero-valued weights, activations, and gradients. Proposed sparse Convolution Neural Network (CNN) accelerators support training with no more than one dynamic sparse convolution input. Among existing accelerator classes, the only ones supporting two-sided dynamic sparsity are outer-product-based accelerators. However, when mapping a convolution onto an outer product, multiplications occur that do not correspond to any valid output. These Redundant Cartesian Products (RCPs) decrease energy efficiency and performance. We observe that in sparse training, up to 90% of computations are RCPs resulting from the convolution of large matrices for weight updates during the backward pass of CNN training.In this work, we design a mechanism, ANT, to anticipate and eliminate RCPs, enabling more efficient sparse training when integrated with an outer-product accelerator. By anticipating over 90% of RCPs, ANT achieves a geometric mean of 3.71\texttimes{} speed up over an SCNN-like accelerator [67] on 90% sparse training using DenseNet-121 [38], ResNet18 [35], VGG16 [73], Wide ResNet (WRN) [85], and ResNet-50 [35], with 4.40\texttimes{} decrease in energy consumption and 0.0017mm2 of additional area. We extend ANT to sparse matrix multiplication, so that the same accelerator can anticipate RCPs in sparse fully-connected layers, transformers, and RNNs.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {536–551},
numpages = {16},
keywords = {sparse CNN training, sparse matrix multiplication, hardware acceleration},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527405,
author = {Abts, Dennis and Kimmell, Garrin and Ling, Andrew and Kim, John and Boyd, Matt and Bitar, Andrew and Parmar, Sahil and Ahmed, Ibrahim and DiCecco, Roberto and Han, David and Thompson, John and Bye, Michael and Hwang, Jennifer and Fowers, Jeremy and Lillian, Peter and Murthy, Ashwin and Mehtabuddin, Elyas and Tekur, Chetan and Sohmers, Thomas and Kang, Kris and Maresh, Stephen and Ross, Jonathan},
title = {A Software-Defined Tensor Streaming Multiprocessor for Large-Scale Machine Learning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527405},
doi = {10.1145/3470496.3527405},
abstract = {We describe our novel commercial software-defined approach for large-scale interconnection networks of tensor streaming processing (TSP) elements. The system architecture includes packaging, routing, and flow control of the interconnection network of TSPs. We describe the communication and synchronization primitives of a bandwidth-rich substrate for global communication. This scalable communication fabric provides the backbone for large-scale systems based on a software-defined Dragonfly topology, ultimately yielding a parallel machine learning system with elasticity to support a variety of workloads, both training and inference. We extend the TSP's producer-consumer stream programming model to include global memory which is implemented as logically shared, but physically distributed SRAM on-chip memory. Each TSP contributes 220 MiBytes to the global memory capacity, with the maximum capacity limited only by the network's scale --- the maximum number of endpoints in the system. The TSP acts as both a processing element (endpoint) and network switch for moving tensors across the communication links. We describe a novel software-controlled networking approach that avoids the latency variation introduced by dynamic contention for network links. We describe the topology, routing and flow control to characterize the performance of the network that serves as the fabric for a large-scale parallel machine learning system with up to 10,440 TSPs and more than 2 TeraBytes of global memory accessible in less than 3 microseconds of end-to-end system latency.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {567–580},
numpages = {14},
keywords = {tensor streaming processor, dragonfly, software scheduling, machine learning},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527418,
author = {Hua, Weizhe and Umar, Muhammad and Zhang, Zhiru and Suh, G. Edward},
title = {MGX: Near-Zero Overhead Memory Protection for Data-Intensive Accelerators},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527418},
doi = {10.1145/3470496.3527418},
abstract = {This paper introduces MGX, a near-zero overhead memory protection scheme for hardware accelerators. MGX minimizes the performance overhead of off-chip memory encryption and integrity verification by exploiting the application-specific properties of the accelerator execution. In particular, accelerators tend to explicitly manage data movement between on-chip and off-chip memories. Therefore, the general memory access pattern of an accelerator can largely be determined for a given application. Exploiting these characteristics, MGX generates version numbers used in memory encryption and integrity verification using on-chip accelerator state rather than storing them in the off-chip memory; it also customizes the granularity of the memory protection to match the granularity used by the accelerator. To demonstrate the efficacy of MGX, we present an in-depth study of MGX for DNN and graph algorithms. Experimental results show that on average, MGX lowers the performance overhead of memory protection from 28% and 33% to 4% and 5% for DNN and graph processing accelerators in a wide range of benchmarks, respectively.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {726–741},
numpages = {16},
keywords = {secure accelerators, off-chip memory protection, version number generation, graph algorithms, neural networks},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527419,
author = {Hanson, Edward and Li, Shiyu and Li, Hai 'Helen' and Chen, Yiran},
title = {Cascading Structured Pruning: Enabling High Data Reuse for Sparse DNN Accelerators},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527419},
doi = {10.1145/3470496.3527419},
abstract = {Performance and efficiency of running modern Deep Neural Networks (DNNs) are heavily bounded by data movement. To mitigate the data movement bottlenecks, recent DNN inference accelerator designs widely adopt aggressive compression techniques and sparse-skipping mechanisms. These mechanisms avoid transferring or computing with zero-valued weights or activations to save time and energy. However, such sparse-skipping logic involves large input buffers and irregular data access patterns, thus precluding many energy-efficient data reuse opportunities and dataflows. In this work, we propose Cascading Structured Pruning (CSP), a technique that preserves significantly more data reuse opportunities for higher energy efficiency while maintaining comparable performance relative to recent sparse architectures such as SparTen. CSP includes the following two components: At algorithm level, CSP-A induces a predictable sparsity pattern that allows for low-overhead compression of weight data and sequential access to both activation and weight data. At architecture level, CSP-H leverages CSP-A's induced sparsity pattern with a novel dataflow to access unique activation data only once, thus removing the demand for large input buffers. Each CSP-H processing element (PE) employs a novel accumulation buffer design and a counter-based sparse-skipping mechanism to support the dataflow with minimum controller overhead. We verify our approach on several representative models. Our simulated results show that CSP achieves on average 15\texttimes{} energy efficiency improvement over SparTen with comparable or superior speedup under most evaluations.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {522–535},
numpages = {14},
keywords = {ML acceleration, low power microarchitecture, model compression, hardware/software co-design},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527423,
author = {Li, Zheng and Ghodrati, Soroush and Yazdanbakhsh, Amir and Esmaeilzadeh, Hadi and Kang, Mingu},
title = {Accelerating Attention through Gradient-Based Learned Runtime Pruning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527423},
doi = {10.1145/3470496.3527423},
abstract = {Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. This attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention, which is only determined at runtime. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. The main challenge is finding the threshold for the scores below which subsequent computation will be inconsequential. Although such a threshold is discrete, this paper formulates its search through a soft differentiable regularizer integrated into the loss function of the training. This formulation piggy backs on the back-propagation training to analytically co-optimize the threshold and the weights simultaneously, striking a formally optimal balance between accuracy and computation pruning. To best utilize this mathematical innovation, we devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism. We evaluate our design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision transformer models. Post-layout results show that, on average, LeOPArd yields 1.9\texttimes{}and 3.9\texttimes{}speedup and energy reduction, respectively, while keeping the average accuracy virtually intact (< 0.2% degradation).},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {902–915},
numpages = {14},
keywords = {self-attention, transformer, attention mechanism, learned pruning, neural processing units, accelerators, deep learning, gradient-based optimization},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527438,
author = {Zadeh, Ali Hadi and Mahmoud, Mostafa and Abdelhadi, Ameer and Moshovos, Andreas},
title = {Mokey: Enabling Narrow Fixed-Point Inference for out-of-the-Box Floating-Point Transformer Models},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527438},
doi = {10.1145/3470496.3527438},
abstract = {Increasingly larger and better Transformer models keep advancing state-of-the-art accuracy and capability for Natural Language Processing applications. These models demand more computational power, storage, and energy. Mokey reduces the footprint of state-of-the-art 32-bit or 16-bit floating-point transformer models by quantizing all values to 4-bit indexes into dictionaries of representative 16-bit fixed-point centroids. Mokey does not need fine-tuning, an essential feature as often the training resources or datasets are not available to many. Exploiting the range of values that naturally occur in transformer models, Mokey selects centroid values to also fit an exponential curve. This unique feature enables Mokey to replace the bulk of the original multiply-accumulate operations with narrow 3b fixed-point additions resulting in an area- and energy-efficient hardware accelerator design. Over a set of state-of-the-art transformer models, the Mokey accelerator delivers an order of magnitude improvements in energy efficiency over a Tensor Cores-based accelerator while improving performance by at least 4\texttimes{} and as much as 15\texttimes{} depending on the model and on-chip buffering capacity. Optionally, Mokey can be used as memory compression assist for any other accelerator transparently stashing wide floating-point or fixed-point activations or weights into narrow 4-bit indexes. Mokey proves superior to prior state-of-the-art quantization methods for Transformers.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {888–901},
numpages = {14},
keywords = {transformer models, natural language processing, quantization},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527440,
author = {Zheng, Size and Chen, Renze and Wei, Anjiang and Jin, Yicheng and Han, Qin and Lu, Liqiang and Wu, Bingyang and Li, Xiuhong and Yan, Shengen and Liang, Yun},
title = {AMOS: Enabling <U>A</U>Utomatic <U>M</U>Apping for Tensor Computations <U>O</U>n <U>S</U>Patial Accelerators with Hardware Abstraction},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527440},
doi = {10.1145/3470496.3527440},
abstract = {Hardware specialization is a promising trend to sustain performance growth. Spatial hardware accelerators that employ specialized and hierarchical computation and memory resources have recently shown high performance gains for tensor applications such as deep learning, scientific computing, and data mining. To harness the power of these hardware accelerators, programmers have to use specialized instructions with certain hardware constraints. However, these hardware accelerators and instructions are quite new and there is a lack of understanding of the hardware abstraction, performance optimization space, and automatic methodologies to explore the space. Existing compilers use hand-tuned computation implementations and optimization templates, resulting in sub-optimal performance and heavy development costs.In this paper, we propose AMOS, which is an automatic compilation framework for spatial hardware accelerators. Central to this framework is the hardware abstraction that not only clearly specifies the behavior of spatial hardware instructions, but also formally defines the mapping problem from software to hardware. Based on the abstraction, we develop algorithms and performance models to explore various mappings automatically. Finally, we build a compilation framework that uses the hardware abstraction as compiler intermediate representation (IR), explores both compute mappings and memory mappings, and generates high-performance code for different hardware backends. Our experiments show that AMOS achieves more than 2.50\texttimes{} speedup to hand-optimized libraries on Tensor Core, 1.37\texttimes{} speedup to TVM on vector units of Intel CPU for AVX-512, and up to 25.04\texttimes{} speedup to AutoTVM on dot units of Mali GPU. The source code of AMOS is publicly available.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {874–887},
numpages = {14},
keywords = {mapping, tensor computations, code generation, spatial accelerators},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3527444,
author = {Xu, Ceyu and Kjellqvist, Chris and Wills, Lisa Wu},
title = {SNS's Not a Synthesizer: A Deep-Learning-Based Synthesis Predictor},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527444},
doi = {10.1145/3470496.3527444},
abstract = {The number of transistors that can fit on one monolithic chip has reached billions to tens of billions in this decade thanks to Moore's Law. With the advancement of every technology generation, the transistor counts per chip grow at a pace that brings about exponential increase in design time, including the synthesis process used to perform design space explorations. Such a long delay in obtaining synthesis results hinders an efficient chip development process, significantly impacting time-to-market. In addition, these large-scale integrated circuits tend to have larger and higher-dimension design spaces to explore, making it prohibitively expensive to obtain physical characteristics of all possible designs using traditional synthesis tools.In this work, we propose a deep-learning-based synthesis predictor called SNS (SNS's not a Synthesizer), that predicts the area, power, and timing physical characteristics of a broad range of designs at two to three orders of magnitude faster than the Synopsys Design Compiler while providing on average a 0.4998 RRSE (root relative square error). We further evaluate SNS via two representative case studies, a general-purpose out-of-order CPU case study using RISC-V Boom open-source design and an accelerator case study using an in-house Chisel implementation of DianNao, to demonstrate the capabilities and validity of SNS.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {847–859},
numpages = {13},
keywords = {neural networks, RTL-level synthesis, integrated circuits},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3470496.3533040,
author = {Sankaralingam, Karthikeyan and Nowatzki, Tony and Gangadhar, Vinay and Shah, Preyas and Davies, Michael and Galliher, William and Guo, Ziliang and Khare, Jitu and Vijay, Deepak and Palamuttam, Poly and Punde, Maghawan and Tan, Alex and Thiruvengadam, Vijay and Wang, Rongyi and Xu, Shunmiao},
title = {The Mozart Reuse Exposed Dataflow Processor for AI and beyond: Industrial Product},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3533040},
doi = {10.1145/3470496.3533040},
abstract = {In this paper we introduce the Mozart Processor, which implements a new processing paradigm called Reuse Exposed Dataflow (RED). RED is a counterpart to existing execution models of Von-Neumann, SIMT, Dataflow, and FPGA. Dataflow and data reuse are the fundamental architecture primitives in RED, implemented with mechanisms for inter-worker communication and synchronization. The paper defines the processor architecture, the details of the microarchitecture, chip implementation, software stack development, and performance results. The architecture's goal is to achieve near-CPU like flexibility while having ASIC-like efficiency for a large-class of data-intensive workloads. An additional goal was software maturity --- have large coverage of applications immediately, avoiding the need for a long-drawn hand-tuning software development phase. The architecture was defined with this software-maturity/compiler friendliness in mind. In short, the goal was to do to GPUs, what GPUs did to CPUs --- i.e. be a better solution for a large range of workloads, while preserving flexibility and programmability. The chip was implemented with HBM and PCIe interfaces and taken to production on a 16nm TSMC FFC process. For ML inference tasks with batch-size=4, Mozart is integer factors better than state-of-the-art GPUs even while being nearly 2 technology nodes behind. We conclude with a set of lessons learned, the unique challenges of a clean-slate architecture in a commercial setting, and pointers for uncovered research problems.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {978–992},
numpages = {15},
keywords = {reuse, accelerator, machine learning, multicasting, chips, dataflow},
location = {New York, New York},
series = {ISCA '22}
}

@inproceedings{10.1145/3503222.3507708,
author = {Zhou, Keren and Hao, Yueming and Mellor-Crummey, John and Meng, Xiaozhu and Liu, Xu},
title = {ValueExpert: Exploring Value Patterns in GPU-Accelerated Applications},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507708},
doi = {10.1145/3503222.3507708},
abstract = {General-purpose GPUs have become common in modern computing systems to accelerate applications in many domains, including machine learning, high-performance computing, and autonomous driving. However, inefficiencies abound in GPU-accelerated applications, which prevent them from obtaining bare-metal performance. Performance tools play an important role in understanding performance inefficiencies in complex code bases. Many GPU performance tools pinpoint time-consuming code and provide high-level performance insights but overlook one important performance issue---value-related inefficiencies, which exist in many GPU code bases. In this paper, we present ValueExpert, a novel tool to pinpoint value-related inefficiencies in GPU applications. ValueExpert monitors application execution to capture values produced and used by each load and store operation in GPU kernels, recognizes multiple value patterns, and provides intuitive optimization guidance. We address systemic challenges in collecting, maintaining, and analyzing voluminous performance data from many GPU threads to make ValueExpert applicable to complex applications. We evaluate ValueExpert on a wide range of well-tuned benchmarks and applications, including PyTorch, Darknet, LAMMPS, Castro, and many others. ValueExpert is able to identify previously unknown performance issues and provide suggestions for nontrivial performance improvements with typically less than five lines of code changes. We verify our optimizations with application developers and upstream fixes to their repositories.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {171–185},
numpages = {15},
keywords = {Value Patterns, Value Analysis, GPUs, GPU profilers, Profiling Tools},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507709,
author = {Yang, Yanan and Zhao, Laiping and Li, Yiming and Zhang, Huanyu and Li, Jie and Zhao, Mingyang and Chen, Xingzhen and Li, Keqiu},
title = {INFless: A Native Serverless System for Low-Latency, High-Throughput Inference},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507709},
doi = {10.1145/3503222.3507709},
abstract = {Modern websites increasingly rely on machine learning (ML) to improve their business efficiency. Developing and maintaining ML services incurs high costs for developers. Although serverless systems are a promising solution to reduce costs, we find that the current general purpose serverless systems cannot meet the low latency, high throughput demands of ML services. While simply ”patching” general serverless systems does not resolve the problem completely, we propose that such a system should natively combine the features of inference with a serverless paradigm. We present INFless, the first ML domain-specific serverless platform. It provides a unified, heterogeneous resource abstraction between CPU and accelerators, and achieves high throughput using built-in batching and non-uniform scaling mechanisms. It also supports low latency through coordinated management of batch queuing time, execution time and coldstart rate. We evaluate INFless using both a local cluster testbed and a large-scale simulation. Experimental results show that INFless outperforms state-of-the-art systems by 2\texttimes{}-5\texttimes{} on system throughput, meeting the latency goals of ML services.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {768–781},
numpages = {14},
keywords = {Inference System, Serverless Computing, Machine Learning},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507721,
author = {Zhang, Wei and Chen, Quan and Fu, Kaihua and Zheng, Ningxin and Huang, Zhiyi and Leng, Jingwen and Guo, Minyi},
title = {Astraea: Towards QoS-Aware and Resource-Efficient Multi-Stage GPU Services},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507721},
doi = {10.1145/3503222.3507721},
abstract = {Multi-stage user-facing applications on GPUs are widely-used nowa- days, and are often implemented to be microservices. Prior re- search works are not applicable to ensuring QoS of GPU-based microservices due to the different communication patterns and shared resource contentions. We propose Astraea to manage GPU microservices considering the above factors. In Astraea, a microser- vice deployment policy is used to maximize the supported peak service load while ensuring the required QoS. To adaptively switch the communication methods between microservices according to different deployments, we propose an auto-scaling GPU communi- cation framework. The framework automatically scales based on the currently used hardware topology and microservice location, and adopts global memory-based techniques to reduce intra-GPU communication. Astraea increases the supported peak load by up to 82.3% while achieving the desired 99%-ile latency target compared with state-of-the-art solutions.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {570–582},
numpages = {13},
keywords = {Resource management, Microservice, GPU, QoS},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507722,
author = {Robson, Eyes and Xu, Ceyu and Wills, Lisa Wu},
title = {ProSE: The Architecture and Design of a Protein Discovery Engine},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507722},
doi = {10.1145/3503222.3507722},
abstract = {Protein language models have enabled breakthrough approaches to protein structure prediction, function annotation, and drug discovery. A primary limitation to the widespread adoption of these powerful models is the high computational cost associated with the training and inference of these models, especially at longer sequence lengths. We present the architecture, microarchitecture, and hardware implementation of a protein design and discovery accelerator, ProSE (Protein Systolic Engine). ProSE has a collection of custom heterogeneous systolic arrays and special functions that process transfer learning model inferences efficiently. The architecture marries SIMD-style computations with systolic array architectures, optimizing coarse-grained operation sequences across model layers to achieve efficiency without sacrificing generality. ProSE performs Protein BERT inference at up to 6.9\texttimes{} speedup and 48\texttimes{} power efficiency (performance/Watt) compared to one NVIDIA A100 GPU. ProSE achieves up to 5.5 \texttimes{} (12.7\texttimes{}) speedup and 173\texttimes{} (249\texttimes{}) power efficiency compared to TPUv3 (TPUv2).},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {655–668},
numpages = {14},
keywords = {Neural Networks, NLP, Transformers, Domain-Specific Architecture, Protein Design, Accelerators},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507723,
author = {Zheng, Zhen and Yang, Xuanda and Zhao, Pengzhan and Long, Guoping and Zhu, Kai and Zhu, Feiwen and Zhao, Wenyi and Liu, Xiaoyong and Yang, Jun and Zhai, Jidong and Song, Shuaiwen Leon and Lin, Wei},
title = {AStitch: Enabling a New Multi-Dimensional Optimization Space for Memory-Intensive ML Training and Inference on Modern SIMT Architectures},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507723},
doi = {10.1145/3503222.3507723},
abstract = {This work reveals that memory-intensive computation is a rising performance-critical factor in recent machine learning models. Due to a unique set of new challenges, existing ML optimizing compilers cannot perform efficient fusion under complex two-level dependencies combined with just-in-time demand. They face the dilemma of either performing costly fusion due to heavy redundant computation, or skipping fusion which results in massive number of kernels. Furthermore, they often suffer from low parallelism due to the lack of support for real-world production workloads with irregular tensor shapes. To address these rising challenges, we propose AStitch, a machine learning optimizing compiler that opens a new multi-dimensional optimization space for memory-intensive ML computations. It systematically abstracts four operator-stitching schemes while considering multi-dimensional optimization objectives, tackles complex computation graph dependencies with novel hierarchical data reuse, and efficiently processes various tensor shapes via adaptive thread mapping. Finally, AStitch provides just-in-time support incorporating our proposed optimizations for both ML training and inference. Although AStitch serves as a stand-alone compiler engine that is portable to any version of TensorFlow, its basic ideas can be generally applied to other ML frameworks and optimization compilers. Experimental results show that AStitch can achieve an average of 1.84x speedup (up to 2.73x) over the state-of-the-art Google's XLA solution across five production workloads. We also deploy AStitch onto a production cluster for ML workloads with thousands of GPUs. The system has been in operation for more than 10 months and saves about 20,000 GPU hours for 70,000 tasks per week.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {359–373},
numpages = {15},
keywords = {Memory-Intensive Computation, Compiler Optimization, Fusion, Machine Learning},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507735,
author = {Zhao, Shixiong and Li, Fanxin and Chen, Xusheng and Shen, Tianxiang and Chen, Li and Wang, Sen and Zhang, Nicholas and Li, Cheng and Cui, Heming},
title = {NASPipe: High Performance and Reproducible Pipeline Parallel Supernet Training via Causal Synchronous Parallelism},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507735},
doi = {10.1145/3503222.3507735},
abstract = {Supernet training, a prevalent and important paradigm in Neural Architecture Search, embeds the whole DNN architecture search space into one monolithic supernet, iteratively activates a subset of the supernet (i.e., a subnet) for fitting each batch of data, and searches a high-quality subnet which meets specific requirements. Although training subnets in parallel on multiple GPUs is desirable for acceleration, there inherently exists a race hazard that concurrent subnets may access the same DNN layers. Existing systems support neither efficiently parallelizing subnets’ training executions, nor resolving the race hazard deterministically, leading to unreproducible training procedures and potentiallly non-trivial accuracy loss. We present NASPipe, the first high-performance and reproducible distributed supernet training system via causal synchronous parallel (CSP) pipeline scheduling abstraction: NASPipe partitions a supernet across GPUs and concurrently executes multiple generated sub-tasks (subnets) in a pipelined manner; meanwhile, it oversees the correlations between the subnets and deterministically resolves any causal dependency caused by subnets’ layer sharing. To obtain high performance, NASPipe’s CSP scheduler exploits the fact that the larger a supernet spans, the fewer dependencies manifest between chronologically close subnets; therefore, it aggressively schedules the subnets with larger chronological orders into execution, only if they are not causally dependent on unfinished precedent subnets. Moreover, to relieve the excessive GPU memory burden for holding the whole supernet’s parameters, NASPipe uses a context switch technique that stashes the whole supernet in CPU memory, precisely predicts the subnets’ schedule, and pre-fetches/evicts a subnet before/after its execution. The evaluation shows that NASPipe is the only system that retains supernet training reproducibility, while achieving a comparable and even higher performance (up to 7.8X) compared to three recent pipeline training systems (e.g., GPipe).},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {374–387},
numpages = {14},
keywords = {neural architecture search, distributed training, one-shot, Neural networks, automatic machine learning, parallel training},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507738,
author = {Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan},
title = {DOTA: Detect and Omit Weak Attentions for Scalable Transformer Acceleration},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507738},
doi = {10.1145/3503222.3507738},
abstract = {Transformer Neural Networks have demonstrated leading performance in many applications spanning over language understanding, image processing, and generative modeling. Despite the impressive performance, long-sequence Transformer processing is expensive due to quadratic computation complexity and memory consumption of self-attention. In this paper, we present DOTA, an algorithm-architecture co-design that effectively addresses the challenges of scalable Transformer inference. Based on the insight that not all connections in an attention graph are equally important, we propose to jointly optimize a lightweight Detector with the Transformer model to accurately detect and omit weak connections during runtime. Furthermore, we design a specialized system architecture for end-to-end Transformer acceleration using the proposed attention detection mechanism. Experiments on a wide range of benchmarks demonstrate the superior performance of DOTA over other solutions. In summary, DOTA achieves 152.6x and 4.5x performance speedup and orders of magnitude energy-efficiency improvements over GPU and customized hardware, respectively.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {14–26},
numpages = {13},
keywords = {Sparse Architecture, SW-HW Co-design, Transformer Acceleration},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507752,
author = {Liu, Zihan and Leng, Jingwen and Zhang, Zhihui and Chen, Quan and Li, Chao and Guo, Minyi},
title = {VELTAIR: Towards High-Performance Multi-Tenant Deep Learning Services via Adaptive Compilation and Scheduling},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507752},
doi = {10.1145/3503222.3507752},
abstract = {Deep learning (DL) models have achieved great success in many application domains. As such, many industrial companies such as Google and Facebook have acknowledged the importance of multi-tenant DL services. Although the multi-tenant service has been studied in conventional workloads, it is not been deeply studied on deep learning service, especially on general-purpose hardware. In this work, we systematically analyze the opportunities and challenges of providing multi-tenant deep learning services on the general-purpose CPU architecture from the aspects of scheduling granularity and code generation. We propose an adaptive granularity scheduling scheme to both guarantee resource usage efficiency and reduce the scheduling conflict rate. We also propose an adaptive compilation strategy, by which we can dynamically and intelligently pick a program with proper exclusive and shared resource usage to reduce overall interference-induced performance loss. Compared to the existing works, our design can serve more requests under the same QoS target in various scenarios (e.g., +71%, +62%, +45% for light, medium, and heavy workloads, respectively), and reduce the averaged query latency by 50%.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {388–401},
numpages = {14},
keywords = {Multi-tenant, Compiling, Scheduling, Deep Learning Service},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507767,
author = {Zhang, Dan and Huda, Safeen and Songhori, Ebrahim and Prabhu, Kartik and Le, Quoc and Goldie, Anna and Mirhoseini, Azalia},
title = {A Full-Stack Search Technique for Domain Optimized Deep Learning Accelerators},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507767},
doi = {10.1145/3503222.3507767},
abstract = {The rapidly-changing deep learning landscape presents a unique opportunity for building inference accelerators optimized for specific datacenter-scale workloads. We propose Full-stack Accelerator Search Technique (FAST), a hardware accelerator search framework that defines a broad optimization environment covering key design decisions within the hardware-software stack, including hardware datapath, software scheduling, and compiler passes such as operation fusion and tensor padding. In this paper, we analyze bottlenecks in state-of-the-art vision and natural language processing (NLP) models, including EfficientNet and BERT, and use FAST to design accelerators capable of addressing these bottlenecks. FAST-generated accelerators optimized for single workloads improve Perf/TDP by 3.7\texttimes{} on average across all benchmarks compared to TPU-v3. A FAST-generated accelerator optimized for serving a suite of workloads improves Perf/TDP by 2.4\texttimes{} on average compared to TPU-v3. Our return on investment analysis shows that FAST-generated accelerators can potentially be practical for moderate-sized datacenter deployments.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {27–42},
numpages = {16},
keywords = {design space exploration, hardware-software codesign, machine learning, operation fusion, tensor processing unit},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3503222.3507778,
author = {Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
title = {Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507778},
doi = {10.1145/3503222.3507778},
abstract = {Recent trends towards large machine learning models require both training and inference tasks to be distributed. Considering the huge cost of training these models, it is imperative to unlock optimizations in computation and communication to obtain best performance. However, the current logical separation between computation and communication kernels in machine learning frameworks misses optimization opportunities across this barrier. Breaking this abstraction can provide many optimizations to improve the performance of distributed workloads. However, manually applying these optimizations requires modifying the underlying computation and communication libraries for each scenario, which is both time consuming and error-prone. Therefore, we present CoCoNet, which contains (i) a domain specific language to express a distributed machine learning program in the form of computation and communication operations, (ii) a set of semantics preserving transformations to optimize the program, and (iii) a compiler to generate jointly optimized communication and computation GPU kernels. Providing both computation and communication as first class constructs allows users to work on a high-level abstraction and apply powerful optimizations, such as fusion or overlapping of communication and computation. CoCoNet enabled us to optimize data-, model- and pipeline-parallel workloads in large language models with only a few lines of code. Our experiments show that CoCoNet significantly outperforms state-of-the-art distributed machine learning implementations.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {402–416},
numpages = {15},
keywords = {Collective Communication, CUDA, MPI, Compiler Optimizations, Code Generation, Distributed Machine Learning},
location = {Lausanne, Switzerland},
series = {ASPLOS '22}
}

@inproceedings{10.1145/3567955.3567959,
author = {Wang, Shibo and Wei, Jinliang and Sabne, Amit and Davis, Andy and Ilbeyi, Berkin and Hechtman, Blake and Chen, Dehao and Murthy, Karthik Srinivasa and Maggioni, Marcello and Zhang, Qiao and Kumar, Sameer and Guo, Tongfei and Xu, Yuanzhong and Zhou, Zongwei},
title = {Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models},
year = {2022},
isbn = {9781450399159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3567955.3567959},
doi = {10.1145/3567955.3567959},
abstract = {Large deep learning models have shown great potential with state-of-the-art results in many tasks. However, running these large models is quite challenging on an accelerator (GPU or TPU) because the on-device memory is too limited for the size of these models. Intra-layer model parallelism is an approach to address the issues by partitioning individual layers or operators across multiple devices in a distributed accelerator cluster. But, the data communications generated by intra-layer model parallelism can contribute to a significant proportion of the overall execution time and severely hurt the computational efficiency. As intra-layer model parallelism is critical to enable large deep learning models, this paper proposes a novel technique to effectively reduce its data communication overheads by overlapping communication with computation. With the proposed technique, an identified original communication collective is decomposed along with the dependent computation operation into a sequence of finer-grained operations. By creating more overlapping opportunities and executing the newly created, finer-grained communication and computation operations in parallel, it effectively hides the data transfer latency and achieves a better system utilization. Evaluated on TPU v4 Pods using different types of large models that have 10 billion to 1 trillion parameters, the proposed technique improves system throughput by 1.14 - 1.38x. The achieved highest peak FLOPS utilization is 72% on 1024 TPU chips with a large language model that has 500 billion parameters.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
pages = {93–106},
numpages = {14},
keywords = {Large scale machine learning, Compiler optimization, Collective communication hiding},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575698,
author = {Guo, Liwei and Choe, Wonkyo and Lin, Felix Xiaozhu},
title = {STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575698},
doi = {10.1145/3575693.3575698},
abstract = {Natural Language Processing (NLP) inference is seeing increasing adoption by mobile applications, where on-device inference is desirable for crucially preserving user data privacy and avoiding network roundtrips. Yet, the unprecedented size of an NLP model stresses both latency and memory, creating a tension between the two key resources of a mobile device. To meet a target latency, holding the whole model in memory launches execution as soon as possible but increases one app’s memory footprints by several times, limiting its benefits to only a few inferences before being recycled by mobile memory management. On the other hand, loading the model from storage on demand incurs IO as long as a few seconds, far exceeding the delay range satisfying to a user; pipelining layerwise model loading and execution does not hide IO either, due to the high skewness between IO and computation delays. To this end, we propose Speedy Transformer Inference (STI). Built on the key idea of maximizing IO/compute resource utilization on the most important parts of a model, STI reconciles the latency v.s. memory tension via two novel techniques. First, model sharding. STI manages model parameters as independently tunable shards, and profiles their importance to accuracy. Second, elastic pipeline planning with a preload buffer. STI instantiates an IO/compute pipeline and uses a small buffer for preload shards to bootstrap execution without stalling at early stages; it judiciously selects, tunes, and assembles shards per their importance for resource-elastic execution, maximizing inference accuracy. Atop two commodity SoCs, we build STI and evaluate it against a wide range of NLP tasks, under a practical range of target latencies, and on both CPU and GPU. We demonstrate that STI delivers high accuracies with 1–2 orders of magnitude lower memory, outperforming competitive baselines.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {791–803},
numpages = {13},
keywords = {Edge computing, Machine Learning Systems, NLP inference},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575702,
author = {Ding, Yaoyao and Yu, Cody Hao and Zheng, Bojian and Liu, Yizhi and Wang, Yida and Pekhimenko, Gennady},
title = {Hidet: Task-Mapping Programming Paradigm for Deep Learning Tensor Programs},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575702},
doi = {10.1145/3575693.3575702},
abstract = {As deep learning models nowadays are widely adopted by both cloud services and edge devices, reducing the latency of deep learning model inferences becomes crucial to provide efficient model serving. However, it is challenging to develop efficient tensor programs for deep learning operators due to the high complexity of modern accelerators (e.g., NVIDIA GPUs and Google TPUs) and the rapidly growing number of operators. Deep learning compilers, such as Apache TVM, adopt declarative scheduling primitives to lower the bar of developing tensor programs. However, we show that this approach is insufficient to cover state-of-the-art tensor program optimizations (e.g., double buffering). In this paper, we propose to embed the scheduling process into tensor programs and use dedicated mappings, called task mappings, to define the computation assignment and ordering directly in the tensor programs. This new approach greatly enriches the expressible optimizations by allowing developers to manipulate tensor programs at a much finer granularity (e.g., allowing program-statement-level optimizations). We call the proposed method the task-mapping programming paradigm. In addition, we propose a new post-scheduling fusion optimization that allows developers to focus on scheduling every single operator and automates the fusion after scheduling. It greatly reduces the engineering efforts for operator fusion. Our proposed paradigm also constructs an efficient hardware-centric schedule space, which is agnostic to the program input size and greatly reduces the tuning time. With the proposed paradigm, we implement a deep learning compiler Hidet. Extensive experiments on modern convolution and transformer models show that Hidet outperforms state-of-the-art DNN inference framework, ONNX Runtime, and compiler, TVM equipped with scheduler AutoTVM and Ansor, by up to 1.48x (1.22x on average). It also reduces the tuning time by 20x and 11x compared with AutoTVM and Ansor, respectively. We open-sourced hidet at https://www.github.com/hidet-org/hidet.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {370–384},
numpages = {15},
keywords = {deep learning systems, compilation, programming models, systems for machine learning, tensor computation},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575703,
author = {Feng, Yangyang and Xie, Minhui and Tian, Zijie and Wang, Shuo and Lu, Youyou and Shu, Jiwu},
title = {Mobius: Fine Tuning Large-Scale Models on Commodity GPU Servers},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575703},
doi = {10.1145/3575693.3575703},
abstract = {Fine-tuning on cheap commodity GPU servers makes large-scale deep learning models benefit more people. However, the low inter-GPU communication bandwidth and pressing communication contention on the commodity GPU server obstruct training efficiency. In this paper, we present Mobius, a communication-efficient system for fine tuning large-scale models on commodity GPU servers. The key idea is a novel pipeline parallelism scheme enabling heterogeneous memory for large-scale model training, while bringing fewer communications than existing systems. Mobius partitions the model into stages and carefully schedules them between GPU memory and DRAM to overlap communication with computation. It formulates pipeline execution into a mixed-integer program problem to find the optimal pipeline partition. It also features a new stage-to-GPU mapping method termed cross mapping, to minimize communication contention. Experiments on various scale models and GPU topologies show that Mobius significantly reduces the training time by 3.8-5.1\texttimes{} compared with the prior art.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {489–501},
numpages = {13},
keywords = {distributed training, Neural networks, parallel training},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575705,
author = {Hu, Qinghao and Zhang, Meng and Sun, Peng and Wen, Yonggang and Zhang, Tianwei},
title = {Lucid: A Non-Intrusive, Scalable and Interpretable Scheduler for Deep Learning Training Jobs},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575705},
doi = {10.1145/3575693.3575705},
abstract = {While recent deep learning workload schedulers exhibit excellent performance, it is arduous to deploy them in practice due to some substantial defects, including inflexible intrusive manner, exorbitant integration and maintenance cost, limited scalability, as well as opaque decision processes. Motivated by these issues, we design and implement Lucid, a non-intrusive deep learning workload scheduler based on interpretable models. It consists of three innovative modules. First, a two-dimensional optimized profiler is introduced for efficient job metric collection and timely debugging job feedback. Second, Lucid utilizes an indolent packing strategy to circumvent interference. Third, Lucid orchestrates resources based on estimated job priority values and sharing scores to achieve efficient scheduling. Additionally, Lucid promotes model performance maintenance and system transparent adjustment via a well-designed system optimizer. Our evaluation shows that Lucid reduces the average job completion time by up to 1.3\texttimes{} compared with state-of-the-art preemptive scheduler Tiresias. Furthermore, it provides explicit system interpretations and excellent scalability for practical deployment.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {457–472},
numpages = {16},
keywords = {Workload Scheduling, Cluster Management, Machine Learning},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575706,
author = {Li, Zhiyao and Li, Jiaxiang and Chen, Taijie and Niu, Dimin and Zheng, Hongzhong and Xie, Yuan and Gao, Mingyu},
title = {Spada: Accelerating Sparse Matrix Multiplication with Adaptive Dataflow},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575706},
doi = {10.1145/3575693.3575706},
abstract = {Sparse matrix-matrix multiplication (SpGEMM) is widely used in many scientific and deep learning applications. The highly irregular structures of SpGEMM limit its performance and efficiency on conventional computation platforms, and thus motivate a large body of specialized hardware designs. Existing SpGEMM accelerators only support specific types of rigid execution dataflow such as inner/output-product or row-based schemes. Each dataflow is only optimized for certain sparse patterns and fails to generalize with robust performance to the widely diverse SpGEMM workloads across various domains. We propose Spada, a combination of three novel techniques for SpGEMM accelerators to efficiently adapt to various sparse patterns. First, we describe a window-based adaptive dataflow that can be flexibly adapted to different modes to best match the data distributions and realize different reuse benefits. Then, our hardware architecture efficiently supports this dataflow template, with flexible, fast, and low-cost reconfigurability and effective load balancing features. Finally, we use a profiling-guided approach to detect the sparse pattern and determine the optimized dataflow mode to use, based on the key observations of sparse pattern similarity in nearby matrix regions. Our evaluation results demonstrate that Spada is able to match or exceed the best among three state-of-the-art SpGEMM accelerators, and avoid the performance degradation of the others if data distribution and dataflow mismatch. It achieves an average 1.44\texttimes{} speedup across a wide range of sparse matrices and compressed neural network models.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {747–761},
numpages = {15},
keywords = {sparse matrix multiplication, dataflow, hardware acceleration},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575707,
author = {Liu, Jiawei and Lin, Jinkun and Ruffy, Fabian and Tan, Cheng and Li, Jinyang and Panda, Aurojit and Zhang, Lingming},
title = {NNSmith: Generating Diverse and Valid Test Cases for Deep Learning Compilers},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575707},
doi = {10.1145/3575693.3575707},
abstract = {Deep-learning (DL) compilers such as TVM and TensorRT are increasingly being used to optimize deep neural network (DNN) models to meet performance, resource utilization and other requirements. Bugs in these compilers can result in models whose semantics differ from the original ones, producing incorrect results that corrupt the correctness of downstream applications. However, finding bugs in these compilers is challenging due to their complexity. In this work, we propose a new fuzz testing approach for finding bugs in deep-learning compilers. Our core approach consists of (i) generating diverse yet valid DNN test models that can exercise a large part of the compiler's transformation logic using light-weight operator specifications; (ii) performing gradient-based search to find model inputs that avoid any floating-point exceptional values during model execution, reducing the chance of missed bugs or false alarms; and (iii) using differential testing to identify bugs. We implemented this approach in NNSmith which has found 72 new bugs for TVM, TensorRT, ONNXRuntime, and PyTorch to date. Of these 58 have been confirmed and 51 have been fixed by their respective project maintainers.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {530–543},
numpages = {14},
keywords = {Compiler Testing, Deep Learning Compilers, Fuzzing},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575712,
author = {Song, Jaeyong and Yim, Jinkyu and Jung, Jaewon and Jang, Hongsun and Kim, Hyung-Jin and Kim, Youngsok and Lee, Jinho},
title = {Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575712},
doi = {10.1145/3575693.3575712},
abstract = {In training of modern large natural language processing (NLP) models, it has become a common practice to split models using 3D parallelism to multiple GPUs. Such technique, however, suffers from a high overhead of inter-node communication. Compressing the communication is one way to mitigate the overhead by reducing the inter-node traffic volume; however, the existing compression techniques have critical limitations to be applied for NLP models with 3D parallelism in that 1) only the data parallelism traffic is targeted, and 2) the existing compression schemes already harm the model quality too much. In this paper, we present Optimus-CC, a fast and scalable distributed training framework for large NLP models with aggressive communication compression. Optimus-CC differs from existing communication compression frameworks in the following ways: First, we compress pipeline parallel (inter-stage) traffic. In specific, we compress the inter-stage backpropagation and the embedding synchronization in addition to the existing data-parallel traffic compression methods. Second, we propose techniques to avoid the model quality drop that comes from the compression. We further provide mathematical and empirical analyses to show that our techniques can successfully suppress the compression error. Lastly, we analyze the pipeline and opt to selectively compress those traffic lying on the critical path. This further helps reduce the compression error. We demonstrate our solution on a GPU cluster, and achieve superior speedup from the baseline state-of-the-art solutions for distributed training without sacrificing the model quality.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {560–573},
numpages = {14},
keywords = {Communication Optimization, Pipeline Parallelism, Systems for Machine Learning, Distributed Systems, Gradient Compression, Large-scale NLP Training, 3D Parallelism},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575721,
author = {Gu, Diandian and Zhao, Yihao and Zhong, Yinmin and Xiong, Yifan and Han, Zhenhua and Cheng, Peng and Yang, Fan and Huang, Gang and Jin, Xin and Liu, Xuanzhe},
title = {ElasticFlow: An Elastic Serverless Training Platform for Distributed Deep Learning},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575721},
doi = {10.1145/3575693.3575721},
abstract = {This paper proposes ElasticFlow, an elastic serverless training platform for distributed deep learning. ElasticFlow provides a serverless interface with two distinct features: (i) users specify only the deep neural network (DNN) model and hyperparameters for a job, but not the number of GPUs; (ii) users specify the deadline for a job, but not the amount of time to occupy GPUs. In contrast to existing server-centric platforms, ElasticFlow provides performance guarantees in terms of meeting deadlines while alleviating tedious, low-level, and manual resource management for deep learning developers. The characteristics of distributed training introduce two challenges. First, the training throughput scales non-linearly with the number of GPUs. Second, the scaling efficiency is affected by worker placement. To address these challenges, we propose Minimum Satisfactory Share to capture the resource usage of training jobs to meet deadlines, and ElasticFlow performs admission control based on it. We develop a greedy algorithm that dynamically allocates resources to admitted jobs based on diminishing returns. We apply buddy allocation to worker placement to eliminate the effect of topology. Evaluation results on a cluster of 128 GPUs show that ElasticFlow increases the number of jobs that can meet their deadlines by 1.46–7.65\texttimes{} compared to existing solutions.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {266–280},
numpages = {15},
keywords = {Distributed Deep Learning, Serverless Computing, Cluster Scheduling, GPU Cluster},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575736,
author = {Jung, Jaehoon and Kim, Jinpyo and Lee, Jaejin},
title = {DeepUM: Tensor Migration and Prefetching in Unified Memory},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575736},
doi = {10.1145/3575693.3575736},
abstract = {Deep neural networks (DNNs) are continuing to get wider and deeper. As a result, it requires a tremendous amount of GPU memory and computing power. In this paper, we propose a framework called DeepUM that exploits CUDA Unified Memory (UM) to allow GPU memory oversubscription for DNNs. While UM allows memory oversubscription using a page fault mechanism, page migration introduces enormous overhead. DeepUM uses a new correlation prefetching technique to hide the page migration overhead. It is fully automatic and transparent to users. We also propose two optimization techniques to minimize the GPU fault handling time. We evaluate the performance of DeepUM using nine large-scale DNNs from MLPerf, PyTorch examples, and Hugging Face and compare its performance with six state-of-the-art GPU memory swapping approaches. The evaluation result indicates that DeepUM is very effective for GPU memory oversubscription and can handle larger models that other approaches fail to handle.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {207–221},
numpages = {15},
keywords = {deep learning, neural networks, data prefetching, unified memory, CUDA, device driver, runtime system},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575737,
author = {Zhai, Yi and Zhang, Yu and Liu, Shuo and Chu, Xiaomeng and Peng, Jie and Ji, Jianmin and Zhang, Yanyong},
title = {TLP: A Deep Learning-Based Cost Model for Tensor Program Tuning},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575737},
doi = {10.1145/3575693.3575737},
abstract = {Tensor program tuning is a non-convex objective optimization problem, to which search-based approaches have proven to be effective. At the core of the search-based approaches lies the design of the cost model. Though deep learning-based cost models perform significantly better than other methods, they still fall short and suffer from the following problems. First, their feature extraction heavily relies on expert-level domain knowledge in hardware architectures. Even so, the extracted features are often unsatisfactory and require separate considerations for CPUs and GPUs. Second, a cost model trained on one hardware platform usually performs poorly on another, a problem we call cross-hardware unavailability. In order to address these problems, we propose TLP and MTL-TLP. TLP is a deep learning-based cost model that facilitates tensor program tuning. Instead of extracting features from the tensor program itself, TLP extracts features from the schedule primitives. We treat schedule primitives as tensor languages. TLP is thus a Tensor Language Processing task. In this way, the task of predicting the tensor program latency through the cost model is transformed into a natural language processing (NLP) regression task. MTL-TLP combines Multi-Task Learning and TLP to cope with the cross-hardware unavailability problem. We incorporate these techniques into the Ansor framework and conduct detailed experiments. Results show that TLP can speed up the average search time by 9.1\texttimes{} and 3.0\texttimes{} on CPU and GPU workloads, respectively, compared to the state-of-the-art implementation. MTL-TLP can achieve a speed-up of 4.7\texttimes{} and 2.9\texttimes{} on CPU and GPU workloads, respectively, using only 7% of the target hardware data. To the best of our knowledge, TLP is the first tensor program cost model to extract features directly from schedule primitives, and MTL-TLP is the first open-sourced work that effectively addresses the cross-platform unavailability problem. The code is available at https://github.com/zhaiyi000/tlp.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {833–845},
numpages = {13},
keywords = {tensor program, deep Learning, cost model, compiler optimization},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3575747,
author = {Kao, Sheng-Chun and Subramanian, Suvinay and Agrawal, Gaurav and Yazdanbakhsh, Amir and Krishna, Tushar},
title = {FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575747},
doi = {10.1145/3575693.3575747},
abstract = {Attention mechanisms, primarily designed to capture pairwise correlations between words, have become the backbone of machine learning, expanding beyond natural language processing into other domains. This growth in adaptation comes at the cost of prohibitively large memory requirements and computational complexity, especially at higher number of input elements. This limitation is due to inherently limited data reuse opportunities and quadratic growth in memory footprints, leading to severe memory-boundedness and limited scalability of input elements. This work addresses these challenges by devising a tailored dataflow optimization, called FLAT, for attention mechanisms without altering their functionality. This dataflow processes costly attention operations through a unique fusion mechanism, transforming the memory footprint quadratic growth to merely a linear one. To realize the full potential of this bespoke mechanism, we propose a tiling approach to enhance the data reuse across attention operations. Our method both mitigates the off-chip bandwidth bottleneck as well as reduces the on-chip memory requirement. FLAT delivers 1.94x (1.76x) speedup and 49% and (42%) of energy savings compared to the state-of-the-art Edge (Cloud) accelerators with no customized dataflow optimization. When on-chip resources are scarce (20 KB-200 KB), FLAT yields, on average, 1.5x end-to-end latency reduction across a diverse range of conventional attention-based models with input sequence lengths ranging from 512-token to 64K-token. Our evaluations demonstrate that state-of-the-art DNN dataflow applied to attention operations reach the efficiency limit for inputs above 512 elements. In contrast, FLAT unblocks transformer models for inputs with up to 64K elements.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {295–310},
numpages = {16},
keywords = {Transformer, DNN Accelerators, Dataflow, Attention},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3575693.3576933,
author = {Feng, Siyuan and Hou, Bohan and Jin, Hongyi and Lin, Wuwei and Shao, Junru and Lai, Ruihang and Ye, Zihao and Zheng, Lianmin and Yu, Cody Hao and Yu, Yong and Chen, Tianqi},
title = {TensorIR: An Abstraction for Automatic Tensorized Program Optimization},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3576933},
doi = {10.1145/3575693.3576933},
abstract = {Deploying deep learning models on various devices has become an important topic. The wave of hardware specialization brings a diverse set of acceleration primitives for multi-dimensional ten- sor computations. These new acceleration primitives, along with the emerging machine learning models, bring tremendous engineering challenges. In this paper, we present TensorIR, a compiler abstraction for optimizing programs with these tensor computation primitives. TensorIR generalizes the loop nest representation used in existing machine learning compilers to bring tensor computation as the first-class citizen. Finally, we build an end-to-end framework on top of our abstraction to automatically optimize deep learning models for given tensor computation primitives. Experimental results show that TensorIR compilation automatically uses the tensor computation primitives for given hardware backends and delivers performance that is competitive to state-of-art hand-optimized systems across platforms.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {804–817},
numpages = {14},
keywords = {Deep Neural Network, Tensor Computation, Machine Learning Compiler},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@misc{https://doi.org/10.48550/arxiv.2111.02080,
  doi = {10.48550/ARXIV.2111.02080},
  
  url = {https://arxiv.org/abs/2111.02080},
  
  author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{bjerva-etal-2020-sigtyp,
    title = "{SIGTYP} 2020 Shared Task: Prediction of Typological Features",
    author = "Bjerva, Johannes  and
      Salesky, Elizabeth  and
      Mielke, Sabrina J.  and
      Chaudhary, Aditi  and
      Celano, Giuseppe G. A.  and
      Ponti, Edoardo Maria  and
      Vylomova, Ekaterina  and
      Cotterell, Ryan  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the Second Workshop on Computational Research in Linguistic Typology",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.sigtyp-1.1",
    doi = "10.18653/v1/2020.sigtyp-1.1",
    pages = "1--11",
    abstract = "Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world{'}s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.",
}

@article{Hart1994ToDS,
  title={To decode short cryptograms},
  author={George W. Hart},
  journal={Commun. ACM},
  year={1994},
  volume={37},
  pages={102-108},
  url={https://www.cs.rochester.edu/~brown/Crypto/reading/ShortCrypts.pdf},
}

@article{Olson2007RobustDA,
  title={Robust Dictionary Attack of Short Simple Substitution Ciphers},
  author={Edwin Olson},
  journal={Cryptologia},
  year={2007},
  volume={31},
  pages={332 - 342},
  abstract={Simple substitution ciphers are a class of puzzles often found in newspapers, in which each plaintext letter is mapped to a fixed ciphertext letter and spaces are preserved. In this article, a system for automatically solving them is described even when the ciphertext is too short for statistical analysis, and when the puzzle contains non-dictionary words. The approach is based around a dictionary attack; several important performance optimizations are described as well as effective techniques for dealing with non-dictionary words. Quantitative performance results for several variations of the approach and two other implementations are presented.},
  url={https://april.eecs.umich.edu/media/pdfs/olson2007crypt.pdf},
}

@inproceedings{Oranchak2008EvolutionaryAF,
  title={Evolutionary algorithm for decryption of monoalphabetic homophonic substitution ciphers encoded as constraint satisfaction problems},
  author={David Oranchak},
  booktitle={Annual Conference on Genetic and Evolutionary Computation},
  year={2008},
  url={http://gpbib.cs.ucl.ac.uk/gecco2008/docs/p1717.pdf},
}

@inproceedings {eacl2023,
       title = "Decipherment as Regression: Solving Historical Substitution Ciphers by Learning Symbol Recurrence Relations",
       author = "Kambhatla, Nishant and Born, Logan  and Sarkar, Anoop",
       booktitle = "Findings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: EACL 2023 Findings",
       month = jan,
       year = "2023",
       address = "Dubrovnik, Croatia",
       publisher = "Association for Computational Linguistics",
       url = "https://aclanthology.org/",
       abstract = "Solving substitution ciphers involves mapping sequences of cipher symbols to fluent text in a target language. This has conventionally been formulated as a search problem, to find the decipherment key using a character-level language model to constrain the search space. This work instead frames decipherment as a sequence prediction task, using a Transformer-based causal language model to learn recurrences between characters in a ciphertext. We introduce a novel technique for transcribing arbitrary substitution ciphers into a common \textit { recurrence encoding } . By leveraging this technique, we (i) create a large synthetic dataset of homophonic ciphers using random keys, and (ii) train a decipherment model that predicts the plaintext sequence given a recurrence-encoded ciphertext. Our method achieves strong results on synthetic 1:1 and homophonic ciphers, and cracks several real historic homophonic ciphers. Our analysis shows that the model learns recurrence relations between cipher symbols and recovers decipherment keys in its self-attention."
    } 

@inproceedings{mccarthy-etal-2020-measuring,
    title = "Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions",
    author = "McCarthy, Arya D.  and
      Williams, Adina  and
      Liu, Shijia  and
      Yarowsky, David  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.456",
    doi = "10.18653/v1/2020.emnlp-main.456",
    pages = "5664--5675",
    abstract = "A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages{'} gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.",
}

@inproceedings{rama-etal-2020-probing,
    title = "Probing Multilingual {BERT} for Genetic and Typological Signals",
    author = "Rama, Taraka  and
      Beinborn, Lisa  and
      Eger, Steffen",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.105",
    doi = "10.18653/v1/2020.coling-main.105",
    pages = "1214--1228",
    abstract = "We probe the layers in multilingual BERT (mBERT) for phylogenetic and geographic language signals across 100 languages and compute language distances based on the mBERT representations. We 1) employ the language distances to infer and evaluate language trees, finding that they are close to the reference family tree in terms of quartet tree distance, 2) perform distance matrix regression analysis, finding that the language distances can be best explained by phylogenetic and worst by structural factors and 3) present a novel measure for measuring diachronic meaning stability (based on cross-lingual representation variability) which correlates significantly with published ranked lists based on linguistic approaches. Our results contribute to the nascent field of typological interpretability of cross-lingual text representations.",
}

@inproceedings{dutta-chowdhury-etal-2020-understanding,
    title = "Understanding Translationese in Multi-view Embedding Spaces",
    author = "Dutta Chowdhury, Koel  and
      Espa{\~n}a-Bonet, Cristina  and
      van Genabith, Josef",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.532",
    doi = "10.18653/v1/2020.coling-main.532",
    pages = "6056--6062",
    abstract = "Recent studies use a combination of lexical and syntactic features to show that footprints of the source language remain visible in translations, to the extent that it is possible to predict the original source language from the translation. In this paper, we focus on embedding-based semantic spaces, exploiting departures from isomorphism between spaces built from original target language and translations into this target language to predict relations between languages in an unsupervised way. We use different views of the data {---} words, parts of speech, semantic tags and synsets {---} to track translationese. Our analysis shows that (i) semantic distances between original target language and translations into this target language can be detected using the notion of isomorphism, (ii) language family ties with characteristics similar to linguistically motivated phylogenetic trees can be inferred from the distances and (iii) with delexicalised embeddings exhibiting source-language interference most significantly, other levels of abstraction display the same tendency, indicating the lexicalised results to be not {``}just{''} due to possible topic differences between original and translated texts. To the best of our knowledge, this is the first time departures from isomorphism between embedding spaces are used to track translationese.",
}

@inproceedings{dutta-chowdhury-etal-2021-tracing,    title = "Tracing Source Language Interference in Translation with Graph-Isomorphism Measures",    author = "Dutta Chowdhury, Koel  and      Espa{\~n}a-Bonet, Cristina  and      van Genabith, Josef",    booktitle = "Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)",    month = sep,    year = "2021",    address = "Held Online",    publisher = "INCOMA Ltd.",    url = "https://aclanthology.org/2021.ranlp-1.43",    pages = "375--385",    abstract = "Previous research has used linguistic features to show that translations exhibit traces of source language interference and that phylogenetic trees between languages can be reconstructed from the results of translations into the same language. Recent research has shown that instances of translationese (source language interference) can even be detected in embedding spaces, comparing embeddings spaces of original language data with embedding spaces resulting from translations into the same language, using a simple Eigenvector-based divergence from isomorphism measure. To date, it remains an open question whether alternative graph-isomorphism measures can produce better results. In this paper, we (i) explore Gromov-Hausdorff distance, (ii) present a novel spectral version of the Eigenvector-based method, and (iii) evaluate all approaches against a broad linguistic typological database (URIEL). We show that language distances resulting from our spectral isomorphism approaches can reproduce genetic trees on a par with previous work without requiring any explicit linguistic information and that the results can be extended to non-Indo-European languages. Finally, we show that the methods are robust under a variety of modeling conditions.",}

@inproceedings{rabinovich-etal-2017-found,
    title = "Found in Translation: Reconstructing Phylogenetic Language Trees from Translations",
    author = "Rabinovich, Ella  and
      Ordan, Noam  and
      Wintner, Shuly",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1049",
    doi = "10.18653/v1/P17-1049",
    pages = "530--540",
    abstract = "Translation has played an important role in trade, law, commerce, politics, and literature for thousands of years. Translators have always tried to be invisible; ideal translations should look as if they were written originally in the target language. We show that traces of the source language remain in the translation product to the extent that it is possible to uncover the history of the source language by looking only at the translation. Specifically, we automatically reconstruct phylogenetic language trees from monolingual texts (translated from several source languages). The signal of the source language is so powerful that it is retained even after two phases of translation. This strongly indicates that source language interference is the most dominant characteristic of translated texts, overshadowing the more subtle signals of universal properties of translation.",
}

@inproceedings{yu-etal-2021-language,    title = "Language Embeddings for Typology and Cross-lingual Transfer Learning",    author = "Yu, Dian  and      He, Taiqi  and      Sagae, Kenji",    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",    month = aug,    year = "2021",    address = "Online",    publisher = "Association for Computational Linguistics",    url = "https://aclanthology.org/2021.acl-long.560",    doi = "10.18653/v1/2021.acl-long.560",    pages = "7210--7225",    abstract = "Cross-lingual language tasks typically require a substantial amount of annotated data or parallel translation data. We explore whether language representations that capture relationships among languages can be learned and subsequently leveraged in cross-lingual tasks without the use of parallel data. We generate dense embeddings for 29 languages using a denoising autoencoder, and evaluate the embeddings using the World Atlas of Language Structures (WALS) and two extrinsic tasks in a zero-shot setting: cross-lingual dependency parsing and cross-lingual natural language inference.",}

@article{Zhao2020ConstructingAF,
  title={Constructing a Family Tree of Ten Indo-European Languages with Delexicalized Cross-linguistic Transfer Patterns},
  author={Yuanyuan Zhao and Weiwei Sun and Xiaojun Wan},
  journal={ArXiv},
  year={2020},
  volume={abs/2007.09076}
}

@inproceedings{oncevay-etal-2020-bridging,
    title = "Bridging Linguistic Typology and Multilingual Machine Translation with Multi-View Language Representations",
    author = "Oncevay, Arturo  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.187",
    doi = "10.18653/v1/2020.emnlp-main.187",
    pages = "2391--2406",
    abstract = "Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other{'}s language characterisation. We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source. By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships. We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer. With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.",
}

@inproceedings{scholivet-etal-2019-typological,
    title = "Typological Features for Multilingual Delexicalised Dependency Parsing",
    author = "Scholivet, Manon  and
      Dary, Franck  and
      Nasr, Alexis  and
      Favre, Benoit  and
      Ramisch, Carlos",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1393",
    doi = "10.18653/v1/N19-1393",
    pages = "3919--3930",
    abstract = "The existence of universal models to describe the syntax of languages has been debated for decades. The availability of resources such as the Universal Dependencies treebanks and the World Atlas of Language Structures make it possible to study the plausibility of universal grammar from the perspective of dependency parsing. Our work investigates the use of high-level language descriptions in the form of typological features for multilingual dependency parsing. Our experiments on multilingual parsing for 40 languages show that typological information can indeed guide parsers to share information between similar languages beyond simple language identification.",
}

@article{beinborn-choenni-2020-semantic,
    title = "Semantic Drift in Multilingual Representations",
    author = "Beinborn, Lisa  and
      Choenni, Rochelle",
    journal = "Computational Linguistics",
    volume = "46",
    number = "3",
    month = sep,
    year = "2020",
    url = "https://aclanthology.org/2020.cl-3.2",
    doi = "10.1162/coli_a_00382",
    pages = "571--603",
    abstract = "Multilingual representations have mostly been evaluated based on their performance on specific tasks. In this article, we look beyond engineering goals and analyze the relations between languages in computational representations. We introduce a methodology for comparing languages based on their organization of semantic concepts. We propose to conduct an adapted version of representational similarity analysis of a selected set of concepts in computational multilingual representations. Using this analysis method, we can reconstruct a phylogenetic tree that closely resembles those assumed by linguistic experts. These results indicate that multilingual distributional representations that are only trained on monolingual text and bilingual dictionaries preserve relations between languages without the need for any etymological information. In addition, we propose a measure to identify semantic drift between language families. We perform experiments on word-based and sentence-based multilingual models and provide both quantitative results and qualitative examples. Analyses of semantic drift in multilingual representations can serve two purposes: They can indicate unwanted characteristics of the computational models and they provide a quantitative means to study linguistic phenomena across languages.",
}

@article{bjerva-etal-2019-language,
    title = "What Do Language Representations Really Represent?",
    author = {Bjerva, Johannes  and
      {\"O}stling, Robert  and
      Veiga, Maria Han  and
      Tiedemann, J{\"o}rg  and
      Augenstein, Isabelle},
    journal = "Computational Linguistics",
    volume = "45",
    number = "2",
    month = jun,
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J19-2006",
    doi = "10.1162/coli_a_00351",
    pages = "381--389",
    abstract = "A neural language model trained on a text corpus can be used to induce distributed representations of words, such that similar words end up with similar representations. If the corpus is multilingual, the same model can be used to learn distributed representations of languages, such that similar languages end up with similar representations. We show that this holds even when the multilingual corpus has been translated into English, by picking up the faint signal left by the source languages. However, just as it is a thorny problem to separate semantic from syntactic similarity in word representations, it is not obvious what type of similarity is captured by language representations. We investigate correlations and causal relationships between language representations learned from translations on one hand, and genetic, geographical, and several levels of structural similarity between languages on the other. Of these, structural similarity is found to correlate most strongly with language representation similarity, whereas genetic relationships{---}a convenient benchmark used for evaluation in previous work{---}appears to be a confounding factor. Apart from implications about translation effects, we see this more generally as a case where NLP and linguistic typology can interact and benefit one another.",
}

@inproceedings{Oncevay2019TowardsAM,
  title={Towards a Multi-view Language Representation: A Shared Space of Discrete and Continuous Language Features},
  author={Arturo Oncevay and Barry Haddow and Alexandra Birch},
  year={2019},
  url={https://www.semanticscholar.org/paper/Towards-a-Multi-view-Language-Representation%3A-A-of-Oncevay-Haddow/72d3b4fd389d0e80e57386aa6c98b8c7a2ad1e16},
}

@article{rabinovich-etal-2018-native,
    title = "Native Language Cognate Effects on Second Language Lexical Choice",
    author = "Rabinovich, Ella  and
      Tsvetkov, Yulia  and
      Wintner, Shuly",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1024",
    doi = "10.1162/tacl_a_00024",
    pages = "329--342",
    abstract = "We present a computational analysis of cognate effects on the spontaneous linguistic productions of advanced non-native speakers. Introducing a large corpus of highly competent non-native English speakers, and using a set of carefully selected lexical items, we show that the lexical choices of non-natives are affected by cognates in their native language. This effect is so powerful that we are able to reconstruct the phylogenetic language tree of the Indo-European language family solely from the frequencies of specific lexical items in the English of authors with various native languages. We quantitatively analyze non-native lexical choice, highlighting cognate facilitation as one of the important phenomena shaping the language of non-native speakers.",
}

@inproceedings{koehn-knight-2002-learning,
    title = "Learning a Translation Lexicon from Monolingual Corpora",
    author = "Koehn, Philipp  and
      Knight, Kevin",
    booktitle = "Proceedings of the {ACL}-02 Workshop on Unsupervised Lexical Acquisition",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W02-0902",
    doi = "10.3115/1118627.1118629",
    pages = "9--16",
    abstract = "This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora. We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency. Experimental results for the construction of a German-English noun lexicon are reported. Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.",
}
@inproceedings{bouchard-etal-2007-probabilistic,
    title = "A Probabilistic Approach to Diachronic Phonology",
    author = "Bouchard, Alexandre  and
      Liang, Percy  and
      Griffiths, Thomas  and
      Klein, Dan",
    booktitle = "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL})",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D07-1093",
    pages = "887--896",
}

@inproceedings{hauer2021HistoCrypt-dorabella,
 abstract = {The Dorabella cipher is a symbolic message written in 1897 by English composer Edward Elgar. We analyze the cipher using modern computational and statistical techniques. We consider several open questions: Is the underlying message natural language text or music? If it is language, what is the most likely language? Is Dorabella a simple substitution cipher? If so, why has nobody managed to produce a plausible decipherment? Are some unusual-looking patterns in the cipher likely to occur by chance? Can state-of-the-art algorithmic solvers decipher at least some words of the message? This work is intended as a contribution towards finding answers to these questions.},
 accepted = {2021-04-23},
 author = {Bradley Hauer and Colin Choi and Anirudh Sundar and Abram Hindle and Scott Smallwood and Grzegorz Kondrak},
 authors = {Bradley Hauer, Colin Choi, Anirudh Sundar, Abram Hindle, Scott Smallwood, Grzegorz Kondrak},
 booktitle = {The International Conference on Historical Cryptology (HistoCrypt 2021)},
 code = {hauer2021HistoCrypt-dorabella},
 date = {2021-09-20},
 funding = {NSERC Discovery},
 pagerange = {1--10},
 pages = {1--10},
 role = {Co-author},
 title = {Experimental Analysis of the Dorabella Cipher with Statistical Language Models},
 type = {inproceedings},
 url = {http://softwareprocess.ca/pubs/hauer2021HistoCrypt-dorabella.pdf},
 venue = {The International Conference on Historical Cryptology (HistoCrypt 2021)},
 year = {2021}
}
@inproceedings{gambardella-etal-2022-identifying,
    title = "Identifying Cleartext in Historical Ciphers",
    author = "Gambardella, Maria-Elena  and
      Megyesi, Beata  and
      Pettersson, Eva",
    booktitle = "Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lt4hala-1.1",
    pages = "1--9",
    abstract = "In historical encrypted sources we can find encrypted text sequences, also called ciphertext, as well as non-encrypted cleartexts written in a known language. While most of the cryptanalysis focuses on the decryption of ciphertext, cleartext is often overlooked although it can give us important clues about the historical interpretation and contextualisation of the manuscript. In this paper, we investigate to what extent we can automatically distinguish cleartext from ciphertext in historical ciphers and to what extent we are able to identify its language. The problem is challenging as cleartext sequences in ciphers are often short, up to a few words, in different languages due to historical code-switching. To identify the sequences and the language(s), we chose a rule-based approach and run 7 different models using historical language models on various ciphertexts.",
}
@inproceedings{corazza-etal-2022-contextual,
    title = "Contextual Unsupervised Clustering of Signs for Ancient Writing Systems",
    author = "Corazza, Michele  and
      Tamburini, Fabio  and
      Val{\'e}rio, Miguel  and
      Ferrara, Silvia",
    booktitle = "Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lt4hala-1.12",
    pages = "84--93",
    abstract = "The application of machine learning techniques to ancient writing systems is a relatively new idea, and it poses interesting challenges for researchers. One particularly challenging aspect is the scarcity of data for these scripts, which contrasts with the large amounts of data usually available when applying neural models to computational linguistics and other fields. For this reason, any method that attempts to work on ancient scripts needs to be ad-hoc and consider paleographic aspects, in addition to computational ones. Considering the peculiar characteristics of the script that we used is therefore be a crucial part of our work, as any solution needs to consider the particular nature of the writing system that it is applied to. In this work we propose a preliminary evaluation of a novel unsupervised clustering method on Cypro-Greek syllabary, a writing system from Cyprus. This evaluation shows that our method improves clustering performance using information about the attested sequences of signs in combination with an unsupervised model for images, with the future goal of applying the methodology to undeciphered writing systems from a related and typologically similar script.",
}
@inproceedings{vertan-prager-2022-inscription,
    title = "From Inscription to Semi-automatic Annotation of {M}aya Hieroglyphic Texts",
    author = "Vertan, Cristina  and
      Prager, Christian",
    booktitle = "Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lt4hala-1.16",
    pages = "114--118",
    abstract = "The Maya script is the only readable autochthonous writing system of the Americas and consists of more than 1000 word signs and syllables. It is only partially deciphered and is the subject of the project {``}Text Database and Dictionary of the Classic Maya{''} . Texts are recorded in TEI XML and on the basis of a digital sign and graph catalog, which are stored in the TextGrid virtual repository. Due to the state of decipherment, it is not possible to record hieroglyphic texts directly in phonemically transliterated values. The texts are therefore documented numerically using numeric sign codes based on Eric Thompson{'}s catalog of the Maya script. The workflow for converting numerical transliteration into textual form involves several steps, with variable solutions possible at each step. For this purpose, the authors have developed ALMAH {``}Annotator for the Linguistic Analysis of Maya Hieroglyphs{''}. The tool is a client application and allows semi-automatic generation of phonemic transliteration from numerical transliteration and enables multi-step linguistic annotation. Alternative readings can be entered, and two or more decipherment proposals can be processed in parallel. ALMAH is implemented in JAVA, is based on a graph-data model, and has a user-friendly interface.",
}
@inproceedings{chi-etal-2022-zinet,
    title = "{Z}i{N}et: {L}inking {C}hinese Characters Spanning Three Thousand Years",
    author = "Chi, Yang  and
      Giunchiglia, Fausto  and
      Shi, Daqian  and
      Diao, Xiaolei  and
      Li, Chuntao  and
      Xu, Hao",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.242",
    doi = "10.18653/v1/2022.findings-acl.242",
    pages = "3061--3070",
    abstract = "Modern Chinese characters evolved from 3,000 years ago. Up to now, tens of thousands of glyphs of ancient characters have been discovered, which must be deciphered by experts to interpret unearthed documents. Experts usually need to compare each ancient character to be examined with similar known ones in whole historical periods. However, it is inevitably limited by human memory and experience, which often cost a lot of time but associations are limited to a small scope. To help researchers discover glyph similar characters, this paper introduces ZiNet, the first diachronic knowledge base describing relationships and evolution of Chinese characters and words. In addition, powered by the knowledge of radical systems in ZiNet, this paper introduces glyph similarity measurement between ancient Chinese characters, which could capture similar glyph pairs that are potentially related in origins or semantics. Results show strong positive correlations between scores from the method and from human experts. Finally, qualitative analysis and implicit future applications are presented.",
}
@inproceedings{aldarrab-may-2022-segmenting,
    title = "Segmenting Numerical Substitution Ciphers",
    author = "Aldarrab, Nada  and
      May, Jonathan",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.44",
    pages = "706--714",
    abstract = "Deciphering historical substitution ciphers is a challenging problem. Example problems that have been previously studied include detecting cipher type, detecting plaintext language, and acquiring the substitution key for segmented ciphers. However, attacking unsegmented ciphers is still a challenging task. Segmentation (i.e. finding substitution units) is essential for cracking those ciphers. In this work, we propose the first automatic methods to segment those ciphers using Byte Pair Encoding (BPE) and unigram language models. Our methods achieve an average segmentation error of 2{\%} on 100 randomly-generated monoalphabetic ciphers and 27{\%} on 3 real historical homophonic ciphers. We also propose a method for solving non-deterministic ciphers with existing keys using a lattice and a pretrained language model. Our method leads to the full solution of the IA cipher; a real historical cipher that has not been fully solved until this work.",
}
@inproceedings{wang-etal-2022-breaking,
    title = "Breaking the Representation Bottleneck of {C}hinese Characters: Neural Machine Translation with Stroke Sequence Modeling",
    author = "Wang, Zhijun  and
      Liu, Xuebo  and
      Zhang, Min",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.434",
    pages = "6473--6484",
    abstract = "Existing research generally treats Chinese character as a minimum unit for representation. However, such Chinese character representation will suffer two bottlenecks: 1) Learning bottleneck, the learning cannot benefit from its rich internal features (e.g., radicals and strokes); and 2) Parameter bottleneck, each individual character has to be represented by a unique vector. In this paper, we introduce a novel representation method for Chinese characters to break the bottlenecks, namely StrokeNet, which represents a Chinese character by a Latinized stroke sequence (e.g., {``}凹 (concave){''} to {``}ajaie{''} and {``}凸 (convex){''} to {``}aeaqe{''}). Specifically, StrokeNet maps each stroke to a specific Latin character, thus allowing similar Chinese characters to have similar Latin representations. With the introduction of StrokeNet to neural machine translation (NMT), many powerful but not applicable techniques to non-Latin languages (e.g., shared subword vocabulary learning and ciphertext-based data augmentation) can now be perfectly implemented. Experiments on the widely-used NIST Chinese-English, WMT17 Chinese-English and IWSLT17 Japanese-English NMT tasks show that StrokeNet can provide a significant performance boost over the strong baselines with fewer model parameters, achieving 26.5 BLEU on the WMT17 Chinese-English task which is better than any previously reported results without using monolingual data. Code and scripts are freely available at https://github.com/zjwang21/StrokeNet.",
}
@inproceedings{born-etal-2022-sequence,
    title = "Sequence Models for Document Structure Identification in an Undeciphered Script",
    author = "Born, Logan  and
      Monroe, M.  and
      Kelley, Kathryn  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.620",
    pages = "9111--9121",
    abstract = "This work describes the first thorough analysis of {``}header{''} signs in proto-Elamite, an undeciphered script from 3100-2900 BCE. Headers are a category of signs which have been provisionally identified through painstaking manual analysis of this script by domain experts. We use unsupervised neural and statistical sequence modeling techniques to provide new and independent evidence for the existence of headers, without supervision from domain experts. Having affirmed the existence of headers as a legitimate structural feature, we next arrive at a richer understanding of their possible meaning and purpose by (i) examining which features predict their presence; (ii) identifying correlations between these features and other document properties; and (iii) examining cases where these features predict the presence of a header in texts where domain experts do not expect one (or vice versa). We provide more concrete processes for labeling headers in this corpus and a clearer justification for existing intuitions about document structure in proto-Elamite.",
}
@inproceedings{kambhatla-etal-2022-cipherdaug,
    title = "{C}ipher{DA}ug: Ciphertext based Data Augmentation for Neural Machine Translation",
    author = "Kambhatla, Nishant  and
      Born, Logan  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.17",
    doi = "10.18653/v1/2022.acl-long.17",
    pages = "201--218",
    abstract = "We propose a novel data-augmentation technique for neural machine translation based on ROT-$k$ ciphertexts. ROT-$k$ is a simple letter substitution cipher that replaces a letter in the plaintext with the $k$th letter after it in the alphabet. We first generate multiple ROT-$k$ ciphertexts using different values of $k$ for the plaintext which is the source side of the parallel data. We then leverage this enciphered training data along with the original parallel data via multi-source training to improve neural machine translation. Our method, CipherDAug, uses a co-regularization-inspired training procedure, requires no external data sources other than the original training data, and uses a standard Transformer to outperform strong data augmentation techniques on several datasets by a significant margin. This technique combines easily with existing approaches to data augmentation, and yields particularly strong results in low-resource settings.",
}
@article{luo-etal-2021-deciphering,
    title = "Deciphering Undersegmented Ancient Scripts Using Phonetic Prior",
    author = "Luo, Jiaming  and
      Hartmann, Frederik  and
      Santus, Enrico  and
      Barzilay, Regina  and
      Cao, Yuan",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.5",
    doi = "10.1162/tacl_a_00354",
    pages = "69--81",
    abstract = "Most undeciphered lost languages exhibit two characteristics that pose significant decipherment challenges: (1) the scripts are not fully segmented into words; (2) the closest known language is not determined. We propose a decipherment model that handles both of these challenges by building on rich linguistic constraints reflecting consistent patterns in historical sound change. We capture the natural phonological geometry by learning character embeddings based on the International Phonetic Alphabet (IPA). The resulting generative framework jointly models word segmentation and cognate alignment, informed by phonological constraints. We evaluate the model on both deciphered languages (Gothic, Ugaritic) and an undeciphered one (Iberian). The experiments show that incorporating phonetic geometry leads to clear and consistent gains. Additionally, we propose a measure for language closeness which correctly identifies related languages for Gothic and Ugaritic. For Iberian, the method does not show strong evidence supporting Basque as a related language, concurring with the favored position by the current scholarship.1",
}
@inproceedings{hauer-etal-2021-dorabella-cipher,
    title = "Dorabella Cipher as Musical Inspiration",
    author = "Hauer, Bradley  and
      Choi, Colin  and
      Hindle, Abram  and
      Smallwood, Scott  and
      Kondrak, Grzegorz",
    booktitle = "Proceedings of the Workshop on Speech and Music Processing 2021",
    month = dec,
    year = "2021",
    address = "NIT Silchar, India",
    publisher = "NLP Association of India (NLPAI)",
    url = "https://aclanthology.org/2021.smp-1.5",
    pages = "33--38",
    abstract = "The Dorabella cipher is an encrypted note of English composer Edward Elgar, which has defied decipherment attempts for more than a century. While most proposed solutions are English texts, we investigate the hypothe- sis that Dorabella represents enciphered music. We weigh the evidence in favor of and against the hypothesis, devise a simplified music nota- tion, and attempt to reconstruct a melody from the cipher. Our tools are n-gram models of mu- sic which we validate on existing music cor- pora enciphered using monoalphabetic substi- tution. By applying our methods to Dorabella, we produce a decipherment with musical qual- ities, which is then transformed via artful com- position into a listenable melody. Far from ar- guing that the end result represents the only true solution, we instead frame the process of decipherment as part of the composition pro- cess.",
}
@inproceedings{born-etal-2021-compositionality,
    title = "Compositionality of Complex Graphemes in the Undeciphered {P}roto-{E}lamite Script using Image and Text Embedding Models",
    author = "Born, Logan  and
      Kelley, Kathryn  and
      Monroe, M. Willis  and
      Sarkar, Anoop",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.362",
    doi = "10.18653/v1/2021.findings-acl.362",
    pages = "4136--4146",
    abstract = "We introduce a language modeling architecture which operates over sequences of images, or over multimodal sequences of images with associated labels. We use this architecture alongside other embedding models to investigate a category of signs called complex graphemes (CGs) in the undeciphered protoElamite script. We argue that CGs have meanings which are at least partly compositional, and we discover novel rules governing the construction of CGs. We find that a language model over sign images produces more interpretable results than a model over text or over sign images and text, which suggests that the names given to signs may be obscuring signals in the corpus. Our results reveal previously unknown regularities in proto-Elamite sign use that can inform future decipherment efforts, and our image-aware language model provides a novel way to abstract away from biases introduced by human annotators.",
}
@inproceedings{aldarrab-may-2021-sequence,
    title = "Can Sequence-to-Sequence Models Crack Substitution Ciphers?",
    author = "Aldarrab, Nada  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.561",
    doi = "10.18653/v1/2021.acl-long.561",
    pages = "7226--7235",
    abstract = "Decipherment of historical ciphers is a challenging problem. The language of the target plaintext might be unknown, and ciphertext can have a lot of noise. State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. We propose an end-to-end multilingual model for solving simple substitution ciphers. We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise.",
}
@inproceedings{chu-etal-2020-learning,
    title = "{L}earning to {P}ronounce {C}hinese {W}ithout a {P}ronunciation {D}ictionary",
    author = "Chu, Christopher  and
      Fang, Scot  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.458",
    doi = "10.18653/v1/2020.emnlp-main.458",
    pages = "5687--5693",
    abstract = "We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary. From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations. Using unsupervised methods, the program effectively deciphers writing into speech. Its token-level character-to-syllable accuracy is 89{\%}, which significantly exceeds the 22{\%} accuracy of prior work.",
}
@inproceedings{chu-etal-2020-solving,
    title = "{S}olving {H}istorical {D}ictionary {C}odes with a {N}eural {L}anguage {M}odel",
    author = "Chu, Christopher  and
      Valenti, Raphael  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.471",
    doi = "10.18653/v1/2020.emnlp-main.471",
    pages = "5845--5854",
    abstract = "We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress. We are able to decipher 75.1{\%} of the cipher-word tokens correctly.",
}
@inproceedings{ryskina-etal-2020-phonetic,
    title = "Phonetic and Visual Priors for Decipherment of Informal {R}omanization",
    author = "Ryskina, Maria  and
      Gormley, Matthew R.  and
      Berg-Kirkpatrick, Taylor",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.737",
    doi = "10.18653/v1/2020.acl-main.737",
    pages = "8308--8319",
    abstract = "Informal romanization is an idiosyncratic process used by humans in informal digital communication to encode non-Latin script languages into Latin character sets found on common keyboards. Character substitution choices differ between users but have been shown to be governed by the same main principles observed across a variety of languages{---}namely, character pairs are often associated through phonetic or visual similarity. We propose a noisy-channel WFST cascade model for deciphering the original non-Latin script from observed romanized text in an unsupervised fashion. We train our model directly on romanized data from two languages: Egyptian Arabic and Russian. We demonstrate that adding inductive bias through phonetic and visual priors on character mappings substantially improves the model{'}s performance on both languages, yielding results much closer to the supervised skyline. Finally, we introduce a new dataset of romanized Russian, collected from a Russian social network website and partially annotated for our experiments.",
}
@inproceedings{pettersson-megyesi-2019-matching,
    title = "Matching Keys and Encrypted Manuscripts",
    author = "Pettersson, Eva  and
      Megyesi, Beata",
    booktitle = "Proceedings of the 22nd Nordic Conference on Computational Linguistics",
    month = sep # "{--}" # oct,
    year = "2019",
    address = "Turku, Finland",
    publisher = {Link{\"o}ping University Electronic Press},
    url = "https://aclanthology.org/W19-6126",
    pages = "253--261",
    abstract = "Historical cryptology is the study of historical encrypted messages aiming at their decryption by analyzing the mathematical, linguistic and other coding patterns and their historical context. In libraries and archives we can find quite a lot of ciphers, as well as keys describing the method used to transform the plaintext message into a ciphertext. In this paper, we present work on automatically mapping keys to ciphers to reconstruct the original plaintext message, and use language models generated from historical texts to guess the underlying plaintext language.",
}
@inproceedings{born-etal-2019-sign,
    title = "Sign Clustering and Topic Extraction in {P}roto-{E}lamite",
    author = "Born, Logan  and
      Kelley, Kate  and
      Kambhatla, Nishant  and
      Chen, Carolyn  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 3rd Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",
    month = jun,
    year = "2019",
    address = "Minneapolis, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-2516",
    doi = "10.18653/v1/W19-2516",
    pages = "122--132",
    abstract = "We describe a first attempt at using techniques from computational linguistics to analyze the undeciphered proto-Elamite script. Using hierarchical clustering, n-gram frequencies, and LDA topic models, we both replicate results obtained by manual decipherment and reveal previously-unobserved relationships between signs. This demonstrates the utility of these techniques as an aid to manual decipherment.",
}
@inproceedings{vig-2019-multiscale,
    title = "A Multiscale Visualization of Attention in the Transformer Model",
    author = "Vig, Jesse",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-3007",
    doi = "10.18653/v1/P19-3007",
    pages = "37--42",
    abstract = "The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.",
}
@inproceedings{luo-etal-2019-neural,
    title = "Neural Decipherment via Minimum-Cost Flow: From {U}garitic to {L}inear {B}",
    author = "Luo, Jiaming  and
      Cao, Yuan  and
      Barzilay, Regina",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1303",
    doi = "10.18653/v1/P19-1303",
    pages = "3146--3155",
    abstract = "In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the model in unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. When applied to decipherment of Ugaritic, we achieve 5{\%} absolute improvement over state-of-the-art results. We also report first automatic results in deciphering Linear B, a syllabic language related to ancient Greek, where our model correctly translates 67.3{\%} of cognates.",
}
@inproceedings{cardenas-etal-2019-grounded,
    title = "A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages",
    author = "Cardenas, Ronald  and
      Lin, Ying  and
      Ji, Heng  and
      May, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1252",
    doi = "10.18653/v1/N19-1252",
    pages = "2428--2439",
    abstract = "Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a {`}ciphertext{'} and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger{'}s utility by incorporating it into a true {`}zero-resource{'} variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.",
}
@inproceedings{qian-etal-2019-learning,
    title = "Learning to Decipher Hate Symbols",
    author = "Qian, Jing  and
      ElSherief, Mai  and
      Belding, Elizabeth  and
      Wang, William Yang",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1305",
    doi = "10.18653/v1/N19-1305",
    pages = "3006--3015",
    abstract = "Existing computational models to understand hate speech typically frame the problem as a simple classification task, bypassing the understanding of hate symbols (e.g., 14 words, kigy) and their secret connotations. In this paper, we propose a novel task of deciphering hate symbols. To do this, we leveraged the Urban Dictionary and collected a new, symbol-rich Twitter corpus of hate speech. We investigate neural network latent context models for deciphering hate symbols. More specifically, we study Sequence-to-Sequence models and show how they are able to crack the ciphers based on context. Furthermore, we propose a novel Variational Decipher and show how it can generalize better to unseen hate symbols in a more challenging testing setting.",
}
@inproceedings{wu-etal-2018-decipherment,
    title = "Decipherment for Adversarial Offensive Language Detection",
    author = "Wu, Zhelun  and
      Kambhatla, Nishant  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5119",
    doi = "10.18653/v1/W18-5119",
    pages = "149--159",
    abstract = "Automated filters are commonly used by online services to stop users from sending age-inappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In this paper, we model the disguised messages as if they are produced by encrypting the original message using an invented cipher. We apply automatic decipherment techniques to decode the disguised malicious text, which can be then filtered using rules or classifiers. We provide experimental results on three different datasets and show that decipherment is an effective tool for this task.",
}
@article{naim-etal-2018-feature,
    title = "Feature-Based Decipherment for Machine Translation",
    author = "Naim, Iftekhar  and
      Riley, Parker  and
      Gildea, Daniel",
    journal = "Computational Linguistics",
    volume = "44",
    number = "3",
    month = sep,
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J18-3006",
    doi = "10.1162/coli_a_00326",
    pages = "525--546",
    abstract = "Orthographic similarities across languages provide a strong signal for unsupervised probabilistic transduction (decipherment) for closely related language pairs. The existing decipherment models, however, are not well suited for exploiting these orthographic similarities. We propose a log-linear model with latent variables that incorporates orthographic similarity features. Maximum likelihood training is computationally expensive for the proposed log-linear model. To address this challenge, we perform approximate inference via Markov chain Monte Carlo sampling and contrastive divergence. Our results show that the proposed log-linear model with contrastive divergence outperforms the existing generative decipherment models by exploiting the orthographic features. The model both scales to large vocabularies and preserves accuracy in low- and no-resource contexts.",
}
@inproceedings{kambhatla-etal-2018-decipherment,
    title = "Decipherment of Substitution Ciphers with Neural Language Models",
    author = "Kambhatla, Nishant  and
      Mansouri Bigvand, Anahita  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1102",
    doi = "10.18653/v1/D18-1102",
    pages = "869--874",
    abstract = "Decipherment of homophonic substitution ciphers using language models is a well-studied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram language models. The most widely used technique is the use of beam search with n-gram language models proposed by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural language model. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural language model. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.",
}
@inproceedings{ge-etal-2018-fine,
    title = "Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition",
    author = "Ge, Tao  and
      Dou, Qing  and
      Ji, Heng  and
      Cui, Lei  and
      Chang, Baobao  and
      Sui, Zhifang  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1271",
    doi = "10.18653/v1/D18-1271",
    pages = "2496--2506",
    abstract = "This paper proposes to study fine-grained coordinated cross-lingual text stream alignment through a novel information network decipherment paradigm. We use Burst Information Networks as media to represent text streams and present a simple yet effective network decipherment algorithm with diverse clues to decipher the networks for accurate text stream alignment. Experiments on Chinese-English news streams show our approach not only outperforms previous approaches on bilingual lexicon extraction from coordinated text streams but also can harvest high-quality alignments from large amounts of streaming data for endless language knowledge mining, which makes it promising to be a new paradigm for automatic language knowledge acquisition.",
}
@inproceedings{thaine-penn-2017-vowel,
    title = "Vowel and Consonant Classification through Spectral Decomposition",
    author = "Thaine, Patricia  and
      Penn, Gerald",
    booktitle = "Proceedings of the First Workshop on Subword and Character Level Models in {NLP}",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4112",
    doi = "10.18653/v1/W17-4112",
    pages = "82--91",
    abstract = "We consider two related problems in this paper. Given an undeciphered alphabetic writing system or mono-alphabetic cipher, determine: (1) which of its letters are vowels and which are consonants; and (2) whether the writing system is a vocalic alphabet or an abjad. We are able to show that a very simple spectral decomposition based on character co-occurrences provides nearly perfect performance with respect to answering both question types.",
}
@inproceedings{ganesan-etal-2017-protein,
    title = "Protein Word Detection using Text Segmentation Techniques",
    author = "Ganesan, Devi  and
      Tendulkar, Ashish V.  and
      Chakraborti, Sutanu",
    booktitle = "{B}io{NLP} 2017",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada,",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-2330",
    doi = "10.18653/v1/W17-2330",
    pages = "238--246",
    abstract = "Literature in Molecular Biology is abundant with linguistic metaphors. There have been works in the past that attempt to draw parallels between linguistics and biology, driven by the fundamental premise that proteins have a language of their own. Since word detection is crucial to the decipherment of any unknown language, we attempt to establish a problem mapping from natural language text to protein sequences at the level of words. Towards this end, we explore the use of an unsupervised text segmentation algorithm to the task of extracting {``}biological words{''} from protein sequences. In particular, we demonstrate the effectiveness of using domain knowledge to complement data driven approaches in the text segmentation task, as well as in its biological counterpart. We also propose a novel extrinsic evaluation measure for protein words through protein family classification.",
}
@inproceedings{pourdamghani-knight-2017-deciphering,
    title = "Deciphering Related Languages",
    author = "Pourdamghani, Nima  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1266",
    doi = "10.18653/v1/D17-1266",
    pages = "2513--2518",
    abstract = "We present a method for translating texts between close language pairs. The method does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs: Afrikaans/Dutch, Bosnian/Serbian, Danish/Swedish, Macedonian/Bulgarian, Malaysian/Indonesian, and Polish/Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data.",
}
@article{hauer-kondrak-2016-decoding,
    title = "Decoding Anagrammed Texts Written in an Unknown Language and Script",
    author = "Hauer, Bradley  and
      Kondrak, Grzegorz",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1006",
    doi = "10.1162/tacl_a_00084",
    pages = "75--86",
    abstract = "Algorithmic decipherment is a prime example of a truly unsupervised problem. The first step in the decipherment process is the identification of the encrypted language. We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher. The best method achieves 97{\%} accuracy on 380 languages. We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed. It obtains the average decryption word accuracy of 93{\%} on a set of 50 ciphertexts in 5 languages. Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document.",
}
@inproceedings{peterson-fyshe-2016-poet,
    title = "Poet Admits // Mute Cypher: Beam Search to find Mutually Enciphering Poetic Texts",
    author = "Peterson, Cole  and
      Fyshe, Alona",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1141",
    doi = "10.18653/v1/D16-1141",
    pages = "1339--1347",
    abstract = "The Xenotext Experiment implants poetry into an extremophile’s DNA, and uses that DNA to generate new poetry in a protein form. The molecular machinery of life requires that these two poems encipher each other under a symmetric substitution cipher. We search for ciphers which permit writing under the Xenotext constraints, incorporating ideas from cipher-cracking algorithms, and using n-gram data to assess a cipher’s “writability”. Our algorithm, Beam Verse, is a beam search which uses new heuristics to navigate the cipher-space. We find thousands of ciphers which score higher than successful ciphers used to write Xenotext constrained texts.",
}
@inproceedings{nuhn-etal-2015-unravel,
    title = "{UNRAVEL}{---}{A} Decipherment Toolkit",
    author = "Nuhn, Malte  and
      Schamper, Julian  and
      Ney, Hermann",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-2090",
    doi = "10.3115/v1/P15-2090",
    pages = "549--553",
    abstract = "In this paper we present the UNRAVEL toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers like e.g. the ZODIAC-408 cipher and Part two of the BEALE ciphers, as well as decipherment of probabilistic ciphers and unsupervised training for machine translation. It also includes data and example configuration files so that the previously published experiments are easy to reproduce.",
}
@inproceedings{dou-etal-2015-unifying,
    title = "Unifying {B}ayesian Inference and Vector Space Models for Improved Decipherment",
    author = "Dou, Qing  and
      Vaswani, Ashish  and
      Knight, Kevin  and
      Dyer, Chris",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1081",
    doi = "10.3115/v1/P15-1081",
    pages = "836--845",
    abstract = "We introduce into Bayesian decipherment a base distribution derived from similarities of word embeddings. We use Dirichlet multinomial regression (Mimno and McCallum, 2012) to learn a mapping between ciphertext and plaintext word embeddings from non-parallel data. Experimental results show that the base distribution is highly beneficial to decipherment, improving state-of-the-art decipherment accuracy from 45.8% to 67.4% for Spanish/English, and from 5.1% to 11.2% for Malagasy/English.",
}
@inproceedings{nuhn-ney-2014-em,
    title = "{EM} Decipherment for Large Vocabularies",
    author = "Nuhn, Malte  and
      Ney, Hermann",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2123",
    doi = "10.3115/v1/P14-2123",
    pages = "759--764",
    abstract = "This paper addresses the problem of EM-based decipherment for large vocabularies. Here, decipherment is essentially a tagging problem: Every cipher token is tagged with some plaintext type. As with other tagging problems, this one can be treated as a Hidden Markov Model (HMM), only here, the vocabularies are large, so the usual O(NV 2 ) exact EM approach is infeasible. When faced with this situation, many people turn to sampling. However, we propose to use a type of approximate EM and show that it works well. The basic idea is to collect fractional counts only over a small subset of links in the forward-backward lattice. The subset is different for each iteration of EM. One option is to use beam search to do the subsetting. The second method restricts the successor words that are looked at, for each hypothesis. It does this by consulting pre-computed tables of likely n-grams and likely substitutions.",
}
@inproceedings{dou-etal-2014-beyond,
    title = "Beyond Parallel Data: Joint Word Alignment and Decipherment Improves Machine Translation",
    author = "Dou, Qing  and
      Vaswani, Ashish  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1061",
    doi = "10.3115/v1/D14-1061",
    pages = "557--565",
    abstract = "Inspired by previous work, where decipherment is used to improve machine translation, we propose a new idea to combine word alignment and decipherment into a single learning process. We use EM to estimate the model parameters, not only to maximize the probability of parallel corpus, but also the monolingual corpus. We apply our approach to improve Malagasy-English machine translation, where only a small amount of parallel data is available. In our experiments, we observe gains of 0.9 to 2.1 Bleu over a strong baseline.",
}
@inproceedings{nuhn-etal-2014-improved,
    title = "Improved Decipherment of Homophonic Ciphers",
    author = "Nuhn, Malte  and
      Schamper, Julian  and
      Ney, Hermann",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1184",
    doi = "10.3115/v1/D14-1184",
    pages = "1764--1768",
    abstract = "In this paper, we present two improvements to the beam search approach for solving homophonic substitution ciphers presented in Nuhn et al. (2013): An improved rest cost estimation together with an optimized strategy for obtaining the order in which the symbols of the cipher are deciphered reduces the beam size needed to successfully decipher the Zodiac-408 cipher from several million down to less than one hundred: The search effort is reduced from several hours of computation time to just a few seconds on a single CPU. These improvements allow us to successfully decipher the second part of the famous Beale cipher (see (Ward et al., 1885) and e.g. (King, 1993)): Having 182 different cipher symbols while having a length of just 762 symbols, the decipherment is way more challenging than the decipherment of the previously deciphered Zodiac408 cipher (length 408, 54 different symbols). To the best of our knowledge, this cipher has not been deciphered automatically before.",
}
@inproceedings{nuhn-knight-2014-cipher,
    title = "Cipher Type Detection",
    author = "Nuhn, Malte  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1185",
    doi = "10.3115/v1/D14-1185",
    pages = "1769--1773",
    abstract = "Manual analysis and decryption of enciphered documents is a tedious and error prone work. Often—even after spending large amounts of time on a particular cipher—no decipherment can be found. Automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives, and to focus human effort only on a small but potentially interesting subset of them. In this work, we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext. We are able to distinguish 50 different cipher types (specified by the American Cryptogram Association) with an accuracy of 58.5%. This is a 11.2% absolute improvement over the best previously published classifier.",
}
@inproceedings{hauer-etal-2014-solving,
    title = "Solving Substitution Ciphers with Combined Language Models",
    author = "Hauer, Bradley  and
      Hayward, Ryan  and
      Kondrak, Grzegorz",
    booktitle = "Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
    month = aug,
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Dublin City University and Association for Computational Linguistics",
    url = "https://aclanthology.org/C14-1218",
    pages = "2314--2325",
    abstract = "We propose a novel approach to deciphering short monoalphabetic ciphers that combines both character-level and word-level language models. We formulate decipherment as tree search, and use Monte Carlo Tree Search (MCTS) as a fast alternative to beam search. Our experiments show a significant improvement over the state of the art on a benchmark suite of short ciphers. Our approach can also handle ciphers without spaces and ciphers with noise, which allows us to explore its applications to unsupervised transliteration and deniable encryption.",
}
@inproceedings{knight-2013-decipherment,
    title = "Decipherment",
    author = "Knight, Kevin",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-5003",
    pages = "3--4",
    abstract = "The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. This tutorial explores connections between early decipherment research and today’s NLP work. We cover classic military and diplomatic ciphers, automatic decipherment algorithms, unsolved ciphers, language translation as decipherment, and analyzing ancient writing as decipherment.",
}
@inproceedings{ravi-2013-scalable,
    title = "Scalable Decipherment for Machine Translation via Hash Sampling",
    author = "Ravi, Sujith",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1036",
    pages = "362--371",
    abstract = "In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocabulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).",
}
@inproceedings{nuhn-ney-2013-decipherment,
    title = "Decipherment Complexity in 1:1 Substitution Ciphers",
    author = "Nuhn, Malte  and
      Ney, Hermann",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1060",
    pages = "615--621",
    abstract = "In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.",
}
@inproceedings{nuhn-etal-2013-beam,
    title = "Beam Search for Solving Substitution Ciphers",
    author = "Nuhn, Malte  and
      Schamper, Julian  and
      Ney, Hermann",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1154",
    pages = "1568--1576",
    abstract = "In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly better (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.",
}
@inproceedings{berg-kirkpatrick-klein-2013-decipherment,
    title = "Decipherment with a Million Random Restarts",
    author = "Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1087",
    pages = "874--878",
    abstract = "This paper investigates the utility and effect of running numerous random restarts when using EM to attack decipherment problems. We find that simple decipherment models are able to crack homophonic substitution ciphers with high accuracy if a large number of random restarts are used but almost completely fail with only a few random restarts. For particularly difficult homophonic ciphers, we find that big gains in accuracy are to be had by running upwards of 100K random restarts, which we accomplish efficiently using a GPU-based parallel implementation. We run a series of experiments using millions of random restarts in order to investigate other empirical properties of decipherment problems, including the famously uncracked Zodiac 340.",
}
@inproceedings{dou-knight-2013-dependency,
    title = "Dependency-Based Decipherment for Resource-Limited Machine Translation",
    author = "Dou, Qing  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1173",
    pages = "1668--1676",
    abstract = "We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.",
}
@inproceedings{reddy-knight-2012-decoding,
    title = "Decoding Running Key Ciphers",
    author = "Reddy, Sravana  and
      Knight, Kevin",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-2016",
    pages = "80--84",
    abstract = "There has been recent interest in the problem of decoding letter substitution ciphers using techniques inspired by natural language processing. We consider a different type of classical encoding scheme known as the running key cipher, and propose a search solution using Gibbs sampling with a word language model. We evaluate our method on synthetic ciphertexts of different lengths, and find that it outperforms previous work that employs Viterbi decoding with character-based models.",
}
@inproceedings{nuhn-etal-2012-deciphering,
    title = "Deciphering Foreign Language by Combining Language Models and Context Vectors",
    author = "Nuhn, Malte  and
      Mauser, Arne  and
      Ney, Hermann",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-1017",
    pages = "156--164",
    abstract = "In this paper we show how to train statistical machine translation systems on reallife tasks using only non-parallel monolingual data from two languages. We present a modification of the method shown in (Ravi and Knight, 2011) that is scalable to vocabulary sizes of several thousand words. On the task shown in (Ravi and Knight, 2011) we obtain better results with only 5% of the computational effort when running our method with an n-gram language model. The efficiency improvement of our method allows us to run experiments with vocabulary sizes of around 5,000 words, such as a non-parallel version of the VERBMOBIL corpus. We also report results using data from the monolingual French and English GIGAWORD corpora.",
}
@inproceedings{dou-knight-2012-large,
    title = "Large Scale Decipherment for Out-of-Domain Machine Translation",
    author = "Dou, Qing  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D12-1025",
    pages = "266--275",
    abstract = "We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. We decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points.",
}
@inproceedings{knight-etal-2011-copiale,
    title = "The Copiale Cipher",
    author = "Knight, Kevin  and
      Megyesi, Be{\'a}ta  and
      Schaefer, Christiane",
    booktitle = "Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web",
    month = jun,
    year = "2011",
    address = "Portland, Oregon",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-1202",
    pages = "2--9",
    abstract = "The Copiale cipher is a 105-page enciphered book dated 1866. We describe the features of the book and the method by which we deciphered it.",
}
@inproceedings{ravi-knight-2011-deciphering,
    title = "Deciphering Foreign Language",
    author = "Ravi, Sujith  and
      Knight, Kevin",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1002",
    pages = "12--21",
    abstract = "In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text.",
}
@inproceedings{ravi-knight-2011-bayesian,
    title = "{B}ayesian Inference for Zodiac and Other Homophonic Ciphers",
    author = "Ravi, Sujith  and
      Knight, Kevin",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1025",
    pages = "239--247",
    abstract = "We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before.",
}
@inproceedings{berg-kirkpatrick-klein-2011-simple,
    title = "Simple Effective Decipherment via Combinatorial Optimization",
    author = "Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1029",
    pages = "313--321",
    abstract = "We present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems. The objective simultaneously scores a matching between two alphabets and a matching between two lexicons, each in a different language. We introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem. Our system requires only a list of words in both languages as input, yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information.",
}
@inproceedings{corlett-penn-2010-exact,
    title = "An Exact {A}* Method for Deciphering Letter-Substitution Ciphers",
    author = "Corlett, Eric  and
      Penn, Gerald",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1106",
    pages = "1040--1047",
    abstract = "Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment problems.",
}
@inproceedings{snyder-etal-2010-statistical,
    title = "A Statistical Model for Lost Language Decipherment",
    author = "Snyder, Benjamin  and
      Barzilay, Regina  and
      Knight, Kevin",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-1107",
    pages = "1048--1057",
    abstract = "In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and highlevel morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.",
}
@inproceedings{sinha-etal-2009-network,
    title = "Network analysis reveals structure indicative of syntax in the corpus of undeciphered Indus civilization inscriptions",
    author = "Sinha, Sitabhra  and
      Pan, Raj Kumar  and
      Yadav, Nisha  and
      Vahia, Mayank  and
      Mahadevan, Iravatham",
    booktitle = "Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing ({T}ext{G}raphs-4)",
    month = aug,
    year = "2009",
    address = "Suntec, Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W09-3202",
    pages = "5--13",
    abstract = "Archaeological excavations in the sites of the Indus Valley civilization (2500-1900 BCE) in Pakistan and northwestern India have unearthed a large number of artifacts with inscriptions made up of hundreds of distinct signs. To date, there is no generally accepted decipherment of these sign sequences, and there have been suggestions that the signs could be non-linguistic. Here we apply complex network analysis techniques on the database of available Indus inscriptions, with the aim of detecting patterns indicative of syntactic structure in this sign system. Our results show the presence of regularities, e.g., in the segmentation trees of the sequences, that suggest the existence of a grammar underlying the construction of the sequences.",
}
@inproceedings{knight-sproat-2009-writing,
    title = "Writing Systems, Transliteration and Decipherment",
    author = "Knight, Kevin  and
      Sproat, Richard",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Tutorial Abstracts",
    month = may,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N09-4008",
    pages = "15--16",
}
@inproceedings{ravi-knight-2008-attacking,
    title = "Attacking Decipherment Problems Optimally with Low-Order {N}-gram Models",
    author = "Ravi, Sujith  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2008",
    address = "Honolulu, Hawaii",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D08-1085",
    pages = "812--819",
    abstract = "We introduce a method for solving substitution ciphers using low-order letter n-gram models. This method enforces global constraints using integer programming, and it guarantees that no decipherment key is overlooked. We carry out extensive empirical experiments showing how decipherment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment.",
}
@inproceedings{knight-etal-2006-unsupervised,
    title = "Unsupervised Analysis for Decipherment Problems",
    author = "Knight, Kevin  and
      Nair, Anish  and
      Rathod, Nishit  and
      Yamada, Kenji",
    booktitle = "Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P06-2065",
    pages = "499--506",
    abstract = "We study a number of natural language decipherment problems using unsupervised learning. These include letter substitution ciphers, character code conversion, phonetic decipherment, and word-based ciphers with relevance to machine translation. Straightforward unsupervised learning techniques most often fail on the first try, so we describe techniques for understanding errors and significantly increasing performance.",
}
@inproceedings{knight-yamada-1999-computational,
    title = "A Computational Approach to Deciphering Unknown Scripts",
    author = "Knight, Kevin  and
      Yamada, Kenji",
    booktitle = "Unsupervised Learning in Natural Language Processing",
    year = "1999",
    url = "https://aclanthology.org/W99-0906",
    abstract = "We propose and evaluate computational techniques for deciphering unknown scripts. We focus on the case in which an unfamiliar script encodes a known language. The decipherment of a brief document or inscription is driven by data about the spoken language. We consider which scripts are easy or hard to decipher, how much data is required, and whether the techniques are robust against language change over time.",
}
@inproceedings{sukhotin-1988-optimization,
    title = "Optimization Algorithms of Deciphering as the Elements of a Linguistic Theory",
    author = "Sukhotin, B. V.",
    booktitle = "{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics",
    year = "1988",
    url = "https://aclanthology.org/C88-2134",
}
@inproceedings{sukhotin-1973-deciphering,
    title = "Deciphering Methods as a Means of Linguistic Research",
    author = "Sukhotin, B. V.",
    booktitle = "{COLING} 1973 Volume 1: Computational And Mathematical Linguistics: Proceedings of the International Conference on Computational Linguistics",
    year = "1973",
    url = "https://aclanthology.org/C73-1017",
}
@article{Jakobsen1995AFM,
  title={A Fast Method for Cryptanalysis of Substitution Ciphers},
  author={Thomas P. Jakobsen},
  journal={Cryptologia},
  year={1995},
  volume={19},
  pages={265-274}
}

@inproceedings{Kopal2019CryptanalysisOH,
  title={Cryptanalysis of Homophonic Substitution Ciphers Using Simulated Annealing with Fixed Temperature},
  author={Nils Kopal},
  booktitle={International Conference on Historical Cryptology},
  year={2019},
  url={https://www.ep.liu.se/ecp/158/012/ecp19158012.pdf},
}

@article{Dhavare2013EfficientCO,
  title={Efficient Cryptanalysis of Homophonic Substitution Ciphers},
  author={Amrapali Dhavare and Richard M. Low and Mark Stamp},
  journal={Cryptologia},
  year={2013},
  volume={37},
  pages={250 - 281},
  url={http://www.cs.sjsu.edu/faculty/stamp/RUA/homophonic.pdf},
}

@inproceedings{berg-kirkpatrick-klein-2013-decipherment,
    title = "Decipherment with a Million Random Restarts",
    author = "Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D13-1087",
    pages = "874--878",
}

@inproceedings{kambhatla-etal-2018-decipherment,
    title = "Decipherment of Substitution Ciphers with Neural Language Models",
    author = "Kambhatla, Nishant  and
      Mansouri Bigvand, Anahita  and
      Sarkar, Anoop",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1102",
    doi = "10.18653/v1/D18-1102",
    pages = "869--874",
    abstract = "Decipherment of homophonic substitution ciphers using language models is a well-studied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram language models. The most widely used technique is the use of beam search with n-gram language models proposed by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural language model. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural language model. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.",
}

@inproceedings{nuhn-etal-2012-deciphering,
    title = "Deciphering Foreign Language by Combining Language Models and Context Vectors",
    author = "Nuhn, Malte  and
      Mauser, Arne  and
      Ney, Hermann",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-1017",
    pages = "156--164",
}

@inproceedings{nuhn-ney-2014-em,
    title = "{EM} Decipherment for Large Vocabularies",
    author = "Nuhn, Malte  and
      Ney, Hermann",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P14-2123",
    doi = "10.3115/v1/P14-2123",
    pages = "759--764",
}

@inproceedings{nuhn-etal-2014-improved,
    title = "Improved Decipherment of Homophonic Ciphers",
    author = "Nuhn, Malte  and
      Schamper, Julian  and
      Ney, Hermann",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1184",
    doi = "10.3115/v1/D14-1184",
    pages = "1764--1768",
}

@inproceedings{nuhn-etal-2013-beam,
    title = "Beam Search for Solving Substitution Ciphers",
    author = "Nuhn, Malte  and
      Schamper, Julian  and
      Ney, Hermann",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P13-1154",
    pages = "1568--1576",
}

@inproceedings{nuhn-etal-2015-unravel,
    title = "{UNRAVEL}{---}{A} Decipherment Toolkit",
    author = "Nuhn, Malte  and
      Schamper, Julian  and
      Ney, Hermann",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-2090",
    doi = "10.3115/v1/P15-2090",
    pages = "549--553",
}

@inproceedings{chu-etal-2020-solving,
    title = "{S}olving {H}istorical {D}ictionary {C}odes with a {N}eural {L}anguage {M}odel",
    author = "Chu, Christopher  and
      Valenti, Raphael  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.471",
    doi = "10.18653/v1/2020.emnlp-main.471",
    pages = "5845--5854",
    abstract = "We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model. We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress. We are able to decipher 75.1{\%} of the cipher-word tokens correctly.",
}
@inproceedings{oh-etal-2021-surprisal,
    title = "Surprisal Estimators for Human Reading Times Need Character Models",
    author = "Oh, Byung-Doh  and
      Clark, Christian  and
      Schuler, William",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.290",
    doi = "10.18653/v1/2021.acl-long.290",
    pages = "3746--3757",
    abstract = "While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.",
}

@inproceedings{boldsen-etal-2022-interpreting,
    title = "Interpreting Character Embeddings With Perceptual Representations: The Case of Shape, Sound, and Color",
    author = "Boldsen, Sidsel  and
      Agirrezabal, Manex  and
      Hollenstein, Nora",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.470",
    doi = "10.18653/v1/2022.acl-long.470",
    pages = "6819--6836",
    abstract = "Character-level information is included in many NLP models, but evaluating the information encoded in character representations is an open issue. We leverage perceptual representations in the form of shape, sound, and color embeddings and perform a representational similarity analysis to evaluate their correlation with textual representations in five languages. This cross-lingual analysis shows that textual character representations correlate strongly with sound representations for languages using an alphabetic script, while shape correlates with featural scripts.We further develop a set of probing classifiers to intrinsically evaluate what phonological information is encoded in character embeddings. Our results suggest that information on features such as voicing are embedded in both LSTM and transformer-based representations.",
}

@inproceedings{alvarez-melis-jaakkola-2018-gromov,
    title = "Gromov-Wasserstein Alignment of Word Embedding Spaces",
    author = "Alvarez-Melis, David  and
      Jaakkola, Tommi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1214",
    doi = "10.18653/v1/D18-1214",
    pages = "1881--1890",
    abstract = "Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",
}

@mastersthesis{Deanne2021,
  author = {Charan, Deanne},
  title = {Generative Adversarial Networks for Classic Cryptanalysis},
  year = {2021},
  url = {https://scholarworks.sjsu.edu/etd_projects/1034/}
}

@inproceedings{Stamp2018HiddenMM,
  title={Hidden Markov Models for Vigen{\`e}re Cryptanalysis},
  author={Mark Stamp and Fabio Di Troia and Miles Stamp and Jasper Huang},
  booktitle={International Conference on Historical Cryptology},
  url={https://www.semanticscholar.org/paper/Hidden-Markov-Models-for-Vigen%C3%A8re-Cryptanalysis-Stamp-Troia/0fab7fabe5d3cf30ee615ca81e30d2ee3d45c295},
  year={2018}
}

@article{Hallman2022PosterEU,
  title={Poster EveGAN: Using Generative Deep Learning for Cryptanalysis},
  author={Roger A. Hallman},
  journal={Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},
  year={2022},
  url={https://www.semanticscholar.org/paper/Poster-EveGAN%3A-Using-Generative-Deep-Learning-for-Hallman/3e23dc5a5b4f8d0a888d201c1c59b1264945de20},
}

@inproceedings{Focardi2018NeuralCO,
  title={Neural Cryptanalysis of Classical Ciphers},
  author={Riccardo Focardi and Flaminia L. Luccio},
  booktitle={Italian Conference on Theoretical Computer Science},
  year={2018},
  url={https://www.semanticscholar.org/paper/Neural-Cryptanalysis-of-Classical-Ciphers-Focardi-Luccio/3768eacda976709580251a6d1de228393d974335}
}

@article{Gomez2018UnsupervisedCC,
  title={Unsupervised Cipher Cracking Using Discrete GANs},
  author={Aidan N. Gomez and Sicong Huang and Ivan Zhang and Bryan M. Li and Muhammad Osama and Lukasz Kaiser},
  journal={ArXiv},
  year={2018},
  volume={abs/1801.04883},
  url={https://www.semanticscholar.org/paper/Unsupervised-Cipher-Cracking-Using-Discrete-GANs-Gomez-Huang/b589b9f10258efb5404f06693c659d68e67929d3}
}

@inproceedings{gambardella-etal-2022-identifying,
    title = "Identifying Cleartext in Historical Ciphers",
    author = "Gambardella, Maria-Elena  and
      Megyesi, Beata  and
      Pettersson, Eva",
    booktitle = "Proceedings of the Second Workshop on Language Technologies for Historical and Ancient Languages",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lt4hala-1.1",
    pages = "1--9",
    abstract = "In historical encrypted sources we can find encrypted text sequences, also called ciphertext, as well as non-encrypted cleartexts written in a known language. While most of the cryptanalysis focuses on the decryption of ciphertext, cleartext is often overlooked although it can give us important clues about the historical interpretation and contextualisation of the manuscript. In this paper, we investigate to what extent we can automatically distinguish cleartext from ciphertext in historical ciphers and to what extent we are able to identify its language. The problem is challenging as cleartext sequences in ciphers are often short, up to a few words, in different languages due to historical code-switching. To identify the sequences and the language(s), we chose a rule-based approach and run 7 different models using historical language models on various ciphertexts.",
}

@article{10.2307/25678614,
 ISSN = {10724117, 19476213},
 URL = {http://www.jstor.org/stable/25678614},
 author = {Sarah Spence Adams},
 journal = {Math Horizons},
 number = {4},
 pages = {5--7},
 publisher = {Mathematical Association of America},
 title = {Historical Ciphers and Ancient Languages},
 urldate = {2023-01-26},
 volume = {13},
 year = {2006}
}

@misc{https://doi.org/10.48550/arxiv.2103.10360,
  doi = {10.48550/ARXIV.2103.10360},
  
  url = {https://arxiv.org/abs/2103.10360},
  
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2202.11705,
  doi = {10.48550/ARXIV.2202.11705},
  
  url = {https://arxiv.org/abs/2202.11705},
  
  author = {Qin, Lianhui and Welleck, Sean and Khashabi, Daniel and Choi, Yejin},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2109.06157,
  doi = {10.48550/ARXIV.2109.06157},
  
  url = {https://arxiv.org/abs/2109.06157},
  
  author = {Zhang, Michael J. Q. and Choi, Eunsol},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SituatedQA: Incorporating Extra-Linguistic Contexts into QA},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2210.13701,
  doi = {10.48550/ARXIV.2210.13701},
  
  url = {https://arxiv.org/abs/2210.13701},
  
  author = {Chen, Hung-Ting and Zhang, Michael J. Q. and Choi, Eunsol},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rich Knowledge Sources Bring Complex Knowledge Conflicts: Recalibrating Models to Reflect Conflicting Evidence},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2102.09690,
  doi = {10.48550/ARXIV.2102.09690},
  
  url = {https://arxiv.org/abs/2102.09690},
  
  author = {Zhao, Tony Z. and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Calibrate Before Use: Improving Few-Shot Performance of Language Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{wallace-etal-2022-automated,
    title = "Automated Crossword Solving",
    author = "Wallace, Eric  and
      Tomlin, Nicholas  and
      Xu, Albert  and
      Yang, Kevin  and
      Pathak, Eshaan  and
      Ginsberg, Matthew  and
      Klein, Dan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.219",
    doi = "10.18653/v1/2022.acl-long.219",
    pages = "3073--3085",
    abstract = "We present the Berkeley Crossword Solver, a state-of-the-art approach for automatically solving crossword puzzles. Our system works by generating answer candidates for each crossword clue using neural question answering models and then combines loopy belief propagation with local search to find full puzzle solutions. Compared to existing approaches, our system improves exact puzzle accuracy from 57{\%} to 82{\%} on crosswords from The New York Times and obtains 99.9{\%} letter accuracy on themeless puzzles. Our system also won first place at the top human crossword tournament, which marks the first time that a computer program has surpassed human performance at this event. To facilitate research on question answering and crossword solving, we analyze our system{'}s remaining errors and release a dataset of over six million question-answer pairs.",
}
@inproceedings{Knight2011TheCC,
  title={The Copiale Cipher},
  author={Kevin Knight and Be{\'a}ta Megyesi and Christiane Schaefer},
  booktitle={BUCC@ACL},
  year={2011},
  url={https://aclanthology.org/W11-1202.pdf},
}
@inproceedings{Knight2012TheSO,
  title={The Secrets of the Copiale Cipher},
  author={Kevin Knight and Be{\'a}ta Megyesi and Christiane Schaefer},
  year={2012},
  url={https://journal.equinoxpub.com/JRFF/article/view/7999},
}

@inproceedings{Reddy2012DecodingRK,
  title={Decoding Running Key Ciphers},
  author={Sravana Reddy and Kevin Knight},
  booktitle={ACL},
  year={2012},
  url={https://aclanthology.org/P12-2016.pdf},
}

@inproceedings{Dou2012LargeSD,
  title={Large Scale Decipherment for Out-of-Domain Machine Translation},
  author={Qing Dou and Kevin Knight},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2012},
  url={https://www.aclweb.org/anthology/D12-1025.pdf},
}

@inproceedings{Dou2013DependencyBasedDF,
  title={Dependency-Based Decipherment for Resource-Limited Machine Translation},
  author={Qing Dou and Kevin Knight},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2013},
  url={https://www.aclweb.org/anthology/D13-1173.pdf},
}

@mastersthesis{Hauer2016ComputationalDO,
  title={Computational Decipherment of Unknown Scripts},
  author={Bradley Hauer},
  year={2016},
  url={https://era.library.ualberta.ca/items/1ff57e8b-825a-40bb-b1b6-941c76f97a3f},
}

@phdthesis{Nuhn2019UnsupervisedTW,
  title={Unsupervised training with applications in natural language processing},
  author={Malte Nuhn},
  year={2019},
  url={http://publications.rwth-aachen.de/record/772331/files/772331.pdf}
}

@inproceedings{Leierzopf2021AMM,
  title={A Massive Machine-Learning Approach For Classical Cipher Type Detection Using Feature Engineering},
  author={Ernst Leierzopf and Nils Kopal and Bernhard Esslinger and Harald Lampesberger and Eckehard Hermann},
  booktitle={International Conference on Historical Cryptology},
  year={2021},
  url={https://ecp.ep.liu.se/index.php/histocrypt/article/view/164}
}

@inproceedings{Dou2014BeyondPD,
  title={Beyond Parallel Data: Joint Word Alignment and Decipherment Improves Machine Translation},
  author={Qing Dou and Ashish Vaswani and Kevin Knight},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url = {https://aclanthology.org/D14-1061.pdf},
  year={2014}
}

@inproceedings{Dou2015UnifyingBI,
  title={Unifying Bayesian Inference and Vector Space Models for Improved Decipherment},
  author={Qing Dou and Ashish Vaswani and Kevin Knight and Chris Dyer},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  url = {http://anthology.aclweb.org/P/P15/P15-1081.pdf},
  year={2015}
}

@inproceedings{pourdamghani-knight-2017-deciphering,
    title = "Deciphering Related Languages",
    author = "Pourdamghani, Nima  and
      Knight, Kevin",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1266",
    doi = "10.18653/v1/D17-1266",
    pages = "2513--2518",
    abstract = "We present a method for translating texts between close language pairs. The method does not require parallel data, and it does not require the languages to be written in the same script. We show results for six language pairs: Afrikaans/Dutch, Bosnian/Serbian, Danish/Swedish, Macedonian/Bulgarian, Malaysian/Indonesian, and Polish/Belorussian. We report BLEU scores showing our method to outperform others that do not use parallel data.",
}

@article{Yin2018DeciphermentOH,
  title={Decipherment of Historical Manuscript Images},
  author={Xusen Yin and Nada Aldarrab and Be{\'a}ta Megyesi and Kevin Knight},
  journal={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  url = {https://arxiv.org/abs/1810.04297},
  year={2018},
  pages={78-85}
}

@mastersthesis{aldarrab_thesis_decipherment,
  url = {https://digitallibrary.usc.edu/Share/61b761151l1kvxj4434b7d1pqovm044r},  
  author = {Nada Aldarrab},  
  title = {Automatic decipherment of historical manuscripts},
  year = {2022},  
}

@phdthesis{dou_thesis_decipherment,
  url = {https://digitallibrary.usc.edu/Share/61b761151l1kvxj4434b7d1pqovm044r},  
  author = {Qing Dou},  
  title = {Beyond parallel data: decipherment for better quality machine translation},
  year = {2015},  
}

@phdthesis{ravi_thesis_deciphering_natural_language,
  url = {https://digitallibrary.usc.edu/Share/ib8lk8pl147gtfg145ed40p13il7o65v},  
  author = {Sujith Ravi},  
  title = {Deciphering natural language},
  year = {2011},  
}

@misc{knight_decipherment_tutorial,
  url = {https://kevincrawfordknight.github.io/extra/acl-tutorial-13-decipher-final.pdf},  
  author = {Kevin Knight},  
  title = {ACL Tutorial on Decipherment},  
  year = {2013},  
}


﻿@Article{Assael2022,
author={Assael, Yannis
and Sommerschield, Thea
and Shillingford, Brendan
and Bordbar, Mahyar
and Pavlopoulos, John
and Chatzipanagiotou, Marita
and Androutsopoulos, Ion
and Prag, Jonathan
and de Freitas, Nando},
title={Restoring and attributing ancient texts using deep neural networks},
journal={Nature},
year={2022},
month={Mar},
day={01},
volume={603},
number={7900},
pages={280-283},
abstract={Ancient history relies on disciplines such as epigraphy---the study of inscribed texts known as inscriptions---for evidence of the thought, language, society and history of past civilizations1. However, over the centuries, many inscriptions have been damaged to the point of illegibility, transported far from their original location and their date of writing is steeped in uncertainty. Here we present Ithaca, a deep neural network for the textual restoration, geographical attribution and chronological attribution of ancient Greek inscriptions. Ithaca is designed to assist and expand the historian's workflow. The architecture of Ithaca focuses on collaboration, decision support and interpretability. While Ithaca alone achieves 62{\%} accuracy when restoring damaged texts, the use of Ithaca by historians improved their accuracy from 25{\%} to 72{\%}, confirming the synergistic effect of this research tool. Ithaca can attribute inscriptions to their original location with an accuracy of 71{\%} and can date them to less than 30{\thinspace}years of their ground-truth ranges, redating key texts of Classical Athens and contributing to topical debates in ancient history. This research shows how models such as Ithaca can unlock the cooperative potential between artificial intelligence and historians, transformationally impacting the way that we study and write about one of the most important periods in human history.},
issn={1476-4687},
doi={10.1038/s41586-022-04448-z},
url={https://doi.org/10.1038/s41586-022-04448-z}
}



@misc{https://doi.org/10.48550/arxiv.2101.00121,
  doi = {10.48550/ARXIV.2101.00121},
  
  url = {https://arxiv.org/abs/2101.00121},
  
  author = {Hambardzumyan, Karen and Khachatrian, Hrant and May, Jonathan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {WARP: Word-level Adversarial ReProgramming},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{gowda-may-2020-finding,
    title = "Finding the Optimal Vocabulary Size for Neural Machine Translation",
    author = "Gowda, Thamme  and
      May, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.352",
    doi = "10.18653/v1/2020.findings-emnlp.352",
    pages = "3955--3964",
    abstract = "We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes, and reveal an explanation for why certain vocabulary sizes are better than others.",
}

@misc{https://doi.org/10.48550/arxiv.2211.08684,
  doi = {10.48550/ARXIV.2211.08684},
  
  url = {https://arxiv.org/abs/2211.08684},
  
  author = {He, Andre and Tomlin, Nicholas and Klein, Dan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Unsupervised Reconstruction of Protolanguage Word Forms},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@inproceedings{Knight2011TheCC,
  title={The Copiale Cipher},
  author={Kevin Knight and Be{\'a}ta Megyesi and Christiane Schaefer},
  booktitle={BUCC@ACL},
  year={2011},
url={https://aclanthology.org/W11-1202.pdf},
}
@article{hauer-kondrak-2016-decoding,
    title = "Decoding Anagrammed Texts Written in an Unknown Language and Script",
    author = "Hauer, Bradley  and
      Kondrak, Grzegorz",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q16-1006",
    doi = "10.1162/tacl_a_00084",
    pages = "75--86",
    abstract = "Algorithmic decipherment is a prime example of a truly unsupervised problem. The first step in the decipherment process is the identification of the encrypted language. We propose three methods for determining the source language of a document enciphered with a monoalphabetic substitution cipher. The best method achieves 97{\%} accuracy on 380 languages. We then present an approach to decoding anagrammed substitution ciphers, in which the letters within words have been arbitrarily transposed. It obtains the average decryption word accuracy of 93{\%} on a set of 50 ciphertexts in 5 languages. Finally, we report the results on the Voynich manuscript, an unsolved fifteenth century cipher, which suggest Hebrew as the language of the document.",
}

@article{Dovbnia2022AutomaticLI,
  title={Automatic Language Identification for Celtic Texts},
  author={Olha Dovbnia and Anna Wr'oblewska},
  journal={ArXiv},
  year={2022},
  volume={abs/2203.04831},
  url={https://www.semanticscholar.org/paper/Automatic-Language-Identification-for-Celtic-Texts-Dovbnia-Wr'oblewska/7e18a345a72b9ff6d8fd54e8f5a9b92a3ba01298}
}

@article{Jauhiainen2019AutomaticLI,
  title={Automatic Language Identification in Texts: A Survey},
  author={T. Jauhiainen and Marco Lui and Marcos Zampieri and Timothy Baldwin and Krister Lind{\'e}n},
  journal={J. Artif. Intell. Res.},
  year={2019},
  volume={65},
  pages={675-782},
  url={https://www.semanticscholar.org/paper/Automatic-Language-Identification-in-Texts%3A-A-Jauhiainen-Lui/739ffc0c6388b853530ee6987fad2d0cdb9014d9},

}

@inproceedings{Minocha2014SubsegmentalLD,
  title={Subsegmental language detection in Celtic language text},
  author={Akshay Minocha and Francis M. Tyers},
  year={2014}
}

@article{taylornsfcareer,
  title={Modeling Language Evolution via Deep Probabilistic Factorization},
  author={Taylor Berg-Kirkpatrick},
  year={2021},
  url={https://www.nsf.gov/awardsearch/showAward?AWD_ID=2146151&HistoricalAwards=false},
}

@article{Skelton2007METHODSOU,
  title={METHODS OF USING PHYLOGENETIC SYSTEMATICS TO RECONSTRUCT THE HISTORY OF THE LINEAR B SCRIPT},
  author={Christina Michelle Skelton},
  journal={Archaeometry},
  year={2007},
  volume={50},
  pages={158-176},
  url={https://www.semanticscholar.org/paper/METHODS-OF-USING-PHYLOGENETIC-SYSTEMATICS-TO-THE-OF-Skelton/4e35498e0ccd754ad817d0ae3d7e133bbdefd821},
}

@inproceedings{Hall2011LargeScaleCR,
  title={Large-Scale Cognate Recovery},
  author={David Hall and Dan Klein},
  booktitle={EMNLP},
  year={2011},
  url={https://www.semanticscholar.org/paper/Large-Scale-Cognate-Recovery-Hall-Klein/cdbcdd8385885aee863783c45320f526f69e55d1},
}

@article{BouchardCt2013AutomatedRO,
  title={Automated reconstruction of ancient languages using probabilistic models of sound change},
  author={Alexandre Bouchard-C{\^o}t{\'e} and David Leo Wright Hall and Thomas L. Griffiths and Dan Klein},
  journal={Proceedings of the National Academy of Sciences},
  url={https://www.semanticscholar.org/paper/Automated-reconstruction-of-ancient-languages-using-Bouchard-C%C3%B4t%C3%A9-Hall/66a07bc51a53d1d2a85c465a79f9da867a1e383c},
  year={2013},
  volume={110},
  pages={4224 - 4229}
}

@misc{https://doi.org/10.48550/arxiv.2103.16046,
  doi = {10.48550/ARXIV.2103.16046},
  
  url = {https://arxiv.org/abs/2103.16046},
  
  author = {Park, Jiwoong and Cho, Junho and Chang, Hyung Jin and Choi, Jin Young},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Hyperbolic Representation Learning via Message Passing Auto-Encoders},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Globalsc77:online,
title = {Global-scale phylogenetic linguistic inference from lexical resources | Scientific Data},
url = {https://www.nature.com/articles/sdata2018189#author-information},
}

@misc{https://doi.org/10.48550/arxiv.1909.02197,
  doi = {10.48550/ARXIV.1909.02197},
  
  url = {https://arxiv.org/abs/1909.02197},
  
  author = {Kudugunta, Sneha Reddy and Bapna, Ankur and Caswell, Isaac and Arivazhagan, Naveen and Firat, Orhan},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Investigating Multilingual NMT Representations at Scale},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.2104.03869,
  doi = {10.48550/ARXIV.2104.03869},
  
  url = {https://arxiv.org/abs/2104.03869},
  
  author = {Chen, Boli and Fu, Yao and Xu, Guangwei and Xie, Pengjun and Tan, Chuanqi and Chen, Mosha and Jing, Liping},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Probing BERT in Hyperbolic Spaces},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{Hyperbol52:online,
title = {Hyperbolic Embeddings with a Hopefully Right Amount of Hyperbole · Stanford DAWN},
url={https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/},
}

@misc{https://doi.org/10.48550/arxiv.2106.02082,
  doi = {10.48550/ARXIV.2106.02082},
  
  url = {https://arxiv.org/abs/2106.02082},
  
  author = {Yu, Dian and He, Taiqi and Sagae, Kenji},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Embeddings for Typology and Cross-lingual Transfer Learning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Manning2020EmergentLS,
  title={Emergent linguistic structure in artificial neural networks trained by self-supervision},
  author={Christopher D. Manning and Kevin Clark and John Hewitt and Urvashi Khandelwal and Omer Levy},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  volume={117},
  pages={30046 - 30054},
  url={https://www.semanticscholar.org/paper/Emergent-linguistic-structure-in-artificial-neural-Manning-Clark/04ef54bd467d5e03dee7b0be601cf06d420bffa0}
}

@misc{https://doi.org/10.48550/arxiv.2002.12005,
  doi = {10.48550/ARXIV.2002.12005},
  
  url = {https://arxiv.org/abs/2002.12005},
  
  author = {Assylbekov, Zhenisbek and Jangeldin, Alibi},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Squashed Shifted PMI Matrix: Bridging Word Embeddings and Hyperbolic Spaces},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2204.12481,
  doi = {10.48550/ARXIV.2204.12481},
  
  url = {https://arxiv.org/abs/2204.12481},
  
  author = {Nurmukhamedov, Sultan and Mach, Thomas and Sheverdin, Arsen and Assylbekov, Zhenisbek},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {From Hyperbolic Geometry Back to Word Embeddings},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{Restorin3:online,
title = {Restoring and attributing ancient texts using deep neural networks | Nature},
url={https://www.nature.com/articles/s41586-022-04448-z#citeas},
}

@misc{NorthEur83:online,
title = {NorthEuraLex - Lexicostatistical Database of Northern Eurasia},
url={http://northeuralex.org/},
}

@misc{Frontier91:online,
title = {Frontiers | Deep Learnability: Using Neural Networks to Quantify Language Similarity and Learnability},
url={https://www.frontiersin.org/articles/10.3389/frai.2020.00043/full},
}

@article{Gamallo2017FromLI,
  title={From language identification to language distance},
  author={Pablo Gamallo and Jos{\'e} Ramom Pichel and I{\~n}aki Alegria},
  journal={Physica A-statistical Mechanics and Its Applications},
  year={2017},
  volume={484},
  pages={152-162},
  url={https://www.semanticscholar.org/paper/From-language-identification-to-language-distance-Gamallo-Pichel/744dec27f5d0e2bb7833bcfbf3ca859e482ca357},
}

@misc{TowardsC42:online,
title = {Towards Computational Lexical Semantic Change Detection},
url={https://languagechange.org/#about},
}

@misc{Hyperbol53:online,
title = {Hyperbolic Geometry to Capture Hierarchical Properties of Words | by Rajwrita Nath | Medium},
url={https://medium.com/@rajwrita/hyperbolic-geometry-to-capture-hierarchical-properties-of-words-57aca80aef5f},
}

@misc{mikucompling,
author = {Mikulyte, Gabija and Gilbert, David},
year = {2020},
month = {01},
pages = {},
title = {An efficient automated data analytics approach to large scale computational comparative linguistics},
url = {https://www.researchgate.net/publication/338989490_An_efficient_automated_data_analytics_approach_to_large_scale_computational_comparative_linguistics},
}

@inproceedings{Samohi2022UsingCP,
  title={Using Cross-Lingual Part of Speech Tagging for Partially Reconstructing the Classic Language Family Tree Model},
  author={Anat Samohi and Daniel Weisberg Mitelman and Kfir Bar},
  booktitle={LCHANGE},
  year={2022},
  url={https://www.semanticscholar.org/paper/Using-Cross-Lingual-Part-of-Speech-Tagging-for-the-Samohi-Mitelman/3feced5e538105040b334883cde6dbf5950033f1}
}

@article{Chiswick2004LinguisticDA,
  title={Linguistic Distance: A Quantitative Measure of the Distance Between English and Other Languages},
  author={Barry R. Chiswick and Paul Washington Miller},
  journal={Journal of Multilingual and Multicultural Development},
  year={2004},
  volume={26},
  pages={1 - 11},
  url={https://www.semanticscholar.org/paper/Linguistic-Distance%3A-A-Quantitative-Measure-of-the-Chiswick-Miller/9d5d973725979cf14798658b2bfe78c11c544652}
}

@misc{LexicalD18,
title = {Lexical Distance Among Languages of Europe 2015 Alternative Transport},
url={https://alternativetransport.wordpress.com/2015/05/05/34/},
}

@inproceedings{Barannikov2022RepresentationTD,
  title={Representation Topology Divergence: A Method for Comparing Neural Network Representations},
  author={S. Barannikov and Ilya Trofimov and Nikita Balabin and Evgeny Burnaev},
  booktitle={ICML},
  year={2022},
url={https://www.semanticscholar.org/paper/Representation-Topology-Divergence%3A-A-Method-for-Barannikov-Trofimov/4756579b2031cc6c6e41a31712b7ef090e88cc5a},
}

@article{Xia2020PredictingPF,
  title={Predicting Performance for Natural Language Processing Tasks},
  author={M. Xia and Antonios Anastasopoulos and Ruochen Xu and Yiming Yang and Graham Neubig},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.00870},
url={https://www.semanticscholar.org/paper/Predicting-Performance-for-Natural-Language-Tasks-Xia-Anastasopoulos/deedb9b61a01d686b28e6034770fccc142e77fab}
}

@misc{https://doi.org/10.48550/arxiv.2006.11239,
  doi = {10.48550/ARXIV.2006.11239},
  
  url = {https://arxiv.org/abs/2006.11239},
  
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Denoising Diffusion Probabilistic Models},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{weng2021diffusion,
  title   = "What are diffusion models?",
  author  = "Weng, Lilian",
  journal = "lilianweng.github.io",
  year    = "2021",
  url     = "https://lilianweng.github.io/posts/2021-07-11-diffusion-models/"
}

@article{joshi2020transformers,
author = {Joshi, Chaitanya},
title = {Transformers are Graph Neural Networks},
journal = {The Gradient},
year = {2020},
howpublished = {\url{https://thegradient.pub/transformers-are-gaph-neural-networks/ } },
url={https://thegradient.pub/transformers-are-graph-neural-networks/},
}

@inproceedings{Rieck19a,
  title     = {Neural Persistence: {A} Complexity Measure for Deep Neural Networks Using Algebraic Topology},
  author    = {Bastian Rieck and Matteo Togninalli and Christian Bock and Michael Moor and Max Horn and Thomas Gumbsch and Karsten Borgwardt},
  booktitle = {International Conference on Learning Representations~(ICLR)},
  year      = {2019},
  url       = {https://openreview.net/forum?id=ByxkijC5FQ},
}

@inproceedings{Jakubowski2020TopologyOW,
  title={Topology of Word Embeddings: Singularities Reflect Polysemy},
  author={Alexander Jakubowski and Milica Ga{\vs}i{\'c} and Marcus Zibrowius},
  booktitle={STARSEM},
  year={2020},
  url={https://www.semanticscholar.org/paper/Topology-of-Word-Embeddings%3A-Singularities-Reflect-Jakubowski-Ga%C5%A1i%C4%87/a379e9e5025d3e2e1897b211bca8338bc0155311},
}

@article{Cherniavskii2022AcceptabilityJV,
  title={Acceptability Judgements via Examining the Topology of Attention Maps},
  author={Daniil Cherniavskii and Eduard Tulchinskii and Vladislav Mikhailov and Irina Proskurina and Laida Kushnareva and E. Artemova and S. Barannikov and Irina Piontkovskaya and D. Piontkovski and Evgeny Burnaev},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.09630},
  url={https://www.semanticscholar.org/paper/Acceptability-Judgements-via-Examining-the-Topology-Cherniavskii-Tulchinskii/320a5d32a22287ec80713f9087d1a300703eb970},
}

@article{https://doi.org/10.48550/arxiv.2205.00953,
  doi = {10.48550/ARXIV.2205.00953},
  
  url = {https://arxiv.org/abs/2205.00953},
  
  author = {Chauhan, Jatin and Kaul, Manohar},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERTops: Studying BERT Representations under a Topological Lens},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{colombo-etal-2021-automatic,
    title = "Automatic Text Evaluation through the Lens of {W}asserstein Barycenters",
    author = "Colombo, Pierre  and
      Staerman, Guillaume  and
      Clavel, Chlo{\'e}  and
      Piantanida, Pablo",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.817",
    doi = "10.18653/v1/2021.emnlp-main.817",
    pages = "10450--10466",
    abstract = "A new metric BaryScore to evaluate text generation based on deep contextualized embeddings (\textit{e.g.}, BERT, Roberta, ELMo) is introduced. This metric is motivated by a new framework relying on optimal transport tools, \textit{i.e.}, Wasserstein distance and barycenter. By modelling the layer output of deep contextualized embeddings as a probability distribution rather than by a vector embedding; this framework provides a natural way to aggregate the different outputs through the Wasserstein space topology. In addition, it provides theoretical grounds to our metric and offers an alternative to available solutions (\textit{e.g.}, MoverScore and BertScore). Numerical evaluation is performed on four different tasks: machine translation, summarization, data2text generation and image captioning. Our results show that BaryScore outperforms other BERT based metrics and exhibits more consistent behaviour in particular for text summarization.",
}

@article{https://doi.org/10.48550/arxiv.2109.04825,
  doi = {10.48550/ARXIV.2109.04825},
  
  url = {https://arxiv.org/abs/2109.04825},
  
  author = {Kushnareva, Laida and Cherniavskii, Daniil and Mikhailov, Vladislav and Artemova, Ekaterina and Barannikov, Serguei and Bernstein, Alexander and Piontkovskaya, Irina and Piontkovski, Dmitri and Burnaev, Evgeny},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Algebraic Topology (math.AT), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Artificial Text Detection via Examining the Topology of Attention Maps},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2106.06469,
  doi = {10.48550/ARXIV.2106.06469},
  
  url = {https://proceedings.neurips.cc/paper/2021/file/8fd7f981e10b41330b618129afcaab2d-Paper.pdf},
  
  author = {Zheng, Songzhu and Zhang, Yikai and Wagner, Hubert and Goswami, Mayank and Chen, Chao},
  
  keywords = {Machine Learning (cs.LG), Computational Geometry (cs.CG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Topological Detection of Trojaned Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{haghighi-etal-2008-learning,
    title = "Learning Bilingual Lexicons from Monolingual Corpora",
    author = "Haghighi, Aria  and
      Liang, Percy  and
      Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of ACL-08: HLT",
    month = jun,
    year = "2008",
    address = "Columbus, Ohio",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P08-1088",
    pages = "771--779",
}

@inproceedings{Huang_2021,
	doi = {10.1109/icassp39728.2021.9413523},
  
	url = {https://doi.org/10.1109%2Ficassp39728.2021.9413523},
  
	year = 2021,
	month = {jun},
  
	publisher = {{IEEE}
},
  
	author = {Ningyuan Teresa Huang and Soledad Villar},
  
	title = {A Short Tutorial on The Weisfeiler-Lehman Test And Its Variants},
  
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})}
}

@misc{https://doi.org/10.48550/arxiv.2202.02495,
  doi = {10.48550/ARXIV.2202.02495},
  
  url = {https://arxiv.org/abs/2202.02495},
  
  author = {Chen, Samantha and Lim, Sunhyuk and Mémoli, Facundo and Wan, Zhengchao and Wang, Yusu},
  
  keywords = {Machine Learning (cs.LG), Metric Geometry (math.MG), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Weisfeiler-Lehman meets Gromov-Wasserstein},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1905.00537,
  doi = {10.48550/ARXIV.1905.00537},
  
  url = {https://arxiv.org/abs/1905.00537},
  
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1706.03762,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
url="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe"
}

@misc{https://doi.org/10.48550/arxiv.1810.04805,
  doi = {10.48550/ARXIV.1810.04805},
  
  url = {https://arxiv.org/abs/1810.04805},
  
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Zador2019ACO,
  title={A critique of pure learning and what artificial neural networks can learn from animal brains},
  author={Anthony M. Zador},
  journal={Nature Communications},
  year={2019},
  volume={10},
url="https://www.semanticscholar.org/paper/A-critique-of-pure-learning-and-what-artificial-can-Zador/32abbcb3aa75ac34a92624dc779a9f7a82ee981c",
}

@article{Bergomi2019TowardsAT,
  title={Towards a topological–geometrical theory of group equivariant non-expansive operators for data analysis and machine learning},
  author={Mattia G. Bergomi and Patrizio Frosini and Daniela Giorgi and Nicola Quercioli},
  journal={Nature Machine Intelligence},
  year={2019},
  pages={1-11},
  url="https://www.semanticscholar.org/paper/Towards-a-topological%E2%80%93geometrical-theory-of-group-Bergomi-Frosini/3449e962accb5a34c4b1a9346452b08e03bf9f59"
}

@inproceedings{ye-etal-2021-one2set,
    title = "{O}ne2{S}et: {G}enerating Diverse Keyphrases as a Set",
    author = "Ye, Jiacheng  and
      Gui, Tao  and
      Luo, Yichao  and
      Xu, Yige  and
      Zhang, Qi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.354",
    doi = "10.18653/v1/2021.acl-long.354",
    pages = "4598--4608",
    abstract = "Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training. However, the keyphrases are inherently an unordered set rather than an ordered sequence. Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases. In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases. To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel. To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step label assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the repetition rate of generated keyphrases. The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.",
}

@inproceedings{yuan-etal-2019-interactive,
    title = "Interactive Language Learning by Question Answering",
    author = "Yuan, Xingdi  and
      C{\^o}t{\'e}, Marc-Alexandre  and
      Fu, Jie  and
      Lin, Zhouhan  and
      Pal, Chris  and
      Bengio, Yoshua  and
      Trischler, Adam",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1280",
    doi = "10.18653/v1/D19-1280",
    pages = "2796--2813",
    abstract = "Humans observe and interact with the world to acquire knowledge. However, most existing machine reading comprehension (MRC) tasks miss the interactive, information-seeking component of comprehension. Such tasks present models with static documents that contain all necessary information, usually concentrated in a single short substring. Thus, models can achieve strong performance through simple word- and phrase-based pattern matching. We address this problem by formulating a novel text-based question answering task: Question Answering with Interactive Text (QAit). In QAit, an agent must interact with a partially observable text-based environment to gather information required to answer questions. QAit poses questions about the existence, location, and attributes of objects found in the environment. The data is built using a text-based game generator that defines the underlying dynamics of interaction with the environment. We propose and evaluate a set of baseline models for the QAit task that includes deep reinforcement learning agents. Experiments show that the task presents a major challenge for machine reading systems, while humans solve it with relative ease.",
}

@inproceedings{Ct2018TextWorldAL,
  title={TextWorld: A Learning Environment for Text-based Games},
  author={Marc-Alexandre C{\^o}t{\'e} and {\'A}kos K{\'a}d{\'a}r and Xingdi Yuan and Ben A. Kybartas and Tavian Barnes and Emery Fine and James Moore and Matthew J. Hausknecht and Layla El Asri and Mahmoud Adada and Wendy Tay and Adam Trischler},
  booktitle={CGW@IJCAI},
  year={2018}
}

@article{Alabdulkarim2021GoalDirectedSG,
  title={Goal-Directed Story Generation: Augmenting Generative Language Models with Reinforcement Learning},
  author={Amal Alabdulkarim and Winston Wai-Tai Li and Lara J. Martin and Mark O. Riedl},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.08593},
  url={https://www.semanticscholar.org/paper/Goal-Directed-Story-Generation%3A-Augmenting-Language-Alabdulkarim-Li/4ae9648bfa97dfe6a01b8bc2282ae363660856f5},
}

@inproceedings{Ammanabrolu2021LearningKG,
  title={Learning Knowledge Graph-based World Models of Textual Environments},
  author={Prithviraj Ammanabrolu and Mark O. Riedl},
  booktitle={NeurIPS},
  year={2021},
  url={https://www.semanticscholar.org/paper/Learning-Knowledge-Graph-based-World-Models-of-Ammanabrolu-Riedl/c8fc50bc2cd673fa4e5c5ac581f6fe3fcdaf6b8d},
}

@article{Jansen2022ASS,
  title={A Systematic Survey of Text Worlds as Embodied Natural Language Environments},
  author={Peter Alexander Jansen},
  journal={ArXiv},
  year={2022},
  volume={abs/2107.04132},
  url={https://www.semanticscholar.org/paper/A-Systematic-Survey-of-Text-Worlds-as-Embodied-Jansen/d54f0b453f205f68e24e1b3515c846e23aca196f},
}

@article{hausknecht19,
  title={Interactive Fiction Games: A Colossal Adventure},
  author={Hausknecht, Matthew and Ammanabrolu, Prithviraj and C\^ot\'{e} Marc-Alexandre and Yuan Xingdi},
  journal={CoRR},
  year={2019},
  url={http://arxiv.org/abs/1909.05398},
  volume={abs/1909.05398}
}

@misc{https://doi.org/10.48550/arxiv.2106.09578,
  doi = {10.48550/ARXIV.2106.09578},
  
  url = {https://arxiv.org/abs/2106.09578},
  
  author = {Ammanabrolu, Prithviraj and Riedl, Mark O.},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Modeling Worlds in Text},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{ammthesis,
  title={Language Learning in Interactive Environments},
  author={Ammanabrolu, Prithviraj},
  url = {https://smartech.gatech.edu/handle/1853/65088},
  year = {2021},
}

@inproceedings{garbe2019storyassembler,
  title={StoryAssembler: an engine for generating dynamic choice-driven narratives},
  author={Garbe, Jacob and Kreminski, Max and Samuel, Ben and Wardrip-Fruin, Noah and Mateas, Michael},
  booktitle={Proceedings of the 14th International Conference on the Foundations of Digital Games},
  pages={1--10},
  year={2019},
  url={https://mkremins.github.io/publications/StoryAssembler.pdf},
}

@inproceedings{mason2019lume,
  title={Lume: a system for procedural story generation},
  author={Mason, Stacey and Stagg, Ceri and Wardrip-Fruin, Noah},
  booktitle={Proceedings of the 14th International Conference on the Foundations of Digital Games},
  pages={1--9},
  year={2019},
  url = {https://eis.ucsc.edu/papers/Mason_Lume.pdf},
}

@article{calderwoodspinning,
  title={Spinning Coherent Interactive Fiction through Foundation Model Prompts},
  author={Calderwood, Alex and Wardrip-Fruin, Noah and Mateas, Michael},
  url = {https://computationalcreativity.net/iccc22/wp-content/uploads/2022/06/ICCC-2022_2L_Calderwood-et-al..pdf},
  year = {2022},

}

@misc{https://doi.org/10.48550/arxiv.2001.10161,
  doi = {10.48550/ARXIV.2001.10161},
  
  url = {https://arxiv.org/abs/2001.10161},
  
  author = {Ammanabrolu, Prithviraj and Cheung, Wesley and Tu, Dan and Broniec, William and Riedl, Mark O.},
  
  keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Bringing Stories Alive: Generating Interactive Fiction Worlds},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{holtzman-etal-2018-learning,
    title = "Learning to Write with Cooperative Discriminators",
    author = "Holtzman, Ari  and
      Buys, Jan  and
      Forbes, Maxwell  and
      Bosselut, Antoine  and
      Golub, David  and
      Choi, Yejin",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1152",
    doi = "10.18653/v1/P18-1152",
    pages = "1638--1649",
    abstract = "Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice{'}s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.",
}

@inproceedings{mireshghallah-etal-2022-mix,
    title = "Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models",
    author = "Mireshghallah, Fatemehsadat  and
      Goyal, Kartik  and
      Berg-Kirkpatrick, Taylor",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.31",
    doi = "10.18653/v1/2022.acl-long.31",
    pages = "401--415",
    abstract = "Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.",
}

@misc{https://doi.org/10.48550/arxiv.2106.02736,
  doi = {10.48550/ARXIV.2106.02736},
  
  url = {https://arxiv.org/abs/2106.02736},
  
  author = {Goyal, Kartik and Dyer, Chris and Berg-Kirkpatrick, Taylor},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{krause-etal-2021-gedi-generative,
    title = "{G}e{D}i: Generative Discriminator Guided Sequence Generation",
    author = "Krause, Ben  and
      Gotmare, Akhilesh Deepak  and
      McCann, Bryan  and
      Keskar, Nitish Shirish  and
      Joty, Shafiq  and
      Socher, Richard  and
      Rajani, Nazneen Fatema",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.424",
    doi = "10.18653/v1/2021.findings-emnlp.424",
    pages = "4929--4952",
    abstract = "",
}

@inproceedings{Chaffin_2022,
	doi = {10.1145/3477495.3531858},
  
	url = {https://doi.org/10.1145%2F3477495.3531858},
  
	year = 2022,
	month = {jul},
  
	publisher = {{ACM}
},
  
	author = {Antoine Chaffin and Thomas Scialom and Sylvain Lamprier and Jacopo Staiano and Benjamin Piwowarski and Ewa Kijak and Vincent Claveau},
  
	title = {Which Discriminator for Cooperative Text Generation?},
  
	booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval}
}

@inproceedings{chaffin-etal-2022-ppl,
    title = "{PPL-MCTS}: {C}onstrained Textual Generation Through Discriminator-Guided {MCTS} Decoding",
    author = "Chaffin, Antoine  and
      Claveau, Vincent  and
      Kijak, Ewa",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.215",
    doi = "10.18653/v1/2022.naacl-main.215",
    pages = "2953--2967",
    abstract = "",
}

@inproceedings{chaffin-etal-2022-ppl,
    title = "{PPL-MCTS}: {C}onstrained Textual Generation Through Discriminator-Guided {MCTS} Decoding",
    author = "Chaffin, Antoine  and
      Claveau, Vincent  and
      Kijak, Ewa",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.215",
    doi = "10.18653/v1/2022.naacl-main.215",
    pages = "2953--2967",
    abstract = "Large language models (LM) based on Transformers allow to generate plausible long texts. In this paper, we explore how this generation can be further controlled at decoding time to satisfy certain constraints (e.g. being non-toxic, conveying certain emotions, using a specific writing style, etc.) without fine-tuning the LM.Precisely, we formalize constrained generation as a tree exploration process guided by a discriminator that indicates how well the associated sequence respects the constraint. This approach, in addition to being easier and cheaper to train than fine-tuning the LM, allows to apply the constraint more finely and dynamically.We propose several original methods to search this generation tree, notably the Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the search efficiency, but also simpler methods based on re-ranking a pool of diverse sequences using the discriminator scores. These methods are evaluated, with automatic and human-based metrics, on two types of constraints and languages: review polarity and emotion control in French and English. We show that discriminator-guided MCTS decoding achieves state-of-the-art results without having to tune the language model, in both tasks and languages. We also demonstrate that other proposed decoding methods based on re-ranking can be really effective when diversity among the generated propositions is encouraged.",
}

@misc{https://doi.org/10.48550/arxiv.2110.07178,
  doi = {10.48550/ARXIV.2110.07178},
  
  url = {https://arxiv.org/abs/2110.07178},
  
  author = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Symbolic Knowledge Distillation: from General Language Models to Commonsense Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@INPROCEEDINGS{8953424,  
author={Corneanu, Ciprian A. and Madadi, Meysam and Escalera, Sergio and Martinez, Aleix M.},  
booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   
title={What Does It Mean to Learn in Deep Networks? And, How Does One Detect Adversarial Attacks?},   
year={2019},  volume={},  number={},  pages={4752-4761},  doi={10.1109/CVPR.2019.00489}
}

@misc{https://doi.org/10.48550/arxiv.2111.15651,
  doi = {10.48550/ARXIV.2111.15651},
  
  url = {https://arxiv.org/abs/2111.15651},
  
  author = {Synakowski, Stuart and Benitez-Quiroz, Fabian and Martinez, Aleix M.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Leveraging The Topological Consistencies of Learning in Deep Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2005.00450,
  doi = {10.48550/ARXIV.2005.00450},
  
  url = {https://arxiv.org/abs/2005.00450},
  
  author = {Corneanu, Ciprian and Madadi, Meysam and Escalera, Sergio and Martinez, Aleix},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Computing the Testing Error without a Testing Set},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.2111.15651,
  doi = {10.48550/ARXIV.2111.15651},
  
  url = {https://arxiv.org/abs/2111.15651},
  
  author = {Synakowski, Stuart and Benitez-Quiroz, Fabian and Martinez, Aleix M.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Leveraging The Topological Consistencies of Learning in Deep Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{arora-etal-2022-estimating,
    title = "Estimating the Entropy of Linguistic Distributions",
    author = "Arora, Aryaman  and
      Meister, Clara  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-short.20",
    doi = "10.18653/v1/2022.acl-short.20",
    pages = "175--195",
    abstract = "Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies.",
}

@misc{https://doi.org/10.48550/arxiv.2205.01068,
  doi = {10.48550/ARXIV.2205.01068},
  
  url = {https://arxiv.org/abs/2205.01068},
  
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OPT: Open Pre-trained Transformer Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{https://doi.org/10.48550/arxiv.2207.14251,
  doi = {10.48550/ARXIV.2207.14251},
  
  url = {https://arxiv.org/abs/2207.14251},
  
  author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Feder, Amir and Ravichander, Abhilasha and Mosbach, Marius and Belinkov, Yonatan and Schütze, Hinrich and Goldberg, Yoav},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Measuring Causal Effects of Data Statistics on Language Model's `Factual' Predictions},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{https://doi.org/10.48550/arxiv.2101.07752,
  doi = {10.48550/ARXIV.2101.07752},
  
  url = {https://arxiv.org/abs/2101.07752},
  
  author = {Pérez-Fernández, David and Gutiérrez-Fandiño, Asier and Armengol-Estapé, Jordi and Villegas, Marta},
  
  keywords = {Machine Learning (cs.LG), Algebraic Topology (math.AT), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Characterizing and Measuring the Similarity of Neural Networks with Persistent Homology},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
